<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>深度學習前菜：感知機、MLP 與激活函數演進全解析 - Yu's Portfolio & Learning Hub</title><meta name=description content='深度學習的基礎始於感知機與多層感知機（MLP）。從單層感知機的線性可分，到多層神經網路的強大表達能力，再到激活函數的演進（Sigmoid、Tanh、ReLU、GELU），這些理論與實作是理解現代深度學習的起點。本章將深入數學推導、直覺圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握深度學習基礎。
感知機（Perceptron）與線性可分 感知機原理 最早的神經元模型，單層線性分類器。 輸出 $y = \text{sign}(w^T x + b)$，僅能解決線性可分問題。 線性可分與限制 僅能分割線性可分資料（如 AND），無法處理 XOR 問題。 多層感知機（MLP）可突破此限制。 import numpy as np class Perceptron: def __init__(self, lr=0.1, n_iter=10): self.lr = lr self.n_iter = n_iter def fit(self, X, y): self.w = np.zeros(X.shape[1]) self.b = 0 for _ in range(self.n_iter): for xi, yi in zip(X, y): update = self.lr * (yi - self.predict(xi)) self.w += update * xi self.b += update def predict(self, X): return np.where(np.dot(X, self.w) + self.b >= 0, 1, -1) X = np.array([[0,0],[0,1],[1,0],[1,1]]) y = np.array([-1, -1, -1, 1]) # AND model = Perceptron() model.fit(X, y) print("預測:", model.predict(X)) 激活函數演進：Sigmoid / Tanh → ReLU / GELU Sigmoid 與 Tanh Sigmoid: $f(x) = \frac{1}{1 + e^{-x}}$，輸出範圍 (0,1) Tanh: $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$，輸出範圍 (-1,1) 缺點：梯度消失，收斂慢 ReLU（Rectified Linear Unit） $f(x) = \max(0, x)$，簡單高效，解決梯度消失 缺點：死神經元（Dead Neuron） GELU（Gaussian Error Linear Unit） $f(x) = x \cdot \Phi(x)$，$\Phi$ 為標準常態 CDF 近年 Transformer 等模型常用，平滑且表現佳 import torch import torch.nn.functional as F x = torch.linspace(-3, 3, 10) print("Sigmoid:", torch.sigmoid(x)) print("Tanh:", torch.tanh(x)) print("ReLU:", F.relu(x)) print("GELU:", F.gelu(x)) 參數量、層數與表達能力 MLP（多層感知機）結構 多層線性層 + 非線性激活函數 理論上單隱藏層即可逼近任意連續函數（Universal Approximation Theorem） 參數量與層數 參數量 = 每層輸入數 × 輸出數 + 偏置 層數增加 → 表達能力提升，但訓練更難，易過擬合 Python 實作：簡單 MLP import torch.nn as nn mlp = nn.Sequential( nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 1) ) print(mlp) 理論直覺、應用場景與常見誤區 應用場景 感知機：早期二分類、線性問題 MLP：表格資料、特徵工程後的分類/迴歸 激活函數：ReLU/GELU 幾乎為深度學習標配 常見誤區 誤以為 MLP 必能解決所有問題（資料需可分） 忽略激活函數選擇對收斂與表現的影響 層數過多未正則化，導致過擬合 面試熱點與經典問題 主題 常見問題 感知機 為何只能解線性可分？ 激活函數 ReLU 為何優於 Sigmoid？GELU 有何優勢？ MLP 為何能逼近任意函數？ 參數量 如何計算？層數與表達能力關係？ 使用注意事項 激活函數選擇會影響梯度傳遞與收斂速度 MLP 易過擬合，建議搭配正則化與早停 感知機僅適合線性問題，複雜資料需多層網路 延伸閱讀與資源 Deep Learning Book: Perceptron & MLP PyTorch Activation Functions Universal Approximation Theorem 經典面試題與解法提示 感知機的學習規則與收斂條件？ 為何感知機無法解 XOR 問題？ Sigmoid、Tanh、ReLU、GELU 優缺點比較？ MLP 為何能逼近任意連續函數？ 如何計算 MLP 參數量？ 激活函數選擇對訓練有何影響？ 如何用 Python 實作簡單感知機/MLP？ MLP 過擬合時有哪些解法？ ReLU 死神經元問題如何緩解？ GELU 為何在 Transformer 中表現佳？ 結語 感知機與 MLP 是深度學習的起點。熟悉激活函數演進、參數量與表達能力，能讓你在後續 CNN、RNN、Transformer 等進階主題中打下堅實基礎。下一章將進入卷積網路精要，敬請期待！
'><meta property="og:title" content="深度學習前菜：感知機、MLP 與激活函數演進全解析"><meta property="og:description" content='深度學習的基礎始於感知機與多層感知機（MLP）。從單層感知機的線性可分，到多層神經網路的強大表達能力，再到激活函數的演進（Sigmoid、Tanh、ReLU、GELU），這些理論與實作是理解現代深度學習的起點。本章將深入數學推導、直覺圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握深度學習基礎。
感知機（Perceptron）與線性可分 感知機原理 最早的神經元模型，單層線性分類器。 輸出 $y = \text{sign}(w^T x + b)$，僅能解決線性可分問題。 線性可分與限制 僅能分割線性可分資料（如 AND），無法處理 XOR 問題。 多層感知機（MLP）可突破此限制。 import numpy as np class Perceptron: def __init__(self, lr=0.1, n_iter=10): self.lr = lr self.n_iter = n_iter def fit(self, X, y): self.w = np.zeros(X.shape[1]) self.b = 0 for _ in range(self.n_iter): for xi, yi in zip(X, y): update = self.lr * (yi - self.predict(xi)) self.w += update * xi self.b += update def predict(self, X): return np.where(np.dot(X, self.w) + self.b >= 0, 1, -1) X = np.array([[0,0],[0,1],[1,0],[1,1]]) y = np.array([-1, -1, -1, 1]) # AND model = Perceptron() model.fit(X, y) print("預測:", model.predict(X)) 激活函數演進：Sigmoid / Tanh → ReLU / GELU Sigmoid 與 Tanh Sigmoid: $f(x) = \frac{1}{1 + e^{-x}}$，輸出範圍 (0,1) Tanh: $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$，輸出範圍 (-1,1) 缺點：梯度消失，收斂慢 ReLU（Rectified Linear Unit） $f(x) = \max(0, x)$，簡單高效，解決梯度消失 缺點：死神經元（Dead Neuron） GELU（Gaussian Error Linear Unit） $f(x) = x \cdot \Phi(x)$，$\Phi$ 為標準常態 CDF 近年 Transformer 等模型常用，平滑且表現佳 import torch import torch.nn.functional as F x = torch.linspace(-3, 3, 10) print("Sigmoid:", torch.sigmoid(x)) print("Tanh:", torch.tanh(x)) print("ReLU:", F.relu(x)) print("GELU:", F.gelu(x)) 參數量、層數與表達能力 MLP（多層感知機）結構 多層線性層 + 非線性激活函數 理論上單隱藏層即可逼近任意連續函數（Universal Approximation Theorem） 參數量與層數 參數量 = 每層輸入數 × 輸出數 + 偏置 層數增加 → 表達能力提升，但訓練更難，易過擬合 Python 實作：簡單 MLP import torch.nn as nn mlp = nn.Sequential( nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 1) ) print(mlp) 理論直覺、應用場景與常見誤區 應用場景 感知機：早期二分類、線性問題 MLP：表格資料、特徵工程後的分類/迴歸 激活函數：ReLU/GELU 幾乎為深度學習標配 常見誤區 誤以為 MLP 必能解決所有問題（資料需可分） 忽略激活函數選擇對收斂與表現的影響 層數過多未正則化，導致過擬合 面試熱點與經典問題 主題 常見問題 感知機 為何只能解線性可分？ 激活函數 ReLU 為何優於 Sigmoid？GELU 有何優勢？ MLP 為何能逼近任意函數？ 參數量 如何計算？層數與表達能力關係？ 使用注意事項 激活函數選擇會影響梯度傳遞與收斂速度 MLP 易過擬合，建議搭配正則化與早停 感知機僅適合線性問題，複雜資料需多層網路 延伸閱讀與資源 Deep Learning Book: Perceptron & MLP PyTorch Activation Functions Universal Approximation Theorem 經典面試題與解法提示 感知機的學習規則與收斂條件？ 為何感知機無法解 XOR 問題？ Sigmoid、Tanh、ReLU、GELU 優缺點比較？ MLP 為何能逼近任意連續函數？ 如何計算 MLP 參數量？ 激活函數選擇對訓練有何影響？ 如何用 Python 實作簡單感知機/MLP？ MLP 過擬合時有哪些解法？ ReLU 死神經元問題如何緩解？ GELU 為何在 Transformer 中表現佳？ 結語 感知機與 MLP 是深度學習的起點。熟悉激活函數演進、參數量與表達能力，能讓你在後續 CNN、RNN、Transformer 等進階主題中打下堅實基礎。下一章將進入卷積網路精要，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/perceptron-mlp/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/perceptron-mlp/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>深度學習前菜：感知機、MLP 與激活函數演進全解析</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>深度學習前菜：感知機、MLP 與激活函數演進全解析</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-03-25</span></div></header><div class=article-body><p>深度學習的基礎始於感知機與多層感知機（MLP）。從單層感知機的線性可分，到多層神經網路的強大表達能力，再到激活函數的演進（Sigmoid、Tanh、ReLU、GELU），這些理論與實作是理解現代深度學習的起點。本章將深入數學推導、直覺圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握深度學習基礎。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#感知機perceptron與線性可分>感知機（Perceptron）與線性可分</a><ul><li><a href=#感知機原理>感知機原理</a></li><li><a href=#線性可分與限制>線性可分與限制</a></li></ul></li><li><a href=#激活函數演進sigmoid--tanh--relu--gelu>激活函數演進：Sigmoid / Tanh → ReLU / GELU</a><ul><li><a href=#sigmoid-與-tanh>Sigmoid 與 Tanh</a></li><li><a href=#relurectified-linear-unit>ReLU（Rectified Linear Unit）</a></li><li><a href=#gelugaussian-error-linear-unit>GELU（Gaussian Error Linear Unit）</a></li></ul></li><li><a href=#參數量層數與表達能力>參數量、層數與表達能力</a><ul><li><a href=#mlp多層感知機結構>MLP（多層感知機）結構</a></li><li><a href=#參數量與層數>參數量與層數</a></li><li><a href=#python-實作簡單-mlp>Python 實作：簡單 MLP</a></li></ul></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=感知機perceptron與線性可分>感知機（Perceptron）與線性可分</h2><h3 id=感知機原理>感知機原理</h3><ul><li>最早的神經元模型，單層線性分類器。</li><li>輸出 $y = \text{sign}(w^T x + b)$，僅能解決線性可分問題。</li></ul><h3 id=線性可分與限制>線性可分與限制</h3><ul><li>僅能分割線性可分資料（如 AND），無法處理 XOR 問題。</li><li>多層感知機（MLP）可突破此限制。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Perceptron</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, n_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> lr
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_iter <span style=color:#f92672>=</span> n_iter
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(self<span style=color:#f92672>.</span>n_iter):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> xi, yi <span style=color:#f92672>in</span> zip(X, y):
</span></span><span style=display:flex><span>                update <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> (yi <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>predict(xi))
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>w <span style=color:#f92672>+=</span> update <span style=color:#f92672>*</span> xi
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>b <span style=color:#f92672>+=</span> update
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>where(np<span style=color:#f92672>.</span>dot(X, self<span style=color:#f92672>.</span>w) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>],[<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>],[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>],[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>]])
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>])  <span style=color:#75715e># AND</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> Perceptron()
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;預測:&#34;</span>, model<span style=color:#f92672>.</span>predict(X))
</span></span></code></pre></div><hr><h2 id=激活函數演進sigmoid--tanh--relu--gelu>激活函數演進：Sigmoid / Tanh → ReLU / GELU</h2><h3 id=sigmoid-與-tanh>Sigmoid 與 Tanh</h3><ul><li>Sigmoid: $f(x) = \frac{1}{1 + e^{-x}}$，輸出範圍 (0,1)</li><li>Tanh: $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$，輸出範圍 (-1,1)</li><li>缺點：梯度消失，收斂慢</li></ul><h3 id=relurectified-linear-unit>ReLU（Rectified Linear Unit）</h3><ul><li>$f(x) = \max(0, x)$，簡單高效，解決梯度消失</li><li>缺點：死神經元（Dead Neuron）</li></ul><h3 id=gelugaussian-error-linear-unit>GELU（Gaussian Error Linear Unit）</h3><ul><li>$f(x) = x \cdot \Phi(x)$，$\Phi$ 為標準常態 CDF</li><li>近年 Transformer 等模型常用，平滑且表現佳</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Sigmoid:&#34;</span>, torch<span style=color:#f92672>.</span>sigmoid(x))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Tanh:&#34;</span>, torch<span style=color:#f92672>.</span>tanh(x))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;ReLU:&#34;</span>, F<span style=color:#f92672>.</span>relu(x))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;GELU:&#34;</span>, F<span style=color:#f92672>.</span>gelu(x))
</span></span></code></pre></div><hr><h2 id=參數量層數與表達能力>參數量、層數與表達能力</h2><h3 id=mlp多層感知機結構>MLP（多層感知機）結構</h3><ul><li>多層線性層 + 非線性激活函數</li><li>理論上單隱藏層即可逼近任意連續函數（Universal Approximation Theorem）</li></ul><h3 id=參數量與層數>參數量與層數</h3><ul><li>參數量 = 每層輸入數 × 輸出數 + 偏置</li><li>層數增加 → 表達能力提升，但訓練更難，易過擬合</li></ul><h3 id=python-實作簡單-mlp>Python 實作：簡單 MLP</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mlp <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>16</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(mlp)
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>感知機：早期二分類、線性問題</li><li>MLP：表格資料、特徵工程後的分類/迴歸</li><li>激活函數：ReLU/GELU 幾乎為深度學習標配</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>誤以為 MLP 必能解決所有問題（資料需可分）</li><li>忽略激活函數選擇對收斂與表現的影響</li><li>層數過多未正則化，導致過擬合</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>感知機</td><td>為何只能解線性可分？</td></tr><tr><td>激活函數</td><td>ReLU 為何優於 Sigmoid？GELU 有何優勢？</td></tr><tr><td>MLP</td><td>為何能逼近任意函數？</td></tr><tr><td>參數量</td><td>如何計算？層數與表達能力關係？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>激活函數選擇會影響梯度傳遞與收斂速度</li><li>MLP 易過擬合，建議搭配正則化與早停</li><li>感知機僅適合線性問題，複雜資料需多層網路</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://www.deeplearningbook.org/contents/mlp.html>Deep Learning Book: Perceptron & MLP</a></li><li><a href=https://pytorch.org/docs/stable/nn.functional.html>PyTorch Activation Functions</a></li><li><a href=https://en.wikipedia.org/wiki/Universal_approximation_theorem>Universal Approximation Theorem</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>感知機的學習規則與收斂條件？</li><li>為何感知機無法解 XOR 問題？</li><li>Sigmoid、Tanh、ReLU、GELU 優缺點比較？</li><li>MLP 為何能逼近任意連續函數？</li><li>如何計算 MLP 參數量？</li><li>激活函數選擇對訓練有何影響？</li><li>如何用 Python 實作簡單感知機/MLP？</li><li>MLP 過擬合時有哪些解法？</li><li>ReLU 死神經元問題如何緩解？</li><li>GELU 為何在 Transformer 中表現佳？</li></ol><hr><h2 id=結語>結語</h2><p>感知機與 MLP 是深度學習的起點。熟悉激活函數演進、參數量與表達能力，能讓你在後續 CNN、RNN、Transformer 等進階主題中打下堅實基礎。下一章將進入卷積網路精要，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>ReLU</span>
<span class=tag>GELU</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>