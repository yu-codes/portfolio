<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>數值穩定技巧全攻略：Log-Sum-Exp、Gradient Clipping、混合精度與防呆實戰 - Yu's Portfolio & Learning Hub</title><meta name=description content='數值穩定性是深度學習訓練與推論不可忽視的細節。從 Log-Sum-Exp、Softmax underflow 防呆，到 Gradient Clipping、FP16/BF16 混合精度，這些技巧能有效避免爆炸、崩潰與精度損失。本章將深入原理、實作、面試熱點與常見誤區，幫助你打造穩健的訓練流程。
Log-Sum-Exp、Softmax underflow 防呆 Log-Sum-Exp Trick 避免 $\exp(x)$ 爆炸或下溢，提升 softmax、log likelihood 計算穩定性 公式：$\log \sum_i e^{x_i} = a + \log \sum_i e^{x_i - a}$，其中 $a = \max(x_i)$ import torch def log_sum_exp(x): a = torch.max(x) return a + torch.log(torch.sum(torch.exp(x - a))) x = torch.tensor([1000.0, 1001.0, 1002.0]) print("穩定計算:", log_sum_exp(x)) Softmax underflow 防呆 Softmax 前先減去最大值，避免指數下溢/溢出 def stable_softmax(x): x = x - torch.max(x) return torch.exp(x) / torch.sum(torch.exp(x)) Gradient Clipping：Value vs. Norm 防止梯度爆炸，將梯度限制在指定範圍 Value Clipping：直接裁剪每個梯度值 Norm Clipping：裁剪整體梯度 L2 範數 import torch.nn.utils as utils # Norm Clipping utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Value Clipping utils.clip_grad_value_(model.parameters(), clip_value=0.5) FP16 / BF16 混合精度踩坑筆記 混合精度訓練（AMP） 結合 float16/bfloat16 與 float32，提升速度、降低顯存 需注意溢出、下溢與數值精度損失 import torch scaler = torch.cuda.amp.GradScaler() for data, target in dataloader: optimizer.zero_grad() with torch.cuda.amp.autocast(): output = model(data) loss = loss_fn(output, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() 常見陷阱 FP16 下溢導致 loss 變 0 或 NaN 梯度未縮放導致溢出 部分運算（如 softmax、log）需保留 float32 理論直覺、應用場景與常見誤區 應用場景 大模型訓練、長序列 Transformer、資源有限設備 需高效訓練與推論的場景 常見誤區 忽略數值穩定性，導致 loss 爆炸或梯度消失 混合精度未檢查溢出/下溢 Gradient Clipping 設置過小，影響收斂 面試熱點與經典問題 主題 常見問題 Log-Sum-Exp 原理與數值優勢？ Softmax 如何防止 underflow/overflow？ Gradient Clipping Value vs Norm 差異？ 混合精度 FP16/BF16 優缺點？ 數值不穩定 常見來源與解法？ 使用注意事項 Softmax、log 運算建議用穩定實作 混合精度需監控 loss 與梯度，必要時回退 Gradient Clipping 建議搭配深層網路與 RNN 延伸閱讀與資源 PyTorch AMP 官方文件 Gradient Clipping 官方文件 數值穩定性與深度學習 經典面試題與解法提示 Log-Sum-Exp Trick 數學推導？ Softmax underflow/overflow 如何防呆？ Gradient Clipping Value vs Norm 差異？ FP16/BF16 混合精度優缺點？ 混合精度訓練常見陷阱？ 如何用 Python 實作數值穩定 softmax？ Gradient Clipping 參數設置原則？ 數值不穩定時如何 debug？ 混合精度下哪些運算需保留 float32？ 數值穩定性對模型訓練有何影響？ 結語 數值穩定技巧是深度學習訓練不可或缺的保障。熟悉 Log-Sum-Exp、Gradient Clipping、混合精度與防呆實戰，能讓你打造更穩健高效的模型。下一章將進入資料增強與合成，敬請期待！
'><meta property="og:title" content="數值穩定技巧全攻略：Log-Sum-Exp、Gradient Clipping、混合精度與防呆實戰"><meta property="og:description" content='數值穩定性是深度學習訓練與推論不可忽視的細節。從 Log-Sum-Exp、Softmax underflow 防呆，到 Gradient Clipping、FP16/BF16 混合精度，這些技巧能有效避免爆炸、崩潰與精度損失。本章將深入原理、實作、面試熱點與常見誤區，幫助你打造穩健的訓練流程。
Log-Sum-Exp、Softmax underflow 防呆 Log-Sum-Exp Trick 避免 $\exp(x)$ 爆炸或下溢，提升 softmax、log likelihood 計算穩定性 公式：$\log \sum_i e^{x_i} = a + \log \sum_i e^{x_i - a}$，其中 $a = \max(x_i)$ import torch def log_sum_exp(x): a = torch.max(x) return a + torch.log(torch.sum(torch.exp(x - a))) x = torch.tensor([1000.0, 1001.0, 1002.0]) print("穩定計算:", log_sum_exp(x)) Softmax underflow 防呆 Softmax 前先減去最大值，避免指數下溢/溢出 def stable_softmax(x): x = x - torch.max(x) return torch.exp(x) / torch.sum(torch.exp(x)) Gradient Clipping：Value vs. Norm 防止梯度爆炸，將梯度限制在指定範圍 Value Clipping：直接裁剪每個梯度值 Norm Clipping：裁剪整體梯度 L2 範數 import torch.nn.utils as utils # Norm Clipping utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Value Clipping utils.clip_grad_value_(model.parameters(), clip_value=0.5) FP16 / BF16 混合精度踩坑筆記 混合精度訓練（AMP） 結合 float16/bfloat16 與 float32，提升速度、降低顯存 需注意溢出、下溢與數值精度損失 import torch scaler = torch.cuda.amp.GradScaler() for data, target in dataloader: optimizer.zero_grad() with torch.cuda.amp.autocast(): output = model(data) loss = loss_fn(output, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() 常見陷阱 FP16 下溢導致 loss 變 0 或 NaN 梯度未縮放導致溢出 部分運算（如 softmax、log）需保留 float32 理論直覺、應用場景與常見誤區 應用場景 大模型訓練、長序列 Transformer、資源有限設備 需高效訓練與推論的場景 常見誤區 忽略數值穩定性，導致 loss 爆炸或梯度消失 混合精度未檢查溢出/下溢 Gradient Clipping 設置過小，影響收斂 面試熱點與經典問題 主題 常見問題 Log-Sum-Exp 原理與數值優勢？ Softmax 如何防止 underflow/overflow？ Gradient Clipping Value vs Norm 差異？ 混合精度 FP16/BF16 優缺點？ 數值不穩定 常見來源與解法？ 使用注意事項 Softmax、log 運算建議用穩定實作 混合精度需監控 loss 與梯度，必要時回退 Gradient Clipping 建議搭配深層網路與 RNN 延伸閱讀與資源 PyTorch AMP 官方文件 Gradient Clipping 官方文件 數值穩定性與深度學習 經典面試題與解法提示 Log-Sum-Exp Trick 數學推導？ Softmax underflow/overflow 如何防呆？ Gradient Clipping Value vs Norm 差異？ FP16/BF16 混合精度優缺點？ 混合精度訓練常見陷阱？ 如何用 Python 實作數值穩定 softmax？ Gradient Clipping 參數設置原則？ 數值不穩定時如何 debug？ 混合精度下哪些運算需保留 float32？ 數值穩定性對模型訓練有何影響？ 結語 數值穩定技巧是深度學習訓練不可或缺的保障。熟悉 Log-Sum-Exp、Gradient Clipping、混合精度與防呆實戰，能讓你打造更穩健高效的模型。下一章將進入資料增強與合成，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/numerical-stability/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/numerical-stability/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>數值穩定技巧全攻略：Log-Sum-Exp、Gradient Clipping、混合精度與防呆實戰</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>數值穩定技巧全攻略：Log-Sum-Exp、Gradient Clipping、混合精度與防呆實戰</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-10-03</span></div></header><div class=article-body><p>數值穩定性是深度學習訓練與推論不可忽視的細節。從 Log-Sum-Exp、Softmax underflow 防呆，到 Gradient Clipping、FP16/BF16 混合精度，這些技巧能有效避免爆炸、崩潰與精度損失。本章將深入原理、實作、面試熱點與常見誤區，幫助你打造穩健的訓練流程。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#log-sum-expsoftmax-underflow-防呆>Log-Sum-Exp、Softmax underflow 防呆</a><ul><li><a href=#log-sum-exp-trick>Log-Sum-Exp Trick</a></li><li><a href=#softmax-underflow-防呆>Softmax underflow 防呆</a></li></ul></li><li><a href=#gradient-clippingvalue-vs-norm>Gradient Clipping：Value vs. Norm</a></li><li><a href=#fp16--bf16-混合精度踩坑筆記>FP16 / BF16 混合精度踩坑筆記</a><ul><li><a href=#混合精度訓練amp>混合精度訓練（AMP）</a></li><li><a href=#常見陷阱>常見陷阱</a></li></ul></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=log-sum-expsoftmax-underflow-防呆>Log-Sum-Exp、Softmax underflow 防呆</h2><h3 id=log-sum-exp-trick>Log-Sum-Exp Trick</h3><ul><li>避免 $\exp(x)$ 爆炸或下溢，提升 softmax、log likelihood 計算穩定性</li><li>公式：$\log \sum_i e^{x_i} = a + \log \sum_i e^{x_i - a}$，其中 $a = \max(x_i)$</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>log_sum_exp</span>(x):
</span></span><span style=display:flex><span>    a <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>max(x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> a <span style=color:#f92672>+</span> torch<span style=color:#f92672>.</span>log(torch<span style=color:#f92672>.</span>sum(torch<span style=color:#f92672>.</span>exp(x <span style=color:#f92672>-</span> a)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1000.0</span>, <span style=color:#ae81ff>1001.0</span>, <span style=color:#ae81ff>1002.0</span>])
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;穩定計算:&#34;</span>, log_sum_exp(x))
</span></span></code></pre></div><h3 id=softmax-underflow-防呆>Softmax underflow 防呆</h3><ul><li>Softmax 前先減去最大值，避免指數下溢/溢出</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>stable_softmax</span>(x):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> x <span style=color:#f92672>-</span> torch<span style=color:#f92672>.</span>max(x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>exp(x) <span style=color:#f92672>/</span> torch<span style=color:#f92672>.</span>sum(torch<span style=color:#f92672>.</span>exp(x))
</span></span></code></pre></div><hr><h2 id=gradient-clippingvalue-vs-norm>Gradient Clipping：Value vs. Norm</h2><ul><li>防止梯度爆炸，將梯度限制在指定範圍</li><li>Value Clipping：直接裁剪每個梯度值</li><li>Norm Clipping：裁剪整體梯度 L2 範數</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.utils <span style=color:#66d9ef>as</span> utils
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Norm Clipping</span>
</span></span><span style=display:flex><span>utils<span style=color:#f92672>.</span>clip_grad_norm_(model<span style=color:#f92672>.</span>parameters(), max_norm<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Value Clipping</span>
</span></span><span style=display:flex><span>utils<span style=color:#f92672>.</span>clip_grad_value_(model<span style=color:#f92672>.</span>parameters(), clip_value<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)
</span></span></code></pre></div><hr><h2 id=fp16--bf16-混合精度踩坑筆記>FP16 / BF16 混合精度踩坑筆記</h2><h3 id=混合精度訓練amp>混合精度訓練（AMP）</h3><ul><li>結合 float16/bfloat16 與 float32，提升速度、降低顯存</li><li>需注意溢出、下溢與數值精度損失</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>scaler <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>amp<span style=color:#f92672>.</span>GradScaler()
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> data, target <span style=color:#f92672>in</span> dataloader:
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>amp<span style=color:#f92672>.</span>autocast():
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> model(data)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> loss_fn(output, target)
</span></span><span style=display:flex><span>    scaler<span style=color:#f92672>.</span>scale(loss)<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    scaler<span style=color:#f92672>.</span>step(optimizer)
</span></span><span style=display:flex><span>    scaler<span style=color:#f92672>.</span>update()
</span></span></code></pre></div><h3 id=常見陷阱>常見陷阱</h3><ul><li>FP16 下溢導致 loss 變 0 或 NaN</li><li>梯度未縮放導致溢出</li><li>部分運算（如 softmax、log）需保留 float32</li></ul><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>大模型訓練、長序列 Transformer、資源有限設備</li><li>需高效訓練與推論的場景</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>忽略數值穩定性，導致 loss 爆炸或梯度消失</li><li>混合精度未檢查溢出/下溢</li><li>Gradient Clipping 設置過小，影響收斂</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>Log-Sum-Exp</td><td>原理與數值優勢？</td></tr><tr><td>Softmax</td><td>如何防止 underflow/overflow？</td></tr><tr><td>Gradient Clipping</td><td>Value vs Norm 差異？</td></tr><tr><td>混合精度</td><td>FP16/BF16 優缺點？</td></tr><tr><td>數值不穩定</td><td>常見來源與解法？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>Softmax、log 運算建議用穩定實作</li><li>混合精度需監控 loss 與梯度，必要時回退</li><li>Gradient Clipping 建議搭配深層網路與 RNN</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://pytorch.org/docs/stable/amp.html>PyTorch AMP 官方文件</a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html>Gradient Clipping 官方文件</a></li><li><a href=https://www.deeplearningbook.org/contents/numerical.html>數值穩定性與深度學習</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>Log-Sum-Exp Trick 數學推導？</li><li>Softmax underflow/overflow 如何防呆？</li><li>Gradient Clipping Value vs Norm 差異？</li><li>FP16/BF16 混合精度優缺點？</li><li>混合精度訓練常見陷阱？</li><li>如何用 Python 實作數值穩定 softmax？</li><li>Gradient Clipping 參數設置原則？</li><li>數值不穩定時如何 debug？</li><li>混合精度下哪些運算需保留 float32？</li><li>數值穩定性對模型訓練有何影響？</li></ol><hr><h2 id=結語>結語</h2><p>數值穩定技巧是深度學習訓練不可或缺的保障。熟悉 Log-Sum-Exp、Gradient Clipping、混合精度與防呆實戰，能讓你打造更穩健高效的模型。下一章將進入資料增強與合成，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>Log-Sum-Exp</span>
<span class=tag>Softmax</span>
<span class=tag>Gradient Clipping</span>
<span class=tag>FP16</span>
<span class=tag>BF16</span>
<span class=tag>Underflow</span>
<span class=tag>Overflow</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>