<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>損失函數百寶箱：回歸、分類、對比學習與自訂 Loss 全解析 - Yu's Portfolio & Learning Hub</title><meta name=description content='損失函數（Loss Function）是模型訓練的核心，直接影響收斂速度、泛化能力與最終表現。本章將深入回歸、分類、對比學習常用損失，並介紹自訂 Loss 的可導性與穩定性設計，結合理論、實作、面試熱點與常見誤區，幫助你全面掌握損失函數設計。
Regression：MSE／MAE／Huber 均方誤差（MSE, Mean Squared Error） $MSE = \frac{1}{n} \sum (y_i - \hat{y}_i)^2$ 對離群值敏感，常用於回歸任務 平均絕對誤差（MAE, Mean Absolute Error） $MAE = \frac{1}{n} \sum |y_i - \hat{y}_i|$ 對離群值不敏感，梯度不連續 Huber Loss 結合 MSE 與 MAE，對小誤差用 MSE，大誤差用 MAE 更穩健於離群值 import torch import torch.nn as nn mse = nn.MSELoss() mae = nn.L1Loss() huber = nn.HuberLoss() y_true = torch.tensor([1.0, 2.0, 3.0]) y_pred = torch.tensor([1.2, 1.9, 2.7]) print("MSE:", mse(y_pred, y_true).item()) print("MAE:", mae(y_pred, y_true).item()) print("Huber:", huber(y_pred, y_true).item()) Classification：Cross-Entropy, Focal Loss, Label Smoothing Cross-Entropy Loss 多分類標配，衡量預測分布與真實分布的距離 適合 one-hot 或機率標籤 Focal Loss 解決類別不平衡，對難分樣本加大懲罰 常用於目標檢測、醫療影像 Label Smoothing 將 one-hot 標籤平滑，降低模型過度自信 import torch.nn.functional as F logits = torch.tensor([[2.0, 0.5, 0.1]]) labels = torch.tensor([0]) ce = F.cross_entropy(logits, labels) print("Cross-Entropy:", ce.item()) # Focal Loss 需自訂實作 Triplet / Contrastive / InfoNCE Triplet Loss 使 anchor 與 positive 距離小於 anchor 與 negative 常用於人臉辨識、度量學習 Contrastive Loss 拉近正對、推遠負對，適合自監督學習 InfoNCE 自監督對比學習標配，最大化正對 mutual information # Triplet Loss (PyTorch) triplet = nn.TripletMarginLoss(margin=1.0) anchor = torch.randn(5, 10) positive = torch.randn(5, 10) negative = torch.randn(5, 10) print("Triplet Loss:", triplet(anchor, positive, negative).item()) 自訂 Loss：梯度可導性與穩定性 自訂 Loss 需確保對參數可導，否則無法反向傳播 避免使用不可微函數（如 hard threshold） 建議用平滑近似（如 softmax, sigmoid）提升穩定性 注意數值穩定（如 log, exp 下溢/溢出） # 自訂 Loss 範例 def custom_loss(y_pred, y_true): diff = y_pred - y_true return torch.mean(torch.sqrt(diff ** 2 + 1e-6)) 理論直覺、應用場景與常見誤區 應用場景 MSE/MAE/Huber：回歸、異常偵測 Cross-Entropy/Focal：分類、目標檢測 Triplet/Contrastive/InfoNCE：度量學習、自監督、檢索 常見誤區 分類任務誤用 MSE，導致收斂慢 自訂 Loss 未考慮可導性，反向傳播失敗 Focal Loss 參數設置不當，模型難以收斂 面試熱點與經典問題 主題 常見問題 MSE vs MAE 何時選用？對離群值敏感度？ Huber Loss 為何更穩健？ Cross-Entropy 數學推導與應用？ Focal Loss 如何解決類別不平衡？ Triplet/Contrastive 適用場景與數學原理？ 自訂 Loss 如何設計可導且穩定？ 使用注意事項 損失函數選擇需根據任務與資料特性 自訂 Loss 建議先理論推導再實作 注意數值穩定與梯度可導性 延伸閱讀與資源 PyTorch Loss Functions Focal Loss 論文 InfoNCE 論文 Triplet Loss 論文 經典面試題與解法提示 MSE、MAE、Huber Loss 差異與適用場景？ Cross-Entropy Loss 數學推導？ Focal Loss 如何設計與調參？ Triplet/Contrastive/InfoNCE 適用場景？ 自訂 Loss 如何確保可導與穩定？ 分類任務誤用 MSE 有何後果？ 如何用 Python 實作自訂 Loss？ Focal Loss 參數設置原則？ InfoNCE 在自監督學習的作用？ Loss 數值不穩定時如何 debug？ 結語 損失函數設計是模型訓練的基石。熟悉回歸、分類、對比學習與自訂 Loss，能讓你在各類任務與面試中展現專業素養。下一章將進入梯度下降家譜，敬請期待！
'><meta property="og:title" content="損失函數百寶箱：回歸、分類、對比學習與自訂 Loss 全解析"><meta property="og:description" content='損失函數（Loss Function）是模型訓練的核心，直接影響收斂速度、泛化能力與最終表現。本章將深入回歸、分類、對比學習常用損失，並介紹自訂 Loss 的可導性與穩定性設計，結合理論、實作、面試熱點與常見誤區，幫助你全面掌握損失函數設計。
Regression：MSE／MAE／Huber 均方誤差（MSE, Mean Squared Error） $MSE = \frac{1}{n} \sum (y_i - \hat{y}_i)^2$ 對離群值敏感，常用於回歸任務 平均絕對誤差（MAE, Mean Absolute Error） $MAE = \frac{1}{n} \sum |y_i - \hat{y}_i|$ 對離群值不敏感，梯度不連續 Huber Loss 結合 MSE 與 MAE，對小誤差用 MSE，大誤差用 MAE 更穩健於離群值 import torch import torch.nn as nn mse = nn.MSELoss() mae = nn.L1Loss() huber = nn.HuberLoss() y_true = torch.tensor([1.0, 2.0, 3.0]) y_pred = torch.tensor([1.2, 1.9, 2.7]) print("MSE:", mse(y_pred, y_true).item()) print("MAE:", mae(y_pred, y_true).item()) print("Huber:", huber(y_pred, y_true).item()) Classification：Cross-Entropy, Focal Loss, Label Smoothing Cross-Entropy Loss 多分類標配，衡量預測分布與真實分布的距離 適合 one-hot 或機率標籤 Focal Loss 解決類別不平衡，對難分樣本加大懲罰 常用於目標檢測、醫療影像 Label Smoothing 將 one-hot 標籤平滑，降低模型過度自信 import torch.nn.functional as F logits = torch.tensor([[2.0, 0.5, 0.1]]) labels = torch.tensor([0]) ce = F.cross_entropy(logits, labels) print("Cross-Entropy:", ce.item()) # Focal Loss 需自訂實作 Triplet / Contrastive / InfoNCE Triplet Loss 使 anchor 與 positive 距離小於 anchor 與 negative 常用於人臉辨識、度量學習 Contrastive Loss 拉近正對、推遠負對，適合自監督學習 InfoNCE 自監督對比學習標配，最大化正對 mutual information # Triplet Loss (PyTorch) triplet = nn.TripletMarginLoss(margin=1.0) anchor = torch.randn(5, 10) positive = torch.randn(5, 10) negative = torch.randn(5, 10) print("Triplet Loss:", triplet(anchor, positive, negative).item()) 自訂 Loss：梯度可導性與穩定性 自訂 Loss 需確保對參數可導，否則無法反向傳播 避免使用不可微函數（如 hard threshold） 建議用平滑近似（如 softmax, sigmoid）提升穩定性 注意數值穩定（如 log, exp 下溢/溢出） # 自訂 Loss 範例 def custom_loss(y_pred, y_true): diff = y_pred - y_true return torch.mean(torch.sqrt(diff ** 2 + 1e-6)) 理論直覺、應用場景與常見誤區 應用場景 MSE/MAE/Huber：回歸、異常偵測 Cross-Entropy/Focal：分類、目標檢測 Triplet/Contrastive/InfoNCE：度量學習、自監督、檢索 常見誤區 分類任務誤用 MSE，導致收斂慢 自訂 Loss 未考慮可導性，反向傳播失敗 Focal Loss 參數設置不當，模型難以收斂 面試熱點與經典問題 主題 常見問題 MSE vs MAE 何時選用？對離群值敏感度？ Huber Loss 為何更穩健？ Cross-Entropy 數學推導與應用？ Focal Loss 如何解決類別不平衡？ Triplet/Contrastive 適用場景與數學原理？ 自訂 Loss 如何設計可導且穩定？ 使用注意事項 損失函數選擇需根據任務與資料特性 自訂 Loss 建議先理論推導再實作 注意數值穩定與梯度可導性 延伸閱讀與資源 PyTorch Loss Functions Focal Loss 論文 InfoNCE 論文 Triplet Loss 論文 經典面試題與解法提示 MSE、MAE、Huber Loss 差異與適用場景？ Cross-Entropy Loss 數學推導？ Focal Loss 如何設計與調參？ Triplet/Contrastive/InfoNCE 適用場景？ 自訂 Loss 如何確保可導與穩定？ 分類任務誤用 MSE 有何後果？ 如何用 Python 實作自訂 Loss？ Focal Loss 參數設置原則？ InfoNCE 在自監督學習的作用？ Loss 數值不穩定時如何 debug？ 結語 損失函數設計是模型訓練的基石。熟悉回歸、分類、對比學習與自訂 Loss，能讓你在各類任務與面試中展現專業素養。下一章將進入梯度下降家譜，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/loss-functions/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/loss-functions/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>損失函數百寶箱：回歸、分類、對比學習與自訂 Loss 全解析</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>損失函數百寶箱：回歸、分類、對比學習與自訂 Loss 全解析</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-02-14</span></div></header><div class=article-body><p>損失函數（Loss Function）是模型訓練的核心，直接影響收斂速度、泛化能力與最終表現。本章將深入回歸、分類、對比學習常用損失，並介紹自訂 Loss 的可導性與穩定性設計，結合理論、實作、面試熱點與常見誤區，幫助你全面掌握損失函數設計。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#regressionmsemaehuber>Regression：MSE／MAE／Huber</a><ul><li><a href=#均方誤差mse-mean-squared-error>均方誤差（MSE, Mean Squared Error）</a></li><li><a href=#平均絕對誤差mae-mean-absolute-error>平均絕對誤差（MAE, Mean Absolute Error）</a></li><li><a href=#huber-loss>Huber Loss</a></li></ul></li><li><a href=#classificationcross-entropy-focal-loss-label-smoothing>Classification：Cross-Entropy, Focal Loss, Label Smoothing</a><ul><li><a href=#cross-entropy-loss>Cross-Entropy Loss</a></li><li><a href=#focal-loss>Focal Loss</a></li><li><a href=#label-smoothing>Label Smoothing</a></li></ul></li><li><a href=#triplet--contrastive--infonce>Triplet / Contrastive / InfoNCE</a><ul><li><a href=#triplet-loss>Triplet Loss</a></li><li><a href=#contrastive-loss>Contrastive Loss</a></li><li><a href=#infonce>InfoNCE</a></li></ul></li><li><a href=#自訂-loss梯度可導性與穩定性>自訂 Loss：梯度可導性與穩定性</a></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=regressionmsemaehuber>Regression：MSE／MAE／Huber</h2><h3 id=均方誤差mse-mean-squared-error>均方誤差（MSE, Mean Squared Error）</h3><ul><li>$MSE = \frac{1}{n} \sum (y_i - \hat{y}_i)^2$</li><li>對離群值敏感，常用於回歸任務</li></ul><h3 id=平均絕對誤差mae-mean-absolute-error>平均絕對誤差（MAE, Mean Absolute Error）</h3><ul><li>$MAE = \frac{1}{n} \sum |y_i - \hat{y}_i|$</li><li>對離群值不敏感，梯度不連續</li></ul><h3 id=huber-loss>Huber Loss</h3><ul><li>結合 MSE 與 MAE，對小誤差用 MSE，大誤差用 MAE</li><li>更穩健於離群值</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mse <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span><span style=display:flex><span>mae <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>L1Loss()
</span></span><span style=display:flex><span>huber <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>HuberLoss()
</span></span><span style=display:flex><span>y_true <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>])
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1.2</span>, <span style=color:#ae81ff>1.9</span>, <span style=color:#ae81ff>2.7</span>])
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;MSE:&#34;</span>, mse(y_pred, y_true)<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;MAE:&#34;</span>, mae(y_pred, y_true)<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Huber:&#34;</span>, huber(y_pred, y_true)<span style=color:#f92672>.</span>item())
</span></span></code></pre></div><hr><h2 id=classificationcross-entropy-focal-loss-label-smoothing>Classification：Cross-Entropy, Focal Loss, Label Smoothing</h2><h3 id=cross-entropy-loss>Cross-Entropy Loss</h3><ul><li>多分類標配，衡量預測分布與真實分布的距離</li><li>適合 one-hot 或機率標籤</li></ul><h3 id=focal-loss>Focal Loss</h3><ul><li>解決類別不平衡，對難分樣本加大懲罰</li><li>常用於目標檢測、醫療影像</li></ul><h3 id=label-smoothing>Label Smoothing</h3><ul><li>將 one-hot 標籤平滑，降低模型過度自信</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.1</span>]])
</span></span><span style=display:flex><span>labels <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>ce <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>cross_entropy(logits, labels)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Cross-Entropy:&#34;</span>, ce<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span><span style=color:#75715e># Focal Loss 需自訂實作</span>
</span></span></code></pre></div><hr><h2 id=triplet--contrastive--infonce>Triplet / Contrastive / InfoNCE</h2><h3 id=triplet-loss>Triplet Loss</h3><ul><li>使 anchor 與 positive 距離小於 anchor 與 negative</li><li>常用於人臉辨識、度量學習</li></ul><h3 id=contrastive-loss>Contrastive Loss</h3><ul><li>拉近正對、推遠負對，適合自監督學習</li></ul><h3 id=infonce>InfoNCE</h3><ul><li>自監督對比學習標配，最大化正對 mutual information</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Triplet Loss (PyTorch)</span>
</span></span><span style=display:flex><span>triplet <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>TripletMarginLoss(margin<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>anchor <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>positive <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>negative <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Triplet Loss:&#34;</span>, triplet(anchor, positive, negative)<span style=color:#f92672>.</span>item())
</span></span></code></pre></div><hr><h2 id=自訂-loss梯度可導性與穩定性>自訂 Loss：梯度可導性與穩定性</h2><ul><li>自訂 Loss 需確保對參數可導，否則無法反向傳播</li><li>避免使用不可微函數（如 hard threshold）</li><li>建議用平滑近似（如 softmax, sigmoid）提升穩定性</li><li>注意數值穩定（如 log, exp 下溢/溢出）</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 自訂 Loss 範例</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>custom_loss</span>(y_pred, y_true):
</span></span><span style=display:flex><span>    diff <span style=color:#f92672>=</span> y_pred <span style=color:#f92672>-</span> y_true
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>mean(torch<span style=color:#f92672>.</span>sqrt(diff <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-6</span>))
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>MSE/MAE/Huber：回歸、異常偵測</li><li>Cross-Entropy/Focal：分類、目標檢測</li><li>Triplet/Contrastive/InfoNCE：度量學習、自監督、檢索</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>分類任務誤用 MSE，導致收斂慢</li><li>自訂 Loss 未考慮可導性，反向傳播失敗</li><li>Focal Loss 參數設置不當，模型難以收斂</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>MSE vs MAE</td><td>何時選用？對離群值敏感度？</td></tr><tr><td>Huber Loss</td><td>為何更穩健？</td></tr><tr><td>Cross-Entropy</td><td>數學推導與應用？</td></tr><tr><td>Focal Loss</td><td>如何解決類別不平衡？</td></tr><tr><td>Triplet/Contrastive</td><td>適用場景與數學原理？</td></tr><tr><td>自訂 Loss</td><td>如何設計可導且穩定？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>損失函數選擇需根據任務與資料特性</li><li>自訂 Loss 建議先理論推導再實作</li><li>注意數值穩定與梯度可導性</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://pytorch.org/docs/stable/nn.html#loss-functions>PyTorch Loss Functions</a></li><li><a href=https://arxiv.org/abs/1708.02002>Focal Loss 論文</a></li><li><a href=https://arxiv.org/abs/1807.03748>InfoNCE 論文</a></li><li><a href=https://arxiv.org/abs/1503.03832>Triplet Loss 論文</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>MSE、MAE、Huber Loss 差異與適用場景？</li><li>Cross-Entropy Loss 數學推導？</li><li>Focal Loss 如何設計與調參？</li><li>Triplet/Contrastive/InfoNCE 適用場景？</li><li>自訂 Loss 如何確保可導與穩定？</li><li>分類任務誤用 MSE 有何後果？</li><li>如何用 Python 實作自訂 Loss？</li><li>Focal Loss 參數設置原則？</li><li>InfoNCE 在自監督學習的作用？</li><li>Loss 數值不穩定時如何 debug？</li></ol><hr><h2 id=結語>結語</h2><p>損失函數設計是模型訓練的基石。熟悉回歸、分類、對比學習與自訂 Loss，能讓你在各類任務與面試中展現專業素養。下一章將進入梯度下降家譜，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>MSE</span>
<span class=tag>MAE</span>
<span class=tag>Huber</span>
<span class=tag>Cross-Entropy</span>
<span class=tag>Focal Loss</span>
<span class=tag>Triplet</span>
<span class=tag>Contrastive</span>
<span class=tag>InfoNCE</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>