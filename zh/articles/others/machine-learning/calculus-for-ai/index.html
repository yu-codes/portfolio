<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>微積分與連鎖法則：AI 必備的微分直覺與應用 - Yu's Portfolio & Learning Hub</title><meta name=description content='微積分是機器學習與深度學習的數學核心。從模型訓練的梯度下降，到神經網路的反向傳播，背後都離不開導數、偏導與連鎖法則。本篇將帶你掌握 AI 常用的微積分觀念，並以直覺、圖解與 Python 範例說明。
極限、導數、偏導與梯度 極限（Limit） 描述函數在某點附近的趨勢，是導數與連續性的基礎。 訓練過程中，損失函數收斂本質上就是極限的應用。 導數（Derivative） 表示函數變化率，記作 $f&rsquo;(x)$ 或 $\frac{df}{dx}$。 在機器學習中，導數用於描述損失函數對參數的敏感度。 偏導數（Partial Derivative） 多變數函數對單一變數的導數，記作 $\frac{\partial f}{\partial x}$。 神經網路每個權重的梯度即為偏導數。 梯度（Gradient） 所有偏導數組成的向量，指向函數上升最快的方向。 在優化中，梯度用於指引參數更新方向。 import numpy as np # 單變數導數 def f(x): return x**2 + 3*x + 2 x = 1.0 h = 1e-5 df = (f(x + h) - f(x)) / h print("f&#39;(1) ≈", df) # 多變數偏導與梯度 def g(x, y): return x**2 + y**2 x, y = 1.0, 2.0 df_dx = (g(x + h, y) - g(x, y)) / h df_dy = (g(x, y + h) - g(x, y)) / h print("∂g/∂x ≈", df_dx) print("∂g/∂y ≈", df_dy) Jacobian 與高維導數 Jacobian 矩陣：多輸入多輸出函數的偏導數矩陣，常見於神經網路的層間傳遞。 在自動微分與反向傳播中，Jacobian 是計算梯度的基礎。 概念 直覺說明 應用 導數 單變數變化率 線性回歸、損失函數 偏導 多變數單方向變化率 神經網路權重更新 梯度 所有偏導組成向量 最佳化、梯度下降 Jacobian 多輸入多輸出偏導 反向傳播、複合模型 連鎖法則在反向傳播的角色 連鎖法則（Chain Rule）：複合函數的導數計算法則，核心公式： $$ \frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx} $$ 在神經網路反向傳播（Backpropagation）中，連鎖法則用於將損失對輸出層的梯度逐層傳回輸入層。 # 連鎖法則範例：z = f(y), y = g(x) def g(x): return 2 * x def f(y): return y ** 3 x = 1.5 y = g(x) z = f(y) # 手動計算 dz_dy = 3 * y ** 2 dy_dx = 2 dz_dx = dz_dy * dy_dx print("dz/dx =", dz_dx) 泰勒展開與損失曲面直覺 泰勒展開（Taylor Expansion）：用多項式近似函數，理解損失曲面形狀與優化步伐。 在優化中，泰勒展開幫助我們預測損失變化，設計更有效的學習率與步長。 例如，二階泰勒展開可用於牛頓法（Newton&rsquo;s Method）等高級優化演算法。
'><meta property="og:title" content="微積分與連鎖法則：AI 必備的微分直覺與應用"><meta property="og:description" content='微積分是機器學習與深度學習的數學核心。從模型訓練的梯度下降，到神經網路的反向傳播，背後都離不開導數、偏導與連鎖法則。本篇將帶你掌握 AI 常用的微積分觀念，並以直覺、圖解與 Python 範例說明。
極限、導數、偏導與梯度 極限（Limit） 描述函數在某點附近的趨勢，是導數與連續性的基礎。 訓練過程中，損失函數收斂本質上就是極限的應用。 導數（Derivative） 表示函數變化率，記作 $f&rsquo;(x)$ 或 $\frac{df}{dx}$。 在機器學習中，導數用於描述損失函數對參數的敏感度。 偏導數（Partial Derivative） 多變數函數對單一變數的導數，記作 $\frac{\partial f}{\partial x}$。 神經網路每個權重的梯度即為偏導數。 梯度（Gradient） 所有偏導數組成的向量，指向函數上升最快的方向。 在優化中，梯度用於指引參數更新方向。 import numpy as np # 單變數導數 def f(x): return x**2 + 3*x + 2 x = 1.0 h = 1e-5 df = (f(x + h) - f(x)) / h print("f&#39;(1) ≈", df) # 多變數偏導與梯度 def g(x, y): return x**2 + y**2 x, y = 1.0, 2.0 df_dx = (g(x + h, y) - g(x, y)) / h df_dy = (g(x, y + h) - g(x, y)) / h print("∂g/∂x ≈", df_dx) print("∂g/∂y ≈", df_dy) Jacobian 與高維導數 Jacobian 矩陣：多輸入多輸出函數的偏導數矩陣，常見於神經網路的層間傳遞。 在自動微分與反向傳播中，Jacobian 是計算梯度的基礎。 概念 直覺說明 應用 導數 單變數變化率 線性回歸、損失函數 偏導 多變數單方向變化率 神經網路權重更新 梯度 所有偏導組成向量 最佳化、梯度下降 Jacobian 多輸入多輸出偏導 反向傳播、複合模型 連鎖法則在反向傳播的角色 連鎖法則（Chain Rule）：複合函數的導數計算法則，核心公式： $$ \frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx} $$ 在神經網路反向傳播（Backpropagation）中，連鎖法則用於將損失對輸出層的梯度逐層傳回輸入層。 # 連鎖法則範例：z = f(y), y = g(x) def g(x): return 2 * x def f(y): return y ** 3 x = 1.5 y = g(x) z = f(y) # 手動計算 dz_dy = 3 * y ** 2 dy_dx = 2 dz_dx = dz_dy * dy_dx print("dz/dx =", dz_dx) 泰勒展開與損失曲面直覺 泰勒展開（Taylor Expansion）：用多項式近似函數，理解損失曲面形狀與優化步伐。 在優化中，泰勒展開幫助我們預測損失變化，設計更有效的學習率與步長。 例如，二階泰勒展開可用於牛頓法（Newton&rsquo;s Method）等高級優化演算法。
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/calculus-for-ai/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/calculus-for-ai/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>微積分與連鎖法則：AI 必備的微分直覺與應用</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>微積分與連鎖法則：AI 必備的微分直覺與應用</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-08-08</span></div></header><div class=article-body><p>微積分是機器學習與深度學習的數學核心。從模型訓練的梯度下降，到神經網路的反向傳播，背後都離不開導數、偏導與連鎖法則。本篇將帶你掌握 AI 常用的微積分觀念，並以直覺、圖解與 Python 範例說明。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#極限導數偏導與梯度>極限、導數、偏導與梯度</a><ul><li><a href=#極限limit>極限（Limit）</a></li><li><a href=#導數derivative>導數（Derivative）</a></li><li><a href=#偏導數partial-derivative>偏導數（Partial Derivative）</a></li><li><a href=#梯度gradient>梯度（Gradient）</a></li></ul></li><li><a href=#jacobian-與高維導數>Jacobian 與高維導數</a></li><li><a href=#連鎖法則在反向傳播的角色>連鎖法則在反向傳播的角色</a></li><li><a href=#泰勒展開與損失曲面直覺>泰勒展開與損失曲面直覺</a></li><li><a href=#常見面試熱點整理>常見面試熱點整理</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=極限導數偏導與梯度>極限、導數、偏導與梯度</h2><h3 id=極限limit>極限（Limit）</h3><ul><li>描述函數在某點附近的趨勢，是導數與連續性的基礎。</li><li>訓練過程中，損失函數收斂本質上就是極限的應用。</li></ul><h3 id=導數derivative>導數（Derivative）</h3><ul><li>表示函數變化率，記作 $f&rsquo;(x)$ 或 $\frac{df}{dx}$。</li><li>在機器學習中，導數用於描述損失函數對參數的敏感度。</li></ul><h3 id=偏導數partial-derivative>偏導數（Partial Derivative）</h3><ul><li>多變數函數對單一變數的導數，記作 $\frac{\partial f}{\partial x}$。</li><li>神經網路每個權重的梯度即為偏導數。</li></ul><h3 id=梯度gradient>梯度（Gradient）</h3><ul><li>所有偏導數組成的向量，指向函數上升最快的方向。</li><li>在優化中，梯度用於指引參數更新方向。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 單變數導數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>3</span><span style=color:#f92672>*</span>x <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>h <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-5</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> (f(x <span style=color:#f92672>+</span> h) <span style=color:#f92672>-</span> f(x)) <span style=color:#f92672>/</span> h
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;f&#39;(1) ≈&#34;</span>, df)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 多變數偏導與梯度</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>g</span>(x, y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> y<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x, y <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>2.0</span>
</span></span><span style=display:flex><span>df_dx <span style=color:#f92672>=</span> (g(x <span style=color:#f92672>+</span> h, y) <span style=color:#f92672>-</span> g(x, y)) <span style=color:#f92672>/</span> h
</span></span><span style=display:flex><span>df_dy <span style=color:#f92672>=</span> (g(x, y <span style=color:#f92672>+</span> h) <span style=color:#f92672>-</span> g(x, y)) <span style=color:#f92672>/</span> h
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;∂g/∂x ≈&#34;</span>, df_dx)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;∂g/∂y ≈&#34;</span>, df_dy)
</span></span></code></pre></div><hr><h2 id=jacobian-與高維導數>Jacobian 與高維導數</h2><ul><li><strong>Jacobian 矩陣</strong>：多輸入多輸出函數的偏導數矩陣，常見於神經網路的層間傳遞。</li><li>在自動微分與反向傳播中，Jacobian 是計算梯度的基礎。</li></ul><table><thead><tr><th>概念</th><th>直覺說明</th><th>應用</th></tr></thead><tbody><tr><td>導數</td><td>單變數變化率</td><td>線性回歸、損失函數</td></tr><tr><td>偏導</td><td>多變數單方向變化率</td><td>神經網路權重更新</td></tr><tr><td>梯度</td><td>所有偏導組成向量</td><td>最佳化、梯度下降</td></tr><tr><td>Jacobian</td><td>多輸入多輸出偏導</td><td>反向傳播、複合模型</td></tr></tbody></table><hr><h2 id=連鎖法則在反向傳播的角色>連鎖法則在反向傳播的角色</h2><ul><li><strong>連鎖法則（Chain Rule）</strong>：複合函數的導數計算法則，核心公式：
$$
\frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}
$$</li><li>在神經網路反向傳播（Backpropagation）中，連鎖法則用於將損失對輸出層的梯度逐層傳回輸入層。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 連鎖法則範例：z = f(y), y = g(x)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>g</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> y <span style=color:#f92672>**</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.5</span>
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> g(x)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> f(y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 手動計算</span>
</span></span><span style=display:flex><span>dz_dy <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span> <span style=color:#f92672>*</span> y <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>dy_dx <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>dz_dx <span style=color:#f92672>=</span> dz_dy <span style=color:#f92672>*</span> dy_dx
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;dz/dx =&#34;</span>, dz_dx)
</span></span></code></pre></div><hr><h2 id=泰勒展開與損失曲面直覺>泰勒展開與損失曲面直覺</h2><ul><li><strong>泰勒展開（Taylor Expansion）</strong>：用多項式近似函數，理解損失曲面形狀與優化步伐。</li><li>在優化中，泰勒展開幫助我們預測損失變化，設計更有效的學習率與步長。</li></ul><blockquote><p>例如，二階泰勒展開可用於牛頓法（Newton&rsquo;s Method）等高級優化演算法。</p></blockquote><hr><h2 id=常見面試熱點整理>常見面試熱點整理</h2><table><thead><tr><th>熱點主題</th><th>面試常問問題</th></tr></thead><tbody><tr><td>導數/偏導</td><td>如何手算梯度？</td></tr><tr><td>連鎖法則</td><td>反向傳播如何應用連鎖法則？</td></tr><tr><td>Jacobian</td><td>什麼時候需要 Jacobian？</td></tr><tr><td>泰勒展開</td><td>為何優化時要考慮二階導數？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>導數與梯度計算是所有深度學習框架（如 PyTorch、TensorFlow）的底層核心。</li><li>連鎖法則是理解反向傳播的關鍵，建議多做手算練習。</li><li>泰勒展開有助於理解學習率、損失曲面與優化策略。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/>MIT OCW：Calculus for Machine Learning</a></li><li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">3Blue1Brown：微積分動畫</a></li><li><a href=https://pytorch.org/docs/stable/autograd.html>PyTorch Autograd 官方文件</a></li></ul><hr><h2 id=結語>結語</h2><p>微積分讓我們能精確描述模型的變化與學習過程。掌握導數、偏導、連鎖法則與泰勒展開，不僅能幫助你理解 AI 模型訓練的本質，也能在面試與實作中展現數學底蘊。下一章將進入最適化理論，敬請期待！</p></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>