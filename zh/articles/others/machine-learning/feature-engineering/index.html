<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>特徵工程與選擇全攻略：編碼、標準化、特徵選擇三大法門 - Yu's Portfolio & Learning Hub</title><meta name=description content="特徵工程是機器學習成敗的關鍵。從資料前處理、編碼、標準化，到特徵選擇，這些步驟直接影響模型表現與泛化能力。本章將深入 One-Hot/Target/Frequency Encoding、標準化/正規化/Whitening、三大特徵選擇法（Filter/Wrapper/Embedded），結合理論、實作、面試熱點與常見誤區，讓你打造高質量特徵集。
編碼技巧：One-Hot、Target、Frequency Encoding One-Hot Encoding 將類別特徵轉為 0/1 向量，適合無序類別。 缺點：高基數時維度爆炸。 import pandas as pd df = pd.DataFrame({'color': ['red', 'blue', 'green', 'blue']}) onehot = pd.get_dummies(df['color']) print(onehot) Target Encoding 用目標變數的平均值取代類別，適合有序類別或高基數特徵。 需防止資料洩漏，建議用交叉驗證計算。 df['target'] = [1, 0, 1, 0] mean_map = df.groupby('color')['target'].mean() df['color_te'] = df['color'].map(mean_map) print(df[['color', 'color_te']]) Frequency Encoding 用類別出現頻率取代原值，適合高基數特徵。 不引入目標資訊，無洩漏風險。 freq_map = df['color'].value_counts() / len(df) df['color_fe'] = df['color'].map(freq_map) print(df[['color', 'color_fe']]) 標準化 vs. 正規化 vs. Whitening 標準化（Standardization） 轉換為均值 0、標準差 1，適合大多數 ML 演算法。 常用於 SVM、Logistic Regression、神經網路。 from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_scaled = scaler.fit_transform([[1, 2], [3, 4], [5, 6]]) print(X_scaled) 正規化（Normalization） 將特徵縮放到固定範圍（如 0~1），適合距離度量敏感的演算法（如 k-NN）。 常用 MinMaxScaler。 from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() X_norm = scaler.fit_transform([[1, 2], [3, 4], [5, 6]]) print(X_norm) Whitening 進一步去除特徵間相關性，使協方差矩陣為單位矩陣。 常用於 PCA 前處理、深度學習 BatchNorm。 特徵選擇三大法門 Filter 方法 根據統計量（如相關係數、卡方、互資訊）篩選特徵。 優點：計算快、無需模型。 缺點：忽略特徵間交互作用。 from sklearn.feature_selection import SelectKBest, f_classif X = [[1,2,3],[4,5,6],[7,8,9],[1,3,5]] y = [0,1,0,1] skb = SelectKBest(f_classif, k=2).fit(X, y) print(&#34;選中特徵索引:&#34;, skb.get_support(indices=True)) Wrapper 方法 用模型評估特徵子集（如遞迴特徵消除 RFE）。 優點：考慮特徵交互，效果好。 缺點：計算量大。 from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression rfe = RFE(LogisticRegression(), n_features_to_select=2) rfe.fit(X, y) print(&#34;RFE 選中特徵:&#34;, rfe.support_) Embedded 方法 特徵選擇與模型訓練同時進行（如 Lasso、樹模型）。 優點：效率高，常為預設選擇。 from sklearn.linear_model import Lasso lasso = Lasso(alpha=0.1).fit(X, y) print(&#34;Lasso 係數:&#34;, lasso.coef_) 理論直覺、應用場景與常見誤區 應用場景 One-Hot：低基數類別、無序特徵 Target/Frequency Encoding：高基數類別、樞紐分析 標準化/正規化：距離敏感模型、梯度下降 特徵選擇：降維、提升泛化、減少過擬合 常見誤區 One-Hot 用於高基數，導致維度爆炸。 Target Encoding 未用交叉驗證，導致資料洩漏。 標準化與正規化混用，影響模型收斂。 Wrapper 方法過度耗時，未做特徵預篩。 面試熱點與經典問題 主題 常見問題 One-Hot 何時不用？有何缺點？ Target Encoding 如何防止資料洩漏？ 標準化/正規化 差異與適用場景？ Filter/Wrapper/Embedded 各自優缺點？ Lasso 為何能做特徵選擇？ 使用注意事項 特徵工程需結合業務知識與資料探索。 編碼與標準化順序需一致，避免資料洩漏。 特徵選擇建議多法並用，交叉驗證效果。 延伸閱讀與資源 StatQuest: Feature Engineering Scikit-learn Feature Selection Kaggle: Feature Engineering 教程 經典面試題與解法提示 One-Hot Encoding 有哪些缺點？如何解決？ Target Encoding 如何防止資料洩漏？ 標準化與正規化差異？ Filter/Wrapper/Embedded 方法比較？ Lasso 為何能做特徵選擇？ 特徵選擇對模型有何影響？ 如何用 Python 實作特徵選擇？ 特徵工程常見陷阱有哪些？ 如何評估特徵工程效果？ 實務上如何設計特徵工程流程？ 結語 特徵工程與選擇是 ML 成敗關鍵。熟悉各種編碼、標準化與特徵選擇方法，能讓你打造高效能模型並在面試中脫穎而出。下一章將進入模型評估與驗證，敬請期待！
"><meta property="og:title" content="特徵工程與選擇全攻略：編碼、標準化、特徵選擇三大法門"><meta property="og:description" content="特徵工程是機器學習成敗的關鍵。從資料前處理、編碼、標準化，到特徵選擇，這些步驟直接影響模型表現與泛化能力。本章將深入 One-Hot/Target/Frequency Encoding、標準化/正規化/Whitening、三大特徵選擇法（Filter/Wrapper/Embedded），結合理論、實作、面試熱點與常見誤區，讓你打造高質量特徵集。
編碼技巧：One-Hot、Target、Frequency Encoding One-Hot Encoding 將類別特徵轉為 0/1 向量，適合無序類別。 缺點：高基數時維度爆炸。 import pandas as pd df = pd.DataFrame({'color': ['red', 'blue', 'green', 'blue']}) onehot = pd.get_dummies(df['color']) print(onehot) Target Encoding 用目標變數的平均值取代類別，適合有序類別或高基數特徵。 需防止資料洩漏，建議用交叉驗證計算。 df['target'] = [1, 0, 1, 0] mean_map = df.groupby('color')['target'].mean() df['color_te'] = df['color'].map(mean_map) print(df[['color', 'color_te']]) Frequency Encoding 用類別出現頻率取代原值，適合高基數特徵。 不引入目標資訊，無洩漏風險。 freq_map = df['color'].value_counts() / len(df) df['color_fe'] = df['color'].map(freq_map) print(df[['color', 'color_fe']]) 標準化 vs. 正規化 vs. Whitening 標準化（Standardization） 轉換為均值 0、標準差 1，適合大多數 ML 演算法。 常用於 SVM、Logistic Regression、神經網路。 from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_scaled = scaler.fit_transform([[1, 2], [3, 4], [5, 6]]) print(X_scaled) 正規化（Normalization） 將特徵縮放到固定範圍（如 0~1），適合距離度量敏感的演算法（如 k-NN）。 常用 MinMaxScaler。 from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() X_norm = scaler.fit_transform([[1, 2], [3, 4], [5, 6]]) print(X_norm) Whitening 進一步去除特徵間相關性，使協方差矩陣為單位矩陣。 常用於 PCA 前處理、深度學習 BatchNorm。 特徵選擇三大法門 Filter 方法 根據統計量（如相關係數、卡方、互資訊）篩選特徵。 優點：計算快、無需模型。 缺點：忽略特徵間交互作用。 from sklearn.feature_selection import SelectKBest, f_classif X = [[1,2,3],[4,5,6],[7,8,9],[1,3,5]] y = [0,1,0,1] skb = SelectKBest(f_classif, k=2).fit(X, y) print(&#34;選中特徵索引:&#34;, skb.get_support(indices=True)) Wrapper 方法 用模型評估特徵子集（如遞迴特徵消除 RFE）。 優點：考慮特徵交互，效果好。 缺點：計算量大。 from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression rfe = RFE(LogisticRegression(), n_features_to_select=2) rfe.fit(X, y) print(&#34;RFE 選中特徵:&#34;, rfe.support_) Embedded 方法 特徵選擇與模型訓練同時進行（如 Lasso、樹模型）。 優點：效率高，常為預設選擇。 from sklearn.linear_model import Lasso lasso = Lasso(alpha=0.1).fit(X, y) print(&#34;Lasso 係數:&#34;, lasso.coef_) 理論直覺、應用場景與常見誤區 應用場景 One-Hot：低基數類別、無序特徵 Target/Frequency Encoding：高基數類別、樞紐分析 標準化/正規化：距離敏感模型、梯度下降 特徵選擇：降維、提升泛化、減少過擬合 常見誤區 One-Hot 用於高基數，導致維度爆炸。 Target Encoding 未用交叉驗證，導致資料洩漏。 標準化與正規化混用，影響模型收斂。 Wrapper 方法過度耗時，未做特徵預篩。 面試熱點與經典問題 主題 常見問題 One-Hot 何時不用？有何缺點？ Target Encoding 如何防止資料洩漏？ 標準化/正規化 差異與適用場景？ Filter/Wrapper/Embedded 各自優缺點？ Lasso 為何能做特徵選擇？ 使用注意事項 特徵工程需結合業務知識與資料探索。 編碼與標準化順序需一致，避免資料洩漏。 特徵選擇建議多法並用，交叉驗證效果。 延伸閱讀與資源 StatQuest: Feature Engineering Scikit-learn Feature Selection Kaggle: Feature Engineering 教程 經典面試題與解法提示 One-Hot Encoding 有哪些缺點？如何解決？ Target Encoding 如何防止資料洩漏？ 標準化與正規化差異？ Filter/Wrapper/Embedded 方法比較？ Lasso 為何能做特徵選擇？ 特徵選擇對模型有何影響？ 如何用 Python 實作特徵選擇？ 特徵工程常見陷阱有哪些？ 如何評估特徵工程效果？ 實務上如何設計特徵工程流程？ 結語 特徵工程與選擇是 ML 成敗關鍵。熟悉各種編碼、標準化與特徵選擇方法，能讓你打造高效能模型並在面試中脫穎而出。下一章將進入模型評估與驗證，敬請期待！
"><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/feature-engineering/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/feature-engineering/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>特徵工程與選擇全攻略：編碼、標準化、特徵選擇三大法門</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>特徵工程與選擇全攻略：編碼、標準化、特徵選擇三大法門</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-02-18</span></div></header><div class=article-body><p>特徵工程是機器學習成敗的關鍵。從資料前處理、編碼、標準化，到特徵選擇，這些步驟直接影響模型表現與泛化能力。本章將深入 One-Hot/Target/Frequency Encoding、標準化/正規化/Whitening、三大特徵選擇法（Filter/Wrapper/Embedded），結合理論、實作、面試熱點與常見誤區，讓你打造高質量特徵集。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#編碼技巧one-hottargetfrequency-encoding>編碼技巧：One-Hot、Target、Frequency Encoding</a><ul><li><a href=#one-hot-encoding>One-Hot Encoding</a></li><li><a href=#target-encoding>Target Encoding</a></li><li><a href=#frequency-encoding>Frequency Encoding</a></li></ul></li><li><a href=#標準化-vs-正規化-vs-whitening>標準化 vs. 正規化 vs. Whitening</a><ul><li><a href=#標準化standardization>標準化（Standardization）</a></li><li><a href=#正規化normalization>正規化（Normalization）</a></li><li><a href=#whitening>Whitening</a></li></ul></li><li><a href=#特徵選擇三大法門>特徵選擇三大法門</a><ul><li><a href=#filter-方法>Filter 方法</a></li><li><a href=#wrapper-方法>Wrapper 方法</a></li><li><a href=#embedded-方法>Embedded 方法</a></li></ul></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=編碼技巧one-hottargetfrequency-encoding>編碼技巧：One-Hot、Target、Frequency Encoding</h2><h3 id=one-hot-encoding>One-Hot Encoding</h3><ul><li>將類別特徵轉為 0/1 向量，適合無序類別。</li><li>缺點：高基數時維度爆炸。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;color&#39;</span>: [<span style=color:#e6db74>&#39;red&#39;</span>, <span style=color:#e6db74>&#39;blue&#39;</span>, <span style=color:#e6db74>&#39;green&#39;</span>, <span style=color:#e6db74>&#39;blue&#39;</span>]})
</span></span><span style=display:flex><span>onehot <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>get_dummies(df[<span style=color:#e6db74>&#39;color&#39;</span>])
</span></span><span style=display:flex><span>print(onehot)
</span></span></code></pre></div><h3 id=target-encoding>Target Encoding</h3><ul><li>用目標變數的平均值取代類別，適合有序類別或高基數特徵。</li><li>需防止資料洩漏，建議用交叉驗證計算。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df[<span style=color:#e6db74>&#39;target&#39;</span>] <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>mean_map <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>groupby(<span style=color:#e6db74>&#39;color&#39;</span>)[<span style=color:#e6db74>&#39;target&#39;</span>]<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#39;color_te&#39;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;color&#39;</span>]<span style=color:#f92672>.</span>map(mean_map)
</span></span><span style=display:flex><span>print(df[[<span style=color:#e6db74>&#39;color&#39;</span>, <span style=color:#e6db74>&#39;color_te&#39;</span>]])
</span></span></code></pre></div><h3 id=frequency-encoding>Frequency Encoding</h3><ul><li>用類別出現頻率取代原值，適合高基數特徵。</li><li>不引入目標資訊，無洩漏風險。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>freq_map <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;color&#39;</span>]<span style=color:#f92672>.</span>value_counts() <span style=color:#f92672>/</span> len(df)
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#39;color_fe&#39;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;color&#39;</span>]<span style=color:#f92672>.</span>map(freq_map)
</span></span><span style=display:flex><span>print(df[[<span style=color:#e6db74>&#39;color&#39;</span>, <span style=color:#e6db74>&#39;color_fe&#39;</span>]])
</span></span></code></pre></div><hr><h2 id=標準化-vs-正規化-vs-whitening>標準化 vs. 正規化 vs. Whitening</h2><h3 id=標準化standardization>標準化（Standardization）</h3><ul><li>轉換為均值 0、標準差 1，適合大多數 ML 演算法。</li><li>常用於 SVM、Logistic Regression、神經網路。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> StandardScaler
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>scaler <span style=color:#f92672>=</span> StandardScaler()
</span></span><span style=display:flex><span>X_scaled <span style=color:#f92672>=</span> scaler<span style=color:#f92672>.</span>fit_transform([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>], [<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>]])
</span></span><span style=display:flex><span>print(X_scaled)
</span></span></code></pre></div><h3 id=正規化normalization>正規化（Normalization）</h3><ul><li>將特徵縮放到固定範圍（如 0~1），適合距離度量敏感的演算法（如 k-NN）。</li><li>常用 MinMaxScaler。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> MinMaxScaler
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>scaler <span style=color:#f92672>=</span> MinMaxScaler()
</span></span><span style=display:flex><span>X_norm <span style=color:#f92672>=</span> scaler<span style=color:#f92672>.</span>fit_transform([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>], [<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>]])
</span></span><span style=display:flex><span>print(X_norm)
</span></span></code></pre></div><h3 id=whitening>Whitening</h3><ul><li>進一步去除特徵間相關性，使協方差矩陣為單位矩陣。</li><li>常用於 PCA 前處理、深度學習 BatchNorm。</li></ul><hr><h2 id=特徵選擇三大法門>特徵選擇三大法門</h2><h3 id=filter-方法>Filter 方法</h3><ul><li>根據統計量（如相關係數、卡方、互資訊）篩選特徵。</li><li>優點：計算快、無需模型。</li><li>缺點：忽略特徵間交互作用。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.feature_selection <span style=color:#f92672>import</span> SelectKBest, f_classif
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> [[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>],[<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>],[<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>],[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>5</span>]]
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>skb <span style=color:#f92672>=</span> SelectKBest(f_classif, k<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;選中特徵索引:&#34;</span>, skb<span style=color:#f92672>.</span>get_support(indices<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>))
</span></span></code></pre></div><h3 id=wrapper-方法>Wrapper 方法</h3><ul><li>用模型評估特徵子集（如遞迴特徵消除 RFE）。</li><li>優點：考慮特徵交互，效果好。</li><li>缺點：計算量大。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.feature_selection <span style=color:#f92672>import</span> RFE
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LogisticRegression
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rfe <span style=color:#f92672>=</span> RFE(LogisticRegression(), n_features_to_select<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>rfe<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;RFE 選中特徵:&#34;</span>, rfe<span style=color:#f92672>.</span>support_)
</span></span></code></pre></div><h3 id=embedded-方法>Embedded 方法</h3><ul><li>特徵選擇與模型訓練同時進行（如 Lasso、樹模型）。</li><li>優點：效率高，常為預設選擇。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> Lasso
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lasso <span style=color:#f92672>=</span> Lasso(alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Lasso 係數:&#34;</span>, lasso<span style=color:#f92672>.</span>coef_)
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>One-Hot：低基數類別、無序特徵</li><li>Target/Frequency Encoding：高基數類別、樞紐分析</li><li>標準化/正規化：距離敏感模型、梯度下降</li><li>特徵選擇：降維、提升泛化、減少過擬合</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>One-Hot 用於高基數，導致維度爆炸。</li><li>Target Encoding 未用交叉驗證，導致資料洩漏。</li><li>標準化與正規化混用，影響模型收斂。</li><li>Wrapper 方法過度耗時，未做特徵預篩。</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>One-Hot</td><td>何時不用？有何缺點？</td></tr><tr><td>Target Encoding</td><td>如何防止資料洩漏？</td></tr><tr><td>標準化/正規化</td><td>差異與適用場景？</td></tr><tr><td>Filter/Wrapper/Embedded</td><td>各自優缺點？</td></tr><tr><td>Lasso</td><td>為何能做特徵選擇？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>特徵工程需結合業務知識與資料探索。</li><li>編碼與標準化順序需一致，避免資料洩漏。</li><li>特徵選擇建議多法並用，交叉驗證效果。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://www.youtube.com/c/joshstarmer>StatQuest: Feature Engineering</a></li><li><a href=https://scikit-learn.org/stable/modules/feature_selection.html>Scikit-learn Feature Selection</a></li><li><a href=https://www.kaggle.com/learn/feature-engineering>Kaggle: Feature Engineering 教程</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>One-Hot Encoding 有哪些缺點？如何解決？</li><li>Target Encoding 如何防止資料洩漏？</li><li>標準化與正規化差異？</li><li>Filter/Wrapper/Embedded 方法比較？</li><li>Lasso 為何能做特徵選擇？</li><li>特徵選擇對模型有何影響？</li><li>如何用 Python 實作特徵選擇？</li><li>特徵工程常見陷阱有哪些？</li><li>如何評估特徵工程效果？</li><li>實務上如何設計特徵工程流程？</li></ol><hr><h2 id=結語>結語</h2><p>特徵工程與選擇是 ML 成敗關鍵。熟悉各種編碼、標準化與特徵選擇方法，能讓你打造高效能模型並在面試中脫穎而出。下一章將進入模型評估與驗證，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>One-Hot</span>
<span class=tag>Encoding</span>
<span class=tag>Feature Selection</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>