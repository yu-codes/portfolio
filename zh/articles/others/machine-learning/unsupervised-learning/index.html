<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>非監督學習大補帖：聚類、密度估計、降維全解析 - Yu's Portfolio & Learning Hub</title><meta name=description content='非監督學習是資料探索、特徵工程與生成模型的基礎。從經典的 K-means、DBSCAN、GMM，到降維利器 PCA、t-SNE、UMAP，這些方法能幫助我們發現資料結構、壓縮高維資訊、提升後續模型表現。本章將深入數學原理、直覺圖解、Python 實作、面試熱點與常見誤區，讓你全面掌握非監督學習。
聚類：K-means、DBSCAN、Spectral Clustering K-means 將資料分為 K 群，最小化群內平方誤差。 迭代步驟：隨機初始化中心→分配點→更新中心→重複。 對初始值敏感，需多次重啟。 from sklearn.cluster import KMeans import numpy as np import matplotlib.pyplot as plt X = np.random.randn(200, 2) kmeans = KMeans(n_clusters=3, n_init=10).fit(X) plt.scatter(X[:,0], X[:,1], c=kmeans.labels_) plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c=&#39;red&#39;, marker=&#39;x&#39;) plt.title("K-means Clustering"); plt.show() DBSCAN 基於密度的聚類，能發現任意形狀的群集。 參數：鄰域半徑 eps、最小點數 min_samples。 可自動識別噪聲點，不需指定群數。 from sklearn.cluster import DBSCAN db = DBSCAN(eps=0.3, min_samples=5).fit(X) plt.scatter(X[:,0], X[:,1], c=db.labels_) plt.title("DBSCAN Clustering"); plt.show() Spectral Clustering 利用圖論與特徵分解，適合非凸形狀資料。 先建鄰接圖，再做特徵分解與 K-means。 from sklearn.cluster import SpectralClustering sc = SpectralClustering(n_clusters=3, affinity=&#39;nearest_neighbors&#39;).fit(X) plt.scatter(X[:,0], X[:,1], c=sc.labels_) plt.title("Spectral Clustering"); plt.show() 密度估計 & 混合模型（GMM, EM） 密度估計 估計資料分布函數，常用於異常偵測、生成模型。 方法：直方圖、核密度估計（KDE）、混合模型。 GMM（Gaussian Mixture Model） 用多個高斯分布混合建模資料，參數用 EM 演算法學習。 可自動分群、密度估計、異常偵測。 from sklearn.mixture import GaussianMixture gmm = GaussianMixture(n_components=3).fit(X) labels = gmm.predict(X) plt.scatter(X[:,0], X[:,1], c=labels) plt.title("GMM Clustering"); plt.show() EM 演算法 交替進行 E 步（計算隱變量期望）與 M 步（最大化參數）。 適用於含隱變量的最大概似估計。 降維：PCA、t-SNE、UMAP 比較 PCA（主成分分析） 線性降維，找最大變異方向。 可視化高維資料、特徵壓縮、去除共線性。 from sklearn.decomposition import PCA pca = PCA(n_components=2).fit(X) X_pca = pca.transform(X) plt.scatter(X_pca[:,0], X_pca[:,1]) plt.title("PCA Projection"); plt.show() t-SNE 非線性降維，保留局部結構，適合視覺化。 對超參數敏感，計算量大。 from sklearn.manifold import TSNE X_tsne = TSNE(n_components=2, perplexity=30).fit_transform(X) plt.scatter(X_tsne[:,0], X_tsne[:,1]) plt.title("t-SNE Projection"); plt.show() UMAP 非線性降維，速度快、可保留全域與局部結構。 適合大規模資料、互動式視覺化。 import umap X_umap = umap.UMAP(n_components=2).fit_transform(X) plt.scatter(X_umap[:,0], X_umap[:,1]) plt.title("UMAP Projection"); plt.show() 理論直覺、應用場景與常見誤區 應用場景 聚類：市場分群、影像分割、社群偵測 密度估計：異常偵測、生成模型 降維：資料視覺化、特徵工程、噪聲過濾 常見誤區 K-means 只適合球狀群集，對離群值敏感。 DBSCAN 參數選擇不當易分錯群。 GMM 假設群內分布為高斯，實務未必成立。 t-SNE 僅適合視覺化，無法做新資料投影。 PCA 只保留線性結構，忽略非線性資訊。 面試熱點與經典問題 主題 常見問題 K-means 如何選 K？初始值敏感怎麼辦？ DBSCAN 參數如何選？優缺點？ GMM/EM EM 步驟推導？何時用 GMM？ PCA/t-SNE/UMAP 差異與適用場景？ 密度估計 KDE 與 GMM 差異？ 使用注意事項 聚類與降維結果需結合領域知識解讀。 非監督學習無標準答案，建議多種方法交叉驗證。 降維前建議先標準化資料，避免特徵尺度影響。 延伸閱讀與資源 StatQuest: K-means, GMM, PCA, t-SNE Scikit-learn Clustering & Decomposition UMAP 官方文件 t-SNE 理論與實作 經典面試題與解法提示 K-means 為何對初始值敏感？如何改進？ DBSCAN 如何自動判斷群數？有何限制？ GMM 與 K-means 差異？ EM 演算法的數學推導？ PCA 如何選主成分數量？ t-SNE/UMAP 適合哪些應用？ 密度估計有哪些方法？各自優缺點？ 聚類評估指標有哪些？ 非監督學習如何驗證效果？ 如何用 Python 實作多種聚類並比較？ 結語 非監督學習是資料探索與特徵工程的關鍵。熟悉聚類、密度估計、降維方法，能讓你在資料分析、模型前處理與面試中展現專業素養。下一章將進入特徵工程與選擇，敬請期待！
'><meta property="og:title" content="非監督學習大補帖：聚類、密度估計、降維全解析"><meta property="og:description" content='非監督學習是資料探索、特徵工程與生成模型的基礎。從經典的 K-means、DBSCAN、GMM，到降維利器 PCA、t-SNE、UMAP，這些方法能幫助我們發現資料結構、壓縮高維資訊、提升後續模型表現。本章將深入數學原理、直覺圖解、Python 實作、面試熱點與常見誤區，讓你全面掌握非監督學習。
聚類：K-means、DBSCAN、Spectral Clustering K-means 將資料分為 K 群，最小化群內平方誤差。 迭代步驟：隨機初始化中心→分配點→更新中心→重複。 對初始值敏感，需多次重啟。 from sklearn.cluster import KMeans import numpy as np import matplotlib.pyplot as plt X = np.random.randn(200, 2) kmeans = KMeans(n_clusters=3, n_init=10).fit(X) plt.scatter(X[:,0], X[:,1], c=kmeans.labels_) plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c=&#39;red&#39;, marker=&#39;x&#39;) plt.title("K-means Clustering"); plt.show() DBSCAN 基於密度的聚類，能發現任意形狀的群集。 參數：鄰域半徑 eps、最小點數 min_samples。 可自動識別噪聲點，不需指定群數。 from sklearn.cluster import DBSCAN db = DBSCAN(eps=0.3, min_samples=5).fit(X) plt.scatter(X[:,0], X[:,1], c=db.labels_) plt.title("DBSCAN Clustering"); plt.show() Spectral Clustering 利用圖論與特徵分解，適合非凸形狀資料。 先建鄰接圖，再做特徵分解與 K-means。 from sklearn.cluster import SpectralClustering sc = SpectralClustering(n_clusters=3, affinity=&#39;nearest_neighbors&#39;).fit(X) plt.scatter(X[:,0], X[:,1], c=sc.labels_) plt.title("Spectral Clustering"); plt.show() 密度估計 & 混合模型（GMM, EM） 密度估計 估計資料分布函數，常用於異常偵測、生成模型。 方法：直方圖、核密度估計（KDE）、混合模型。 GMM（Gaussian Mixture Model） 用多個高斯分布混合建模資料，參數用 EM 演算法學習。 可自動分群、密度估計、異常偵測。 from sklearn.mixture import GaussianMixture gmm = GaussianMixture(n_components=3).fit(X) labels = gmm.predict(X) plt.scatter(X[:,0], X[:,1], c=labels) plt.title("GMM Clustering"); plt.show() EM 演算法 交替進行 E 步（計算隱變量期望）與 M 步（最大化參數）。 適用於含隱變量的最大概似估計。 降維：PCA、t-SNE、UMAP 比較 PCA（主成分分析） 線性降維，找最大變異方向。 可視化高維資料、特徵壓縮、去除共線性。 from sklearn.decomposition import PCA pca = PCA(n_components=2).fit(X) X_pca = pca.transform(X) plt.scatter(X_pca[:,0], X_pca[:,1]) plt.title("PCA Projection"); plt.show() t-SNE 非線性降維，保留局部結構，適合視覺化。 對超參數敏感，計算量大。 from sklearn.manifold import TSNE X_tsne = TSNE(n_components=2, perplexity=30).fit_transform(X) plt.scatter(X_tsne[:,0], X_tsne[:,1]) plt.title("t-SNE Projection"); plt.show() UMAP 非線性降維，速度快、可保留全域與局部結構。 適合大規模資料、互動式視覺化。 import umap X_umap = umap.UMAP(n_components=2).fit_transform(X) plt.scatter(X_umap[:,0], X_umap[:,1]) plt.title("UMAP Projection"); plt.show() 理論直覺、應用場景與常見誤區 應用場景 聚類：市場分群、影像分割、社群偵測 密度估計：異常偵測、生成模型 降維：資料視覺化、特徵工程、噪聲過濾 常見誤區 K-means 只適合球狀群集，對離群值敏感。 DBSCAN 參數選擇不當易分錯群。 GMM 假設群內分布為高斯，實務未必成立。 t-SNE 僅適合視覺化，無法做新資料投影。 PCA 只保留線性結構，忽略非線性資訊。 面試熱點與經典問題 主題 常見問題 K-means 如何選 K？初始值敏感怎麼辦？ DBSCAN 參數如何選？優缺點？ GMM/EM EM 步驟推導？何時用 GMM？ PCA/t-SNE/UMAP 差異與適用場景？ 密度估計 KDE 與 GMM 差異？ 使用注意事項 聚類與降維結果需結合領域知識解讀。 非監督學習無標準答案，建議多種方法交叉驗證。 降維前建議先標準化資料，避免特徵尺度影響。 延伸閱讀與資源 StatQuest: K-means, GMM, PCA, t-SNE Scikit-learn Clustering & Decomposition UMAP 官方文件 t-SNE 理論與實作 經典面試題與解法提示 K-means 為何對初始值敏感？如何改進？ DBSCAN 如何自動判斷群數？有何限制？ GMM 與 K-means 差異？ EM 演算法的數學推導？ PCA 如何選主成分數量？ t-SNE/UMAP 適合哪些應用？ 密度估計有哪些方法？各自優缺點？ 聚類評估指標有哪些？ 非監督學習如何驗證效果？ 如何用 Python 實作多種聚類並比較？ 結語 非監督學習是資料探索與特徵工程的關鍵。熟悉聚類、密度估計、降維方法，能讓你在資料分析、模型前處理與面試中展現專業素養。下一章將進入特徵工程與選擇，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/unsupervised-learning/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/unsupervised-learning/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>非監督學習大補帖：聚類、密度估計、降維全解析</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>非監督學習大補帖：聚類、密度估計、降維全解析</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-12-26</span></div></header><div class=article-body><p>非監督學習是資料探索、特徵工程與生成模型的基礎。從經典的 K-means、DBSCAN、GMM，到降維利器 PCA、t-SNE、UMAP，這些方法能幫助我們發現資料結構、壓縮高維資訊、提升後續模型表現。本章將深入數學原理、直覺圖解、Python 實作、面試熱點與常見誤區，讓你全面掌握非監督學習。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#聚類k-meansdbscanspectral-clustering>聚類：K-means、DBSCAN、Spectral Clustering</a><ul><li><a href=#k-means>K-means</a></li><li><a href=#dbscan>DBSCAN</a></li><li><a href=#spectral-clustering>Spectral Clustering</a></li></ul></li><li><a href=#密度估計--混合模型gmm-em>密度估計 & 混合模型（GMM, EM）</a><ul><li><a href=#密度估計>密度估計</a></li><li><a href=#gmmgaussian-mixture-model>GMM（Gaussian Mixture Model）</a></li><li><a href=#em-演算法>EM 演算法</a></li></ul></li><li><a href=#降維pcat-sneumap-比較>降維：PCA、t-SNE、UMAP 比較</a><ul><li><a href=#pca主成分分析>PCA（主成分分析）</a></li><li><a href=#t-sne>t-SNE</a></li><li><a href=#umap>UMAP</a></li></ul></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=聚類k-meansdbscanspectral-clustering>聚類：K-means、DBSCAN、Spectral Clustering</h2><h3 id=k-means>K-means</h3><ul><li>將資料分為 K 群，最小化群內平方誤差。</li><li>迭代步驟：隨機初始化中心→分配點→更新中心→重複。</li><li>對初始值敏感，需多次重啟。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.cluster <span style=color:#f92672>import</span> KMeans
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>200</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>kmeans <span style=color:#f92672>=</span> KMeans(n_clusters<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, n_init<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)<span style=color:#f92672>.</span>fit(X)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(X[:,<span style=color:#ae81ff>0</span>], X[:,<span style=color:#ae81ff>1</span>], c<span style=color:#f92672>=</span>kmeans<span style=color:#f92672>.</span>labels_)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(kmeans<span style=color:#f92672>.</span>cluster_centers_[:,<span style=color:#ae81ff>0</span>], kmeans<span style=color:#f92672>.</span>cluster_centers_[:,<span style=color:#ae81ff>1</span>], c<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;red&#39;</span>, marker<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;x&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;K-means Clustering&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h3 id=dbscan>DBSCAN</h3><ul><li>基於密度的聚類，能發現任意形狀的群集。</li><li>參數：鄰域半徑 eps、最小點數 min_samples。</li><li>可自動識別噪聲點，不需指定群數。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.cluster <span style=color:#f92672>import</span> DBSCAN
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>db <span style=color:#f92672>=</span> DBSCAN(eps<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>, min_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)<span style=color:#f92672>.</span>fit(X)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(X[:,<span style=color:#ae81ff>0</span>], X[:,<span style=color:#ae81ff>1</span>], c<span style=color:#f92672>=</span>db<span style=color:#f92672>.</span>labels_)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;DBSCAN Clustering&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h3 id=spectral-clustering>Spectral Clustering</h3><ul><li>利用圖論與特徵分解，適合非凸形狀資料。</li><li>先建鄰接圖，再做特徵分解與 K-means。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.cluster <span style=color:#f92672>import</span> SpectralClustering
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sc <span style=color:#f92672>=</span> SpectralClustering(n_clusters<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, affinity<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;nearest_neighbors&#39;</span>)<span style=color:#f92672>.</span>fit(X)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(X[:,<span style=color:#ae81ff>0</span>], X[:,<span style=color:#ae81ff>1</span>], c<span style=color:#f92672>=</span>sc<span style=color:#f92672>.</span>labels_)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Spectral Clustering&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><hr><h2 id=密度估計--混合模型gmm-em>密度估計 & 混合模型（GMM, EM）</h2><h3 id=密度估計>密度估計</h3><ul><li>估計資料分布函數，常用於異常偵測、生成模型。</li><li>方法：直方圖、核密度估計（KDE）、混合模型。</li></ul><h3 id=gmmgaussian-mixture-model>GMM（Gaussian Mixture Model）</h3><ul><li>用多個高斯分布混合建模資料，參數用 EM 演算法學習。</li><li>可自動分群、密度估計、異常偵測。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.mixture <span style=color:#f92672>import</span> GaussianMixture
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>gmm <span style=color:#f92672>=</span> GaussianMixture(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)<span style=color:#f92672>.</span>fit(X)
</span></span><span style=display:flex><span>labels <span style=color:#f92672>=</span> gmm<span style=color:#f92672>.</span>predict(X)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(X[:,<span style=color:#ae81ff>0</span>], X[:,<span style=color:#ae81ff>1</span>], c<span style=color:#f92672>=</span>labels)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;GMM Clustering&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h3 id=em-演算法>EM 演算法</h3><ul><li>交替進行 E 步（計算隱變量期望）與 M 步（最大化參數）。</li><li>適用於含隱變量的最大概似估計。</li></ul><hr><h2 id=降維pcat-sneumap-比較>降維：PCA、t-SNE、UMAP 比較</h2><h3 id=pca主成分分析>PCA（主成分分析）</h3><ul><li>線性降維，找最大變異方向。</li><li>可視化高維資料、特徵壓縮、去除共線性。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.decomposition <span style=color:#f92672>import</span> PCA
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pca <span style=color:#f92672>=</span> PCA(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>fit(X)
</span></span><span style=display:flex><span>X_pca <span style=color:#f92672>=</span> pca<span style=color:#f92672>.</span>transform(X)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(X_pca[:,<span style=color:#ae81ff>0</span>], X_pca[:,<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;PCA Projection&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h3 id=t-sne>t-SNE</h3><ul><li>非線性降維，保留局部結構，適合視覺化。</li><li>對超參數敏感，計算量大。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.manifold <span style=color:#f92672>import</span> TSNE
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X_tsne <span style=color:#f92672>=</span> TSNE(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, perplexity<span style=color:#f92672>=</span><span style=color:#ae81ff>30</span>)<span style=color:#f92672>.</span>fit_transform(X)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(X_tsne[:,<span style=color:#ae81ff>0</span>], X_tsne[:,<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;t-SNE Projection&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h3 id=umap>UMAP</h3><ul><li>非線性降維，速度快、可保留全域與局部結構。</li><li>適合大規模資料、互動式視覺化。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> umap
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X_umap <span style=color:#f92672>=</span> umap<span style=color:#f92672>.</span>UMAP(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>fit_transform(X)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(X_umap[:,<span style=color:#ae81ff>0</span>], X_umap[:,<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;UMAP Projection&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>聚類：市場分群、影像分割、社群偵測</li><li>密度估計：異常偵測、生成模型</li><li>降維：資料視覺化、特徵工程、噪聲過濾</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>K-means 只適合球狀群集，對離群值敏感。</li><li>DBSCAN 參數選擇不當易分錯群。</li><li>GMM 假設群內分布為高斯，實務未必成立。</li><li>t-SNE 僅適合視覺化，無法做新資料投影。</li><li>PCA 只保留線性結構，忽略非線性資訊。</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>K-means</td><td>如何選 K？初始值敏感怎麼辦？</td></tr><tr><td>DBSCAN</td><td>參數如何選？優缺點？</td></tr><tr><td>GMM/EM</td><td>EM 步驟推導？何時用 GMM？</td></tr><tr><td>PCA/t-SNE/UMAP</td><td>差異與適用場景？</td></tr><tr><td>密度估計</td><td>KDE 與 GMM 差異？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>聚類與降維結果需結合領域知識解讀。</li><li>非監督學習無標準答案，建議多種方法交叉驗證。</li><li>降維前建議先標準化資料，避免特徵尺度影響。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://www.youtube.com/c/joshstarmer>StatQuest: K-means, GMM, PCA, t-SNE</a></li><li><a href=https://scikit-learn.org/stable/modules/clustering.html>Scikit-learn Clustering & Decomposition</a></li><li><a href=https://umap-learn.readthedocs.io/en/latest/>UMAP 官方文件</a></li><li><a href=https://distill.pub/2016/misread-tsne/>t-SNE 理論與實作</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>K-means 為何對初始值敏感？如何改進？</li><li>DBSCAN 如何自動判斷群數？有何限制？</li><li>GMM 與 K-means 差異？</li><li>EM 演算法的數學推導？</li><li>PCA 如何選主成分數量？</li><li>t-SNE/UMAP 適合哪些應用？</li><li>密度估計有哪些方法？各自優缺點？</li><li>聚類評估指標有哪些？</li><li>非監督學習如何驗證效果？</li><li>如何用 Python 實作多種聚類並比較？</li></ol><hr><h2 id=結語>結語</h2><p>非監督學習是資料探索與特徵工程的關鍵。熟悉聚類、密度估計、降維方法，能讓你在資料分析、模型前處理與面試中展現專業素養。下一章將進入特徵工程與選擇，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>K-means</span>
<span class=tag>DBSCAN</span>
<span class=tag>GMM</span>
<span class=tag>PCA</span>
<span class=tag>t-SNE</span>
<span class=tag>UMAP</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>