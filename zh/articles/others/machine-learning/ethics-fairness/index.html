<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>倫理、偏差與公平：機器學習的責任、合規與公平性全解析 - Yu's Portfolio & Learning Hub</title><meta name=description content='隨著機器學習廣泛應用於金融、醫療、招聘等敏感領域，模型的倫理、偏差與公平性問題日益受到關注。從公平性指標、數據合規（GDPR/CCPA）、到模型偏見偵測與緩解技巧，這些議題不僅是面試熱點，更是 AI 實務落地的底線。本章將深入理論、法規、實作、面試熱點與常見誤區，幫助你全面掌握 ML 的責任與公平性。
Fairness Metrics（公平性指標） Demographic Parity（人口統計平等） 預測結果與敏感屬性（如性別、種族）無關。 $P(\hat{Y}=1|A=0) = P(\hat{Y}=1|A=1)$ Equal Opportunity（機會平等） 對於正樣本，敏感屬性間的召回率應相等。 $P(\hat{Y}=1|Y=1, A=0) = P(\hat{Y}=1|Y=1, A=1)$ Equalized Odds 對於所有真實標籤，敏感屬性間的 TPR/FPR 均相等。 Python 實作（公平性指標） import numpy as np y_true = np.array([1, 0, 1, 0, 1, 0]) y_pred = np.array([1, 0, 1, 1, 0, 0]) A = np.array([0, 0, 1, 1, 0, 1]) # 敏感屬性 dp_0 = np.mean(y_pred[A == 0]) dp_1 = np.mean(y_pred[A == 1]) print("Demographic Parity:", dp_0, dp_1) 強化數據合規：GDPR / CCPA 與 ML 流程 GDPR（歐盟一般資料保護規則） 強調資料主體權利、資料最小化、可解釋性。 影響：需記錄資料來源、獲取同意、支持刪除請求、模型可解釋。 CCPA（加州消費者隱私法） 強調消費者知情權、刪除權、拒絕銷售權。 影響：需提供資料存取、刪除、拒絕銷售等功能。 合規實踐 記錄資料流、模型決策邏輯 實作資料刪除、匿名化、審計追蹤 提供模型可解釋性報告 模型偏見偵測與緩解技巧 偏見來源 樣本偏差：訓練資料不均衡 標註偏差：標註者主觀影響 特徵偏差：敏感屬性滲透 偏見偵測 分群評估指標（如 TPR, FPR, Precision） 可視化敏感屬性與預測關係 偏見緩解技巧 資料層：重抽樣、資料增強、去敏感化 模型層：公平性正則化、對抗訓練 預測層：後處理調整閾值 from sklearn.utils import resample # 欠抽樣多數類 X_minority = X[y == 1] X_majority = X[y == 0] X_majority_down = resample(X_majority, replace=False, n_samples=len(X_minority)) X_balanced = np.vstack([X_minority, X_majority_down]) 理論直覺、應用場景與常見誤區 應用場景 金融信貸、醫療診斷、招聘篩選、司法判決 需符合法規、避免歧視與不公平 常見誤區 只看整體指標，忽略分群公平性 誤以為去除敏感屬性即可消除偏見 忽略資料合規，導致法律風險 公平性與準確率 trade-off 未評估 面試熱點與經典問題 主題 常見問題 Fairness Metrics 有哪些？如何計算？ GDPR/CCPA 對 ML 有何影響？ 偏見來源 如何偵測與緩解？ 合規實踐 如何設計資料流與審計？ 公平性 vs 準確率 如何權衡？ 使用注意事項 公平性評估需分群進行，避免隱性偏見 合規流程需全程記錄、可追溯 偏見緩解需結合資料、模型、預測多層次方法 延伸閱讀與資源 Fairness Indicators (Google) AIF360: AI Fairness 360 工具包 GDPR 官方文件 CCPA 官方文件 經典面試題與解法提示 Demographic Parity 與 Equal Opportunity 差異？ GDPR/CCPA 對 ML 流程的實際要求？ 偏見來源有哪些？如何偵測？ 資料層/模型層/預測層偏見緩解方法？ 公平性與準確率如何權衡？ 如何用 Python 計算公平性指標？ 合規審計流程如何設計？ 去除敏感屬性是否足夠？為什麼？ AIF360 工具包有哪些功能？ 公平性評估在實務中的挑戰？ 結語 倫理、偏差與公平性是 ML 實務落地的底線。熟悉公平性指標、合規法規與偏見緩解技巧，能讓你打造更負責任、更可信賴的 AI 系統，並在面試與專案中展現專業素養。下一章將進入經典面試題庫，敬請期待！
'><meta property="og:title" content="倫理、偏差與公平：機器學習的責任、合規與公平性全解析"><meta property="og:description" content='隨著機器學習廣泛應用於金融、醫療、招聘等敏感領域，模型的倫理、偏差與公平性問題日益受到關注。從公平性指標、數據合規（GDPR/CCPA）、到模型偏見偵測與緩解技巧，這些議題不僅是面試熱點，更是 AI 實務落地的底線。本章將深入理論、法規、實作、面試熱點與常見誤區，幫助你全面掌握 ML 的責任與公平性。
Fairness Metrics（公平性指標） Demographic Parity（人口統計平等） 預測結果與敏感屬性（如性別、種族）無關。 $P(\hat{Y}=1|A=0) = P(\hat{Y}=1|A=1)$ Equal Opportunity（機會平等） 對於正樣本，敏感屬性間的召回率應相等。 $P(\hat{Y}=1|Y=1, A=0) = P(\hat{Y}=1|Y=1, A=1)$ Equalized Odds 對於所有真實標籤，敏感屬性間的 TPR/FPR 均相等。 Python 實作（公平性指標） import numpy as np y_true = np.array([1, 0, 1, 0, 1, 0]) y_pred = np.array([1, 0, 1, 1, 0, 0]) A = np.array([0, 0, 1, 1, 0, 1]) # 敏感屬性 dp_0 = np.mean(y_pred[A == 0]) dp_1 = np.mean(y_pred[A == 1]) print("Demographic Parity:", dp_0, dp_1) 強化數據合規：GDPR / CCPA 與 ML 流程 GDPR（歐盟一般資料保護規則） 強調資料主體權利、資料最小化、可解釋性。 影響：需記錄資料來源、獲取同意、支持刪除請求、模型可解釋。 CCPA（加州消費者隱私法） 強調消費者知情權、刪除權、拒絕銷售權。 影響：需提供資料存取、刪除、拒絕銷售等功能。 合規實踐 記錄資料流、模型決策邏輯 實作資料刪除、匿名化、審計追蹤 提供模型可解釋性報告 模型偏見偵測與緩解技巧 偏見來源 樣本偏差：訓練資料不均衡 標註偏差：標註者主觀影響 特徵偏差：敏感屬性滲透 偏見偵測 分群評估指標（如 TPR, FPR, Precision） 可視化敏感屬性與預測關係 偏見緩解技巧 資料層：重抽樣、資料增強、去敏感化 模型層：公平性正則化、對抗訓練 預測層：後處理調整閾值 from sklearn.utils import resample # 欠抽樣多數類 X_minority = X[y == 1] X_majority = X[y == 0] X_majority_down = resample(X_majority, replace=False, n_samples=len(X_minority)) X_balanced = np.vstack([X_minority, X_majority_down]) 理論直覺、應用場景與常見誤區 應用場景 金融信貸、醫療診斷、招聘篩選、司法判決 需符合法規、避免歧視與不公平 常見誤區 只看整體指標，忽略分群公平性 誤以為去除敏感屬性即可消除偏見 忽略資料合規，導致法律風險 公平性與準確率 trade-off 未評估 面試熱點與經典問題 主題 常見問題 Fairness Metrics 有哪些？如何計算？ GDPR/CCPA 對 ML 有何影響？ 偏見來源 如何偵測與緩解？ 合規實踐 如何設計資料流與審計？ 公平性 vs 準確率 如何權衡？ 使用注意事項 公平性評估需分群進行，避免隱性偏見 合規流程需全程記錄、可追溯 偏見緩解需結合資料、模型、預測多層次方法 延伸閱讀與資源 Fairness Indicators (Google) AIF360: AI Fairness 360 工具包 GDPR 官方文件 CCPA 官方文件 經典面試題與解法提示 Demographic Parity 與 Equal Opportunity 差異？ GDPR/CCPA 對 ML 流程的實際要求？ 偏見來源有哪些？如何偵測？ 資料層/模型層/預測層偏見緩解方法？ 公平性與準確率如何權衡？ 如何用 Python 計算公平性指標？ 合規審計流程如何設計？ 去除敏感屬性是否足夠？為什麼？ AIF360 工具包有哪些功能？ 公平性評估在實務中的挑戰？ 結語 倫理、偏差與公平性是 ML 實務落地的底線。熟悉公平性指標、合規法規與偏見緩解技巧，能讓你打造更負責任、更可信賴的 AI 系統，並在面試與專案中展現專業素養。下一章將進入經典面試題庫，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/ethics-fairness/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/ethics-fairness/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>倫理、偏差與公平：機器學習的責任、合規與公平性全解析</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>倫理、偏差與公平：機器學習的責任、合規與公平性全解析</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-12-15</span></div></header><div class=article-body><p>隨著機器學習廣泛應用於金融、醫療、招聘等敏感領域，模型的倫理、偏差與公平性問題日益受到關注。從公平性指標、數據合規（GDPR/CCPA）、到模型偏見偵測與緩解技巧，這些議題不僅是面試熱點，更是 AI 實務落地的底線。本章將深入理論、法規、實作、面試熱點與常見誤區，幫助你全面掌握 ML 的責任與公平性。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#fairness-metrics公平性指標>Fairness Metrics（公平性指標）</a><ul><li><a href=#demographic-parity人口統計平等>Demographic Parity（人口統計平等）</a></li><li><a href=#equal-opportunity機會平等>Equal Opportunity（機會平等）</a></li><li><a href=#equalized-odds>Equalized Odds</a></li><li><a href=#python-實作公平性指標>Python 實作（公平性指標）</a></li></ul></li><li><a href=#強化數據合規gdpr--ccpa-與-ml-流程>強化數據合規：GDPR / CCPA 與 ML 流程</a><ul><li><a href=#gdpr歐盟一般資料保護規則>GDPR（歐盟一般資料保護規則）</a></li><li><a href=#ccpa加州消費者隱私法>CCPA（加州消費者隱私法）</a></li><li><a href=#合規實踐>合規實踐</a></li></ul></li><li><a href=#模型偏見偵測與緩解技巧>模型偏見偵測與緩解技巧</a><ul><li><a href=#偏見來源>偏見來源</a></li><li><a href=#偏見偵測>偏見偵測</a></li><li><a href=#偏見緩解技巧>偏見緩解技巧</a></li></ul></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=fairness-metrics公平性指標>Fairness Metrics（公平性指標）</h2><h3 id=demographic-parity人口統計平等>Demographic Parity（人口統計平等）</h3><ul><li>預測結果與敏感屬性（如性別、種族）無關。</li><li>$P(\hat{Y}=1|A=0) = P(\hat{Y}=1|A=1)$</li></ul><h3 id=equal-opportunity機會平等>Equal Opportunity（機會平等）</h3><ul><li>對於正樣本，敏感屬性間的召回率應相等。</li><li>$P(\hat{Y}=1|Y=1, A=0) = P(\hat{Y}=1|Y=1, A=1)$</li></ul><h3 id=equalized-odds>Equalized Odds</h3><ul><li>對於所有真實標籤，敏感屬性間的 TPR/FPR 均相等。</li></ul><h3 id=python-實作公平性指標>Python 實作（公平性指標）</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_true <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>A <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>])  <span style=color:#75715e># 敏感屬性</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dp_0 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(y_pred[A <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>dp_1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(y_pred[A <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Demographic Parity:&#34;</span>, dp_0, dp_1)
</span></span></code></pre></div><hr><h2 id=強化數據合規gdpr--ccpa-與-ml-流程>強化數據合規：GDPR / CCPA 與 ML 流程</h2><h3 id=gdpr歐盟一般資料保護規則>GDPR（歐盟一般資料保護規則）</h3><ul><li>強調資料主體權利、資料最小化、可解釋性。</li><li>影響：需記錄資料來源、獲取同意、支持刪除請求、模型可解釋。</li></ul><h3 id=ccpa加州消費者隱私法>CCPA（加州消費者隱私法）</h3><ul><li>強調消費者知情權、刪除權、拒絕銷售權。</li><li>影響：需提供資料存取、刪除、拒絕銷售等功能。</li></ul><h3 id=合規實踐>合規實踐</h3><ul><li>記錄資料流、模型決策邏輯</li><li>實作資料刪除、匿名化、審計追蹤</li><li>提供模型可解釋性報告</li></ul><hr><h2 id=模型偏見偵測與緩解技巧>模型偏見偵測與緩解技巧</h2><h3 id=偏見來源>偏見來源</h3><ul><li>樣本偏差：訓練資料不均衡</li><li>標註偏差：標註者主觀影響</li><li>特徵偏差：敏感屬性滲透</li></ul><h3 id=偏見偵測>偏見偵測</h3><ul><li>分群評估指標（如 TPR, FPR, Precision）</li><li>可視化敏感屬性與預測關係</li></ul><h3 id=偏見緩解技巧>偏見緩解技巧</h3><ul><li>資料層：重抽樣、資料增強、去敏感化</li><li>模型層：公平性正則化、對抗訓練</li><li>預測層：後處理調整閾值</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.utils <span style=color:#f92672>import</span> resample
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 欠抽樣多數類</span>
</span></span><span style=display:flex><span>X_minority <span style=color:#f92672>=</span> X[y <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>X_majority <span style=color:#f92672>=</span> X[y <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>X_majority_down <span style=color:#f92672>=</span> resample(X_majority, replace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, n_samples<span style=color:#f92672>=</span>len(X_minority))
</span></span><span style=display:flex><span>X_balanced <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>vstack([X_minority, X_majority_down])
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>金融信貸、醫療診斷、招聘篩選、司法判決</li><li>需符合法規、避免歧視與不公平</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>只看整體指標，忽略分群公平性</li><li>誤以為去除敏感屬性即可消除偏見</li><li>忽略資料合規，導致法律風險</li><li>公平性與準確率 trade-off 未評估</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>Fairness Metrics</td><td>有哪些？如何計算？</td></tr><tr><td>GDPR/CCPA</td><td>對 ML 有何影響？</td></tr><tr><td>偏見來源</td><td>如何偵測與緩解？</td></tr><tr><td>合規實踐</td><td>如何設計資料流與審計？</td></tr><tr><td>公平性 vs 準確率</td><td>如何權衡？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>公平性評估需分群進行，避免隱性偏見</li><li>合規流程需全程記錄、可追溯</li><li>偏見緩解需結合資料、模型、預測多層次方法</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://www.tensorflow.org/tfx/guide/fairness_indicators>Fairness Indicators (Google)</a></li><li><a href=https://aif360.mybluemix.net/>AIF360: AI Fairness 360 工具包</a></li><li><a href=https://gdpr-info.eu/>GDPR 官方文件</a></li><li><a href=https://oag.ca.gov/privacy/ccpa>CCPA 官方文件</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>Demographic Parity 與 Equal Opportunity 差異？</li><li>GDPR/CCPA 對 ML 流程的實際要求？</li><li>偏見來源有哪些？如何偵測？</li><li>資料層/模型層/預測層偏見緩解方法？</li><li>公平性與準確率如何權衡？</li><li>如何用 Python 計算公平性指標？</li><li>合規審計流程如何設計？</li><li>去除敏感屬性是否足夠？為什麼？</li><li>AIF360 工具包有哪些功能？</li><li>公平性評估在實務中的挑戰？</li></ol><hr><h2 id=結語>結語</h2><p>倫理、偏差與公平性是 ML 實務落地的底線。熟悉公平性指標、合規法規與偏見緩解技巧，能讓你打造更負責任、更可信賴的 AI 系統，並在面試與專案中展現專業素養。下一章將進入經典面試題庫，敬請期待！</p></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>