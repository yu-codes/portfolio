<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>超參數最佳化全攻略：Grid/Random/Bayesian、Hyperband 與重現性 - Yu's Portfolio & Learning Hub</title><meta name=description content="超參數最佳化是機器學習模型性能提升的關鍵。從傳統的 Grid/Random Search，到進階的 Bayesian Optimization、Hyperband、Population Based Training（PBT），再到重現性與勢能坑問題，這些技巧與理論是面試與實務不可或缺的能力。本章將深入原理、實作、調參策略、面試熱點與常見誤區，幫助你全面掌握超參數調優。
Grid Search vs. Random Search Grid Search 枚舉所有超參數組合，逐一訓練與評估。 適合超參數數量少、每個值都需測試的情境。 缺點：組合數爆炸，計算成本高。 Random Search 隨機抽樣超參數組合，訓練與評估。 適合超參數多、部分參數影響大時。 理論證明：隨機搜尋常能更快找到好組合。 from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.ensemble import RandomForestClassifier param_grid = {'n_estimators': [50, 100], 'max_depth': [3, 5, 7]} grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3) grid.fit([[0,0],[1,1]], [0,1]) param_dist = {'n_estimators': [10, 50, 100], 'max_depth': [3, 5, 7]} rand = RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter=4, cv=3) rand.fit([[0,0],[1,1]], [0,1]) Bayesian Optimization 用貝式方法建模超參數與分數的關係，根據後驗分布選取下次測試點。 常用 Gaussian Process、Tree-structured Parzen Estimator（TPE）。 優點：能在有限次數內找到更佳組合，適合高成本訓練。 Python 實作（Optuna 範例） import optuna def objective(trial): n_estimators = trial.suggest_int('n_estimators', 10, 100) max_depth = trial.suggest_int('max_depth', 3, 10) # ...訓練模型並回傳分數... return n_estimators - max_depth # 範例 study = optuna.create_study(direction=&#34;maximize&#34;) study.optimize(objective, n_trials=10) print(&#34;最佳參數:&#34;, study.best_params) Hyperband / BOHB / Population Based Training Hyperband 結合隨機搜尋與早停，快速淘汰表現差的組合。 適合大規模超參數搜尋。 BOHB（Bayesian Optimization + Hyperband） 結合 Bayesian Optimization 與 Hyperband，兼顧探索與效率。 Population Based Training（PBT） 多組模型並行訓練，定期交換與微調超參數。 適合深度學習大規模訓練。 勢能坑 (Local Minima) 與重現性 (Reproducibility) 勢能坑（Local Minima） 訓練過程易陷入局部最小值，導致模型表現不穩。 解法：多次初始化、使用動量、調整學習率、集成多模型。 重現性（Reproducibility） 設定隨機種子、固定資料分割、記錄環境與參數，確保結果可重現。 常見於論文、競賽、產業部署。 import numpy as np import torch import random def set_seed(seed=42): np.random.seed(seed) torch.manual_seed(seed) random.seed(seed) torch.cuda.manual_seed_all(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False 理論直覺、應用場景與常見誤區 應用場景 Grid/Random Search：小型專案、少量超參數 Bayesian Optimization/Hyperband：深度學習、大型專案 PBT：分散式訓練、AutoML 常見誤區 忽略超參數間交互作用，僅單獨調整 未設隨機種子，導致結果不穩 只用預設參數，未做調參實驗 忽略早停與資源分配，浪費計算資源 面試熱點與經典問題 主題 常見問題 Grid vs Random 何時選用？優缺點？ Bayesian Opt 原理與優勢？ Hyperband 如何加速搜尋？ 勢能坑 如何避免？ 重現性 如何確保？有哪些步驟？ 使用注意事項 超參數調優需結合交叉驗證與多指標評估。 設定隨機種子與記錄參數，確保實驗可重現。 大型搜尋建議用 Hyperband/BOHB/PBT 提升效率。 延伸閱讀與資源 Optuna 官方文件 Ray Tune: Hyperparameter Search Scikit-learn Hyperparameter Tuning Deep Learning Book: Optimization 經典面試題與解法提示 Grid Search 與 Random Search 差異與適用場景？ Bayesian Optimization 如何選下次測試點？ Hyperband 如何加速搜尋？ 勢能坑與全域最小值的差異？ 如何確保實驗重現性？ PBT 的原理與優缺點？ 超參數調優常見指標有哪些？ 如何用 Python 設定隨機種子？ BOHB 與 Hyperband 差異？ AutoML 如何自動化超參數搜尋？ 結語 超參數最佳化是 ML 成敗的最後一哩路。熟悉 Grid/Random/Bayesian、Hyperband、PBT 與重現性技巧，能讓你打造更強大、穩定的模型並在面試中展現專業素養。下一章將進入貝式方法與機率視角，敬請期待！
"><meta property="og:title" content="超參數最佳化全攻略：Grid/Random/Bayesian、Hyperband 與重現性"><meta property="og:description" content="超參數最佳化是機器學習模型性能提升的關鍵。從傳統的 Grid/Random Search，到進階的 Bayesian Optimization、Hyperband、Population Based Training（PBT），再到重現性與勢能坑問題，這些技巧與理論是面試與實務不可或缺的能力。本章將深入原理、實作、調參策略、面試熱點與常見誤區，幫助你全面掌握超參數調優。
Grid Search vs. Random Search Grid Search 枚舉所有超參數組合，逐一訓練與評估。 適合超參數數量少、每個值都需測試的情境。 缺點：組合數爆炸，計算成本高。 Random Search 隨機抽樣超參數組合，訓練與評估。 適合超參數多、部分參數影響大時。 理論證明：隨機搜尋常能更快找到好組合。 from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.ensemble import RandomForestClassifier param_grid = {'n_estimators': [50, 100], 'max_depth': [3, 5, 7]} grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3) grid.fit([[0,0],[1,1]], [0,1]) param_dist = {'n_estimators': [10, 50, 100], 'max_depth': [3, 5, 7]} rand = RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter=4, cv=3) rand.fit([[0,0],[1,1]], [0,1]) Bayesian Optimization 用貝式方法建模超參數與分數的關係，根據後驗分布選取下次測試點。 常用 Gaussian Process、Tree-structured Parzen Estimator（TPE）。 優點：能在有限次數內找到更佳組合，適合高成本訓練。 Python 實作（Optuna 範例） import optuna def objective(trial): n_estimators = trial.suggest_int('n_estimators', 10, 100) max_depth = trial.suggest_int('max_depth', 3, 10) # ...訓練模型並回傳分數... return n_estimators - max_depth # 範例 study = optuna.create_study(direction=&#34;maximize&#34;) study.optimize(objective, n_trials=10) print(&#34;最佳參數:&#34;, study.best_params) Hyperband / BOHB / Population Based Training Hyperband 結合隨機搜尋與早停，快速淘汰表現差的組合。 適合大規模超參數搜尋。 BOHB（Bayesian Optimization + Hyperband） 結合 Bayesian Optimization 與 Hyperband，兼顧探索與效率。 Population Based Training（PBT） 多組模型並行訓練，定期交換與微調超參數。 適合深度學習大規模訓練。 勢能坑 (Local Minima) 與重現性 (Reproducibility) 勢能坑（Local Minima） 訓練過程易陷入局部最小值，導致模型表現不穩。 解法：多次初始化、使用動量、調整學習率、集成多模型。 重現性（Reproducibility） 設定隨機種子、固定資料分割、記錄環境與參數，確保結果可重現。 常見於論文、競賽、產業部署。 import numpy as np import torch import random def set_seed(seed=42): np.random.seed(seed) torch.manual_seed(seed) random.seed(seed) torch.cuda.manual_seed_all(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False 理論直覺、應用場景與常見誤區 應用場景 Grid/Random Search：小型專案、少量超參數 Bayesian Optimization/Hyperband：深度學習、大型專案 PBT：分散式訓練、AutoML 常見誤區 忽略超參數間交互作用，僅單獨調整 未設隨機種子，導致結果不穩 只用預設參數，未做調參實驗 忽略早停與資源分配，浪費計算資源 面試熱點與經典問題 主題 常見問題 Grid vs Random 何時選用？優缺點？ Bayesian Opt 原理與優勢？ Hyperband 如何加速搜尋？ 勢能坑 如何避免？ 重現性 如何確保？有哪些步驟？ 使用注意事項 超參數調優需結合交叉驗證與多指標評估。 設定隨機種子與記錄參數，確保實驗可重現。 大型搜尋建議用 Hyperband/BOHB/PBT 提升效率。 延伸閱讀與資源 Optuna 官方文件 Ray Tune: Hyperparameter Search Scikit-learn Hyperparameter Tuning Deep Learning Book: Optimization 經典面試題與解法提示 Grid Search 與 Random Search 差異與適用場景？ Bayesian Optimization 如何選下次測試點？ Hyperband 如何加速搜尋？ 勢能坑與全域最小值的差異？ 如何確保實驗重現性？ PBT 的原理與優缺點？ 超參數調優常見指標有哪些？ 如何用 Python 設定隨機種子？ BOHB 與 Hyperband 差異？ AutoML 如何自動化超參數搜尋？ 結語 超參數最佳化是 ML 成敗的最後一哩路。熟悉 Grid/Random/Bayesian、Hyperband、PBT 與重現性技巧，能讓你打造更強大、穩定的模型並在面試中展現專業素養。下一章將進入貝式方法與機率視角，敬請期待！
"><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/hyperparameter-optimization/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/hyperparameter-optimization/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>超參數最佳化全攻略：Grid/Random/Bayesian、Hyperband 與重現性</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>超參數最佳化全攻略：Grid/Random/Bayesian、Hyperband 與重現性</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-04-11</span></div></header><div class=article-body><p>超參數最佳化是機器學習模型性能提升的關鍵。從傳統的 Grid/Random Search，到進階的 Bayesian Optimization、Hyperband、Population Based Training（PBT），再到重現性與勢能坑問題，這些技巧與理論是面試與實務不可或缺的能力。本章將深入原理、實作、調參策略、面試熱點與常見誤區，幫助你全面掌握超參數調優。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#grid-search-vs-random-search>Grid Search vs. Random Search</a><ul><li><a href=#grid-search>Grid Search</a></li><li><a href=#random-search>Random Search</a></li></ul></li><li><a href=#bayesian-optimization>Bayesian Optimization</a><ul><li><a href=#python-實作optuna-範例>Python 實作（Optuna 範例）</a></li></ul></li><li><a href=#hyperband--bohb--population-based-training>Hyperband / BOHB / Population Based Training</a><ul><li><a href=#hyperband>Hyperband</a></li><li><a href=#bohbbayesian-optimization--hyperband>BOHB（Bayesian Optimization + Hyperband）</a></li><li><a href=#population-based-trainingpbt>Population Based Training（PBT）</a></li></ul></li><li><a href=#勢能坑-local-minima-與重現性-reproducibility>勢能坑 (Local Minima) 與重現性 (Reproducibility)</a><ul><li><a href=#勢能坑local-minima>勢能坑（Local Minima）</a></li><li><a href=#重現性reproducibility>重現性（Reproducibility）</a></li></ul></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=grid-search-vs-random-search>Grid Search vs. Random Search</h2><h3 id=grid-search>Grid Search</h3><ul><li>枚舉所有超參數組合，逐一訓練與評估。</li><li>適合超參數數量少、每個值都需測試的情境。</li><li>缺點：組合數爆炸，計算成本高。</li></ul><h3 id=random-search>Random Search</h3><ul><li>隨機抽樣超參數組合，訓練與評估。</li><li>適合超參數多、部分參數影響大時。</li><li>理論證明：隨機搜尋常能更快找到好組合。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> GridSearchCV, RandomizedSearchCV
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>param_grid <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;n_estimators&#39;</span>: [<span style=color:#ae81ff>50</span>, <span style=color:#ae81ff>100</span>], <span style=color:#e6db74>&#39;max_depth&#39;</span>: [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>7</span>]}
</span></span><span style=display:flex><span>grid <span style=color:#f92672>=</span> GridSearchCV(RandomForestClassifier(), param_grid, cv<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>grid<span style=color:#f92672>.</span>fit([[<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>],[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>]], [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>param_dist <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;n_estimators&#39;</span>: [<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>50</span>, <span style=color:#ae81ff>100</span>], <span style=color:#e6db74>&#39;max_depth&#39;</span>: [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>7</span>]}
</span></span><span style=display:flex><span>rand <span style=color:#f92672>=</span> RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, cv<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>rand<span style=color:#f92672>.</span>fit([[<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>],[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>]], [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>])
</span></span></code></pre></div><hr><h2 id=bayesian-optimization>Bayesian Optimization</h2><ul><li>用貝式方法建模超參數與分數的關係，根據後驗分布選取下次測試點。</li><li>常用 Gaussian Process、Tree-structured Parzen Estimator（TPE）。</li><li>優點：能在有限次數內找到更佳組合，適合高成本訓練。</li></ul><h3 id=python-實作optuna-範例>Python 實作（Optuna 範例）</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> optuna
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>objective</span>(trial):
</span></span><span style=display:flex><span>    n_estimators <span style=color:#f92672>=</span> trial<span style=color:#f92672>.</span>suggest_int(<span style=color:#e6db74>&#39;n_estimators&#39;</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    max_depth <span style=color:#f92672>=</span> trial<span style=color:#f92672>.</span>suggest_int(<span style=color:#e6db74>&#39;max_depth&#39;</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...訓練模型並回傳分數...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> n_estimators <span style=color:#f92672>-</span> max_depth  <span style=color:#75715e># 範例</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>study <span style=color:#f92672>=</span> optuna<span style=color:#f92672>.</span>create_study(direction<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;maximize&#34;</span>)
</span></span><span style=display:flex><span>study<span style=color:#f92672>.</span>optimize(objective, n_trials<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;最佳參數:&#34;</span>, study<span style=color:#f92672>.</span>best_params)
</span></span></code></pre></div><hr><h2 id=hyperband--bohb--population-based-training>Hyperband / BOHB / Population Based Training</h2><h3 id=hyperband>Hyperband</h3><ul><li>結合隨機搜尋與早停，快速淘汰表現差的組合。</li><li>適合大規模超參數搜尋。</li></ul><h3 id=bohbbayesian-optimization--hyperband>BOHB（Bayesian Optimization + Hyperband）</h3><ul><li>結合 Bayesian Optimization 與 Hyperband，兼顧探索與效率。</li></ul><h3 id=population-based-trainingpbt>Population Based Training（PBT）</h3><ul><li>多組模型並行訓練，定期交換與微調超參數。</li><li>適合深度學習大規模訓練。</li></ul><hr><h2 id=勢能坑-local-minima-與重現性-reproducibility>勢能坑 (Local Minima) 與重現性 (Reproducibility)</h2><h3 id=勢能坑local-minima>勢能坑（Local Minima）</h3><ul><li>訓練過程易陷入局部最小值，導致模型表現不穩。</li><li>解法：多次初始化、使用動量、調整學習率、集成多模型。</li></ul><h3 id=重現性reproducibility>重現性（Reproducibility）</h3><ul><li>設定隨機種子、固定資料分割、記錄環境與參數，確保結果可重現。</li><li>常見於論文、競賽、產業部署。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>set_seed</span>(seed<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>):
</span></span><span style=display:flex><span>    np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(seed)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>manual_seed(seed)
</span></span><span style=display:flex><span>    random<span style=color:#f92672>.</span>seed(seed)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>manual_seed_all(seed)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>backends<span style=color:#f92672>.</span>cudnn<span style=color:#f92672>.</span>deterministic <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>backends<span style=color:#f92672>.</span>cudnn<span style=color:#f92672>.</span>benchmark <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>Grid/Random Search：小型專案、少量超參數</li><li>Bayesian Optimization/Hyperband：深度學習、大型專案</li><li>PBT：分散式訓練、AutoML</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>忽略超參數間交互作用，僅單獨調整</li><li>未設隨機種子，導致結果不穩</li><li>只用預設參數，未做調參實驗</li><li>忽略早停與資源分配，浪費計算資源</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>Grid vs Random</td><td>何時選用？優缺點？</td></tr><tr><td>Bayesian Opt</td><td>原理與優勢？</td></tr><tr><td>Hyperband</td><td>如何加速搜尋？</td></tr><tr><td>勢能坑</td><td>如何避免？</td></tr><tr><td>重現性</td><td>如何確保？有哪些步驟？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>超參數調優需結合交叉驗證與多指標評估。</li><li>設定隨機種子與記錄參數，確保實驗可重現。</li><li>大型搜尋建議用 Hyperband/BOHB/PBT 提升效率。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://optuna.org/>Optuna 官方文件</a></li><li><a href=https://docs.ray.io/en/latest/tune/index.html>Ray Tune: Hyperparameter Search</a></li><li><a href=https://scikit-learn.org/stable/modules/grid_search.html>Scikit-learn Hyperparameter Tuning</a></li><li><a href=https://www.deeplearningbook.org/contents/optimization.html>Deep Learning Book: Optimization</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>Grid Search 與 Random Search 差異與適用場景？</li><li>Bayesian Optimization 如何選下次測試點？</li><li>Hyperband 如何加速搜尋？</li><li>勢能坑與全域最小值的差異？</li><li>如何確保實驗重現性？</li><li>PBT 的原理與優缺點？</li><li>超參數調優常見指標有哪些？</li><li>如何用 Python 設定隨機種子？</li><li>BOHB 與 Hyperband 差異？</li><li>AutoML 如何自動化超參數搜尋？</li></ol><hr><h2 id=結語>結語</h2><p>超參數最佳化是 ML 成敗的最後一哩路。熟悉 Grid/Random/Bayesian、Hyperband、PBT 與重現性技巧，能讓你打造更強大、穩定的模型並在面試中展現專業素養。下一章將進入貝式方法與機率視角，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>Grid Search</span>
<span class=tag>Random Search</span>
<span class=tag>Bayesian Optimization</span>
<span class=tag>Hyperband</span>
<span class=tag>Reproducibility</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>