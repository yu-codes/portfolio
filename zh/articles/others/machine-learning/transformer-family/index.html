<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Transformer 家族全解析：結構、位置編碼、複雜度與主流模型比較 - Yu's Portfolio & Learning Hub</title><meta name=description content='Transformer 架構徹底改變了深度學習格局，從 NLP 到 Vision、語音、推薦系統皆有應用。本章將深入 Encoder/Decoder Block 結構、位置編碼（Sinusoid, ALiBi, RoPE）、參數膨脹與計算複雜度、以及 BERT、GPT、DeiT、Swin 等主流模型差異，結合理論、圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握 Transformer 家族。
Encoder / Decoder Block 結構 Encoder Block 多層 Self-Attention + Feedforward + 殘差連接 + LayerNorm 輸入序列同時處理，捕捉全局依賴 Decoder Block Masked Self-Attention（防洩漏未來資訊）+ Encoder-Decoder Attention + Feedforward 適合自回歸生成（如翻譯、摘要） import torch.nn as nn class TransformerEncoderBlock(nn.Module): def __init__(self, d_model, nhead): super().__init__() self.attn = nn.MultiheadAttention(d_model, nhead) self.ff = nn.Sequential( nn.Linear(d_model, d_model*4), nn.ReLU(), nn.Linear(d_model*4, d_model) ) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) def forward(self, x): attn_out, _ = self.attn(x, x, x) x = self.norm1(x + attn_out) ff_out = self.ff(x) return self.norm2(x + ff_out) Positional Encoding：Sinusoid, ALiBi, RoPE Sinusoid Encoding 用正弦/餘弦函數給每個位置唯一編碼，無需學習參數 公式：$PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})$ ALiBi（Attention with Linear Biases） 用線性偏置調整注意力分數，提升長序列泛化 RoPE（Rotary Position Embedding） 用複數旋轉方式編碼位置，提升長距依賴建模 參數膨脹 vs. 計算複雜度（Quadratic → Linear 改進） 標準 Self-Attention 計算複雜度為 $O(n^2)$，n 為序列長度 長序列改進：Sparse Attention、Performer、Longformer、FlashAttention 等，複雜度降至 $O(n)$ 或 $O(n \log n)$ BERT、GPT、DeiT、Swin 頂層差異 模型 架構特點 任務 代表應用 BERT 雙向 Encoder 預訓練+微調 NLP 理解、問答 GPT 單向 Decoder 自回歸生成 NLP 生成、對話 DeiT ViT 改進，圖像分類 Encoder Vision Swin 局部窗口+移動，層次結構 Encoder 影像分割、檢測 Python 實作：位置編碼 import torch import math def sinusoid_encoding(seq_len, d_model): pe = torch.zeros(seq_len, d_model) position = torch.arange(0, seq_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) return pe print("Sinusoid Encoding:", sinusoid_encoding(5, 8)) 理論直覺、應用場景與常見誤區 應用場景 NLP（BERT、GPT）、Vision（ViT、DeiT、Swin）、語音、推薦系統 常見誤區 忽略位置編碼，導致序列資訊丟失 長序列未優化注意力，計算資源爆炸 Encoder/Decoder Block 混用，導致模型無法訓練 面試熱點與經典問題 主題 常見問題 Encoder/Decoder 結構差異與應用？ Positional Encoding 為何需要？有何種類？ Self-Attention 複雜度 如何優化？ BERT vs GPT 架構與任務差異？ Swin/DeiT Vision Transformer 有何創新？ 使用注意事項 長序列建議用線性/稀疏注意力 位置編碼需與模型架構匹配 Encoder/Decoder Block 須根據任務選擇 延伸閱讀與資源 Attention is All You Need 論文 BERT 論文 GPT 論文 Swin Transformer 論文 FlashAttention 論文 經典面試題與解法提示 Encoder/Decoder Block 結構與差異？ Sinusoid/ALiBi/RoPE 位置編碼原理？ Self-Attention 複雜度如何優化？ BERT 與 GPT 架構與訓練差異？ DeiT/Swin 在 Vision Transformer 的創新？ 長序列 Transformer 如何設計？ 位置編碼缺失會有什麼問題？ 如何用 Python 實作位置編碼？ Encoder/Decoder Block 混用會有什麼後果？ FlashAttention 有何優勢？ 結語 Transformer 家族是現代深度學習的核心。熟悉 Encoder/Decoder 結構、位置編碼、計算複雜度與主流模型差異，能讓你在 NLP、Vision、生成模型等領域發揮 Transformer 強大威力。下一章將進入預訓練策略與微調，敬請期待！
'><meta property="og:title" content="Transformer 家族全解析：結構、位置編碼、複雜度與主流模型比較"><meta property="og:description" content='Transformer 架構徹底改變了深度學習格局，從 NLP 到 Vision、語音、推薦系統皆有應用。本章將深入 Encoder/Decoder Block 結構、位置編碼（Sinusoid, ALiBi, RoPE）、參數膨脹與計算複雜度、以及 BERT、GPT、DeiT、Swin 等主流模型差異，結合理論、圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握 Transformer 家族。
Encoder / Decoder Block 結構 Encoder Block 多層 Self-Attention + Feedforward + 殘差連接 + LayerNorm 輸入序列同時處理，捕捉全局依賴 Decoder Block Masked Self-Attention（防洩漏未來資訊）+ Encoder-Decoder Attention + Feedforward 適合自回歸生成（如翻譯、摘要） import torch.nn as nn class TransformerEncoderBlock(nn.Module): def __init__(self, d_model, nhead): super().__init__() self.attn = nn.MultiheadAttention(d_model, nhead) self.ff = nn.Sequential( nn.Linear(d_model, d_model*4), nn.ReLU(), nn.Linear(d_model*4, d_model) ) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) def forward(self, x): attn_out, _ = self.attn(x, x, x) x = self.norm1(x + attn_out) ff_out = self.ff(x) return self.norm2(x + ff_out) Positional Encoding：Sinusoid, ALiBi, RoPE Sinusoid Encoding 用正弦/餘弦函數給每個位置唯一編碼，無需學習參數 公式：$PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})$ ALiBi（Attention with Linear Biases） 用線性偏置調整注意力分數，提升長序列泛化 RoPE（Rotary Position Embedding） 用複數旋轉方式編碼位置，提升長距依賴建模 參數膨脹 vs. 計算複雜度（Quadratic → Linear 改進） 標準 Self-Attention 計算複雜度為 $O(n^2)$，n 為序列長度 長序列改進：Sparse Attention、Performer、Longformer、FlashAttention 等，複雜度降至 $O(n)$ 或 $O(n \log n)$ BERT、GPT、DeiT、Swin 頂層差異 模型 架構特點 任務 代表應用 BERT 雙向 Encoder 預訓練+微調 NLP 理解、問答 GPT 單向 Decoder 自回歸生成 NLP 生成、對話 DeiT ViT 改進，圖像分類 Encoder Vision Swin 局部窗口+移動，層次結構 Encoder 影像分割、檢測 Python 實作：位置編碼 import torch import math def sinusoid_encoding(seq_len, d_model): pe = torch.zeros(seq_len, d_model) position = torch.arange(0, seq_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) return pe print("Sinusoid Encoding:", sinusoid_encoding(5, 8)) 理論直覺、應用場景與常見誤區 應用場景 NLP（BERT、GPT）、Vision（ViT、DeiT、Swin）、語音、推薦系統 常見誤區 忽略位置編碼，導致序列資訊丟失 長序列未優化注意力，計算資源爆炸 Encoder/Decoder Block 混用，導致模型無法訓練 面試熱點與經典問題 主題 常見問題 Encoder/Decoder 結構差異與應用？ Positional Encoding 為何需要？有何種類？ Self-Attention 複雜度 如何優化？ BERT vs GPT 架構與任務差異？ Swin/DeiT Vision Transformer 有何創新？ 使用注意事項 長序列建議用線性/稀疏注意力 位置編碼需與模型架構匹配 Encoder/Decoder Block 須根據任務選擇 延伸閱讀與資源 Attention is All You Need 論文 BERT 論文 GPT 論文 Swin Transformer 論文 FlashAttention 論文 經典面試題與解法提示 Encoder/Decoder Block 結構與差異？ Sinusoid/ALiBi/RoPE 位置編碼原理？ Self-Attention 複雜度如何優化？ BERT 與 GPT 架構與訓練差異？ DeiT/Swin 在 Vision Transformer 的創新？ 長序列 Transformer 如何設計？ 位置編碼缺失會有什麼問題？ 如何用 Python 實作位置編碼？ Encoder/Decoder Block 混用會有什麼後果？ FlashAttention 有何優勢？ 結語 Transformer 家族是現代深度學習的核心。熟悉 Encoder/Decoder 結構、位置編碼、計算複雜度與主流模型差異，能讓你在 NLP、Vision、生成模型等領域發揮 Transformer 強大威力。下一章將進入預訓練策略與微調，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/transformer-family/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/transformer-family/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>Transformer 家族全解析：結構、位置編碼、複雜度與主流模型比較</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>Transformer 家族全解析：結構、位置編碼、複雜度與主流模型比較</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-02-21</span></div></header><div class=article-body><p>Transformer 架構徹底改變了深度學習格局，從 NLP 到 Vision、語音、推薦系統皆有應用。本章將深入 Encoder/Decoder Block 結構、位置編碼（Sinusoid, ALiBi, RoPE）、參數膨脹與計算複雜度、以及 BERT、GPT、DeiT、Swin 等主流模型差異，結合理論、圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握 Transformer 家族。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#encoder--decoder-block-結構>Encoder / Decoder Block 結構</a><ul><li><a href=#encoder-block>Encoder Block</a></li><li><a href=#decoder-block>Decoder Block</a></li></ul></li><li><a href=#positional-encodingsinusoid-alibi-rope>Positional Encoding：Sinusoid, ALiBi, RoPE</a><ul><li><a href=#sinusoid-encoding>Sinusoid Encoding</a></li><li><a href=#alibiattention-with-linear-biases>ALiBi（Attention with Linear Biases）</a></li><li><a href=#roperotary-position-embedding>RoPE（Rotary Position Embedding）</a></li></ul></li><li><a href=#參數膨脹-vs-計算複雜度quadratic--linear-改進>參數膨脹 vs. 計算複雜度（Quadratic → Linear 改進）</a></li><li><a href=#bertgptdeitswin-頂層差異>BERT、GPT、DeiT、Swin 頂層差異</a></li><li><a href=#python-實作位置編碼>Python 實作：位置編碼</a></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=encoder--decoder-block-結構>Encoder / Decoder Block 結構</h2><h3 id=encoder-block>Encoder Block</h3><ul><li>多層 Self-Attention + Feedforward + 殘差連接 + LayerNorm</li><li>輸入序列同時處理，捕捉全局依賴</li></ul><h3 id=decoder-block>Decoder Block</h3><ul><li>Masked Self-Attention（防洩漏未來資訊）+ Encoder-Decoder Attention + Feedforward</li><li>適合自回歸生成（如翻譯、摘要）</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TransformerEncoderBlock</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, nhead):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>attn <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MultiheadAttention(d_model, nhead)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>ff <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(d_model, d_model<span style=color:#f92672>*</span><span style=color:#ae81ff>4</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(d_model<span style=color:#f92672>*</span><span style=color:#ae81ff>4</span>, d_model)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(d_model)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        attn_out, _ <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>attn(x, x, x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>norm1(x <span style=color:#f92672>+</span> attn_out)
</span></span><span style=display:flex><span>        ff_out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>ff(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>norm2(x <span style=color:#f92672>+</span> ff_out)
</span></span></code></pre></div><hr><h2 id=positional-encodingsinusoid-alibi-rope>Positional Encoding：Sinusoid, ALiBi, RoPE</h2><h3 id=sinusoid-encoding>Sinusoid Encoding</h3><ul><li>用正弦/餘弦函數給每個位置唯一編碼，無需學習參數</li><li>公式：$PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})$</li></ul><h3 id=alibiattention-with-linear-biases>ALiBi（Attention with Linear Biases）</h3><ul><li>用線性偏置調整注意力分數，提升長序列泛化</li></ul><h3 id=roperotary-position-embedding>RoPE（Rotary Position Embedding）</h3><ul><li>用複數旋轉方式編碼位置，提升長距依賴建模</li></ul><hr><h2 id=參數膨脹-vs-計算複雜度quadratic--linear-改進>參數膨脹 vs. 計算複雜度（Quadratic → Linear 改進）</h2><ul><li>標準 Self-Attention 計算複雜度為 $O(n^2)$，n 為序列長度</li><li>長序列改進：Sparse Attention、Performer、Longformer、FlashAttention 等，複雜度降至 $O(n)$ 或 $O(n \log n)$</li></ul><hr><h2 id=bertgptdeitswin-頂層差異>BERT、GPT、DeiT、Swin 頂層差異</h2><table><thead><tr><th>模型</th><th>架構特點</th><th>任務</th><th>代表應用</th></tr></thead><tbody><tr><td>BERT</td><td>雙向 Encoder</td><td>預訓練+微調</td><td>NLP 理解、問答</td></tr><tr><td>GPT</td><td>單向 Decoder</td><td>自回歸生成</td><td>NLP 生成、對話</td></tr><tr><td>DeiT</td><td>ViT 改進，圖像分類</td><td>Encoder</td><td>Vision</td></tr><tr><td>Swin</td><td>局部窗口+移動，層次結構</td><td>Encoder</td><td>影像分割、檢測</td></tr></tbody></table><hr><h2 id=python-實作位置編碼>Python 實作：位置編碼</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sinusoid_encoding</span>(seq_len, d_model):
</span></span><span style=display:flex><span>    pe <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(seq_len, d_model)
</span></span><span style=display:flex><span>    position <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, seq_len)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    div_term <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>exp(torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, d_model, <span style=color:#ae81ff>2</span>) <span style=color:#f92672>*</span> (<span style=color:#f92672>-</span>math<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>10000.0</span>) <span style=color:#f92672>/</span> d_model))
</span></span><span style=display:flex><span>    pe[:, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sin(position <span style=color:#f92672>*</span> div_term)
</span></span><span style=display:flex><span>    pe[:, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cos(position <span style=color:#f92672>*</span> div_term)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pe
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Sinusoid Encoding:&#34;</span>, sinusoid_encoding(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>8</span>))
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>NLP（BERT、GPT）、Vision（ViT、DeiT、Swin）、語音、推薦系統</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>忽略位置編碼，導致序列資訊丟失</li><li>長序列未優化注意力，計算資源爆炸</li><li>Encoder/Decoder Block 混用，導致模型無法訓練</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>Encoder/Decoder</td><td>結構差異與應用？</td></tr><tr><td>Positional Encoding</td><td>為何需要？有何種類？</td></tr><tr><td>Self-Attention 複雜度</td><td>如何優化？</td></tr><tr><td>BERT vs GPT</td><td>架構與任務差異？</td></tr><tr><td>Swin/DeiT</td><td>Vision Transformer 有何創新？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>長序列建議用線性/稀疏注意力</li><li>位置編碼需與模型架構匹配</li><li>Encoder/Decoder Block 須根據任務選擇</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://arxiv.org/abs/1706.03762>Attention is All You Need 論文</a></li><li><a href=https://arxiv.org/abs/1810.04805>BERT 論文</a></li><li><a href=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf>GPT 論文</a></li><li><a href=https://arxiv.org/abs/2103.14030>Swin Transformer 論文</a></li><li><a href=https://arxiv.org/abs/2205.14135>FlashAttention 論文</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>Encoder/Decoder Block 結構與差異？</li><li>Sinusoid/ALiBi/RoPE 位置編碼原理？</li><li>Self-Attention 複雜度如何優化？</li><li>BERT 與 GPT 架構與訓練差異？</li><li>DeiT/Swin 在 Vision Transformer 的創新？</li><li>長序列 Transformer 如何設計？</li><li>位置編碼缺失會有什麼問題？</li><li>如何用 Python 實作位置編碼？</li><li>Encoder/Decoder Block 混用會有什麼後果？</li><li>FlashAttention 有何優勢？</li></ol><hr><h2 id=結語>結語</h2><p>Transformer 家族是現代深度學習的核心。熟悉 Encoder/Decoder 結構、位置編碼、計算複雜度與主流模型差異，能讓你在 NLP、Vision、生成模型等領域發揮 Transformer 強大威力。下一章將進入預訓練策略與微調，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>Transformer</span>
<span class=tag>Encoder</span>
<span class=tag>Decoder</span>
<span class=tag>Positional Encoding</span>
<span class=tag>BERT</span>
<span class=tag>GPT</span>
<span class=tag>DeiT</span>
<span class=tag>Swin</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>