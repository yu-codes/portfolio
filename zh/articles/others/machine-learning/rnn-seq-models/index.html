<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>循環與序列模型全解析：RNN、LSTM、GRU、Seq2Seq 與時序預測 - Yu's Portfolio & Learning Hub</title><meta name=description content='循環神經網路（RNN）及其變體是處理序列資料（如語音、文本、時間序列）的核心。從 Vanilla RNN 的梯度爆炸/消失，到 LSTM/GRU 的 gating 機制、Seq2Seq+Attention、Bi-directional RNN，再到時序預測技巧（Teacher Forcing、Scheduled Sampling），本章將結合理論、圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握序列建模。
Vanilla RNN 與梯度爆炸/消失 RNN 結構 每步輸出依賴前一狀態與當前輸入：$h_t = f(Wx_t + Uh_{t-1} + b)$ 適合序列建模，但長序列訓練困難 梯度爆炸/消失 反向傳播時，梯度連乘導致指數級增大（爆炸）或趨近 0（消失） 影響長期依賴學習 解法 梯度裁剪（clipping）、LSTM/GRU gating、殘差連接 import torch import torch.nn as nn rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1, batch_first=True) x = torch.randn(5, 7, 10) # batch, seq, feature out, h = rnn(x) print("RNN 輸出 shape:", out.shape) LSTM / GRU gating 機制 LSTM（Long Short-Term Memory） 引入輸入、遺忘、輸出閘門，能記憶長期資訊 避免梯度消失，提升長序列學習能力 GRU（Gated Recurrent Unit） 結構更簡單，合併部分閘門，訓練更快 lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=1, batch_first=True) gru = nn.GRU(input_size=10, hidden_size=20, num_layers=1, batch_first=True) out_lstm, _ = lstm(x) out_gru, _ = gru(x) print("LSTM 輸出 shape:", out_lstm.shape) print("GRU 輸出 shape:", out_gru.shape) Seq2Seq＋Attention, Bi-directional RNN Seq2Seq 架構 Encoder-Decoder 結構，常用於翻譯、摘要、對話生成 Encoder 將輸入序列壓縮為上下文向量，Decoder 逐步生成輸出 Attention 機制 讓 Decoder 每步都能關注 Encoder 不同部分，解決長序列資訊瓶頸 Bi-directional RNN 同時考慮前後文，提升序列理解能力 bi_rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1, batch_first=True, bidirectional=True) out_bi, _ = bi_rnn(x) print("Bi-RNN 輸出 shape:", out_bi.shape) 時序預測：Teacher Forcing、Scheduled Sampling Teacher Forcing 訓練時用真實標籤作為下一步輸入，加速收斂 缺點：推論時誤差累積（Exposure Bias） Scheduled Sampling 逐步減少 Teacher Forcing 機率，提升模型魯棒性 理論直覺、應用場景與常見誤區 應用場景 語音辨識、機器翻譯、對話生成、時間序列預測、NLP 任務 常見誤區 忽略梯度爆炸/消失，導致訓練失敗 LSTM/GRU 結構選擇不當，模型過大或過簡 Seq2Seq 未加 Attention，長序列表現差 Teacher Forcing 機率設置不合理，推論效果不佳 面試熱點與經典問題 主題 常見問題 RNN 為何會梯度爆炸/消失？如何解決？ LSTM/GRU 閘門結構與優缺點？ Seq2Seq Encoder-Decoder 如何運作？ Attention 如何幫助長序列建模？ Teacher Forcing 有何優缺點？ 使用注意事項 長序列建議用 LSTM/GRU + Attention 訓練時監控梯度，必要時啟用梯度裁剪 Teacher Forcing/Scheduled Sampling 需根據任務調整 延伸閱讀與資源 Deep Learning Book: Sequence Modeling PyTorch RNN 官方文件 Attention is All You Need 論文 Scheduled Sampling 論文 經典面試題與解法提示 RNN 為何會梯度爆炸/消失？數學推導？ LSTM/GRU 閘門結構與公式？ Seq2Seq 架構與應用場景？ Attention 機制數學原理？ Bi-RNN 有何優勢？ Teacher Forcing 與 Scheduled Sampling 差異？ 如何用 Python 實作 LSTM/GRU？ RNN 在 NLP/時序預測的應用？ Exposure Bias 是什麼？如何緩解？ Seq2Seq 未加 Attention 有何缺點？ 結語 循環與序列模型是處理時序與語言資料的關鍵。熟悉 RNN、LSTM、GRU、Seq2Seq、Attention 與時序預測技巧，能讓你在 NLP、語音、金融等領域發揮深度學習實力。下一章將進入 Attention 機制拆解，敬請期待！
'><meta property="og:title" content="循環與序列模型全解析：RNN、LSTM、GRU、Seq2Seq 與時序預測"><meta property="og:description" content='循環神經網路（RNN）及其變體是處理序列資料（如語音、文本、時間序列）的核心。從 Vanilla RNN 的梯度爆炸/消失，到 LSTM/GRU 的 gating 機制、Seq2Seq+Attention、Bi-directional RNN，再到時序預測技巧（Teacher Forcing、Scheduled Sampling），本章將結合理論、圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握序列建模。
Vanilla RNN 與梯度爆炸/消失 RNN 結構 每步輸出依賴前一狀態與當前輸入：$h_t = f(Wx_t + Uh_{t-1} + b)$ 適合序列建模，但長序列訓練困難 梯度爆炸/消失 反向傳播時，梯度連乘導致指數級增大（爆炸）或趨近 0（消失） 影響長期依賴學習 解法 梯度裁剪（clipping）、LSTM/GRU gating、殘差連接 import torch import torch.nn as nn rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1, batch_first=True) x = torch.randn(5, 7, 10) # batch, seq, feature out, h = rnn(x) print("RNN 輸出 shape:", out.shape) LSTM / GRU gating 機制 LSTM（Long Short-Term Memory） 引入輸入、遺忘、輸出閘門，能記憶長期資訊 避免梯度消失，提升長序列學習能力 GRU（Gated Recurrent Unit） 結構更簡單，合併部分閘門，訓練更快 lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=1, batch_first=True) gru = nn.GRU(input_size=10, hidden_size=20, num_layers=1, batch_first=True) out_lstm, _ = lstm(x) out_gru, _ = gru(x) print("LSTM 輸出 shape:", out_lstm.shape) print("GRU 輸出 shape:", out_gru.shape) Seq2Seq＋Attention, Bi-directional RNN Seq2Seq 架構 Encoder-Decoder 結構，常用於翻譯、摘要、對話生成 Encoder 將輸入序列壓縮為上下文向量，Decoder 逐步生成輸出 Attention 機制 讓 Decoder 每步都能關注 Encoder 不同部分，解決長序列資訊瓶頸 Bi-directional RNN 同時考慮前後文，提升序列理解能力 bi_rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1, batch_first=True, bidirectional=True) out_bi, _ = bi_rnn(x) print("Bi-RNN 輸出 shape:", out_bi.shape) 時序預測：Teacher Forcing、Scheduled Sampling Teacher Forcing 訓練時用真實標籤作為下一步輸入，加速收斂 缺點：推論時誤差累積（Exposure Bias） Scheduled Sampling 逐步減少 Teacher Forcing 機率，提升模型魯棒性 理論直覺、應用場景與常見誤區 應用場景 語音辨識、機器翻譯、對話生成、時間序列預測、NLP 任務 常見誤區 忽略梯度爆炸/消失，導致訓練失敗 LSTM/GRU 結構選擇不當，模型過大或過簡 Seq2Seq 未加 Attention，長序列表現差 Teacher Forcing 機率設置不合理，推論效果不佳 面試熱點與經典問題 主題 常見問題 RNN 為何會梯度爆炸/消失？如何解決？ LSTM/GRU 閘門結構與優缺點？ Seq2Seq Encoder-Decoder 如何運作？ Attention 如何幫助長序列建模？ Teacher Forcing 有何優缺點？ 使用注意事項 長序列建議用 LSTM/GRU + Attention 訓練時監控梯度，必要時啟用梯度裁剪 Teacher Forcing/Scheduled Sampling 需根據任務調整 延伸閱讀與資源 Deep Learning Book: Sequence Modeling PyTorch RNN 官方文件 Attention is All You Need 論文 Scheduled Sampling 論文 經典面試題與解法提示 RNN 為何會梯度爆炸/消失？數學推導？ LSTM/GRU 閘門結構與公式？ Seq2Seq 架構與應用場景？ Attention 機制數學原理？ Bi-RNN 有何優勢？ Teacher Forcing 與 Scheduled Sampling 差異？ 如何用 Python 實作 LSTM/GRU？ RNN 在 NLP/時序預測的應用？ Exposure Bias 是什麼？如何緩解？ Seq2Seq 未加 Attention 有何缺點？ 結語 循環與序列模型是處理時序與語言資料的關鍵。熟悉 RNN、LSTM、GRU、Seq2Seq、Attention 與時序預測技巧，能讓你在 NLP、語音、金融等領域發揮深度學習實力。下一章將進入 Attention 機制拆解，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/rnn-seq-models/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/rnn-seq-models/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>循環與序列模型全解析：RNN、LSTM、GRU、Seq2Seq 與時序預測</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>循環與序列模型全解析：RNN、LSTM、GRU、Seq2Seq 與時序預測</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-03-21</span></div></header><div class=article-body><p>循環神經網路（RNN）及其變體是處理序列資料（如語音、文本、時間序列）的核心。從 Vanilla RNN 的梯度爆炸/消失，到 LSTM/GRU 的 gating 機制、Seq2Seq+Attention、Bi-directional RNN，再到時序預測技巧（Teacher Forcing、Scheduled Sampling），本章將結合理論、圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握序列建模。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#vanilla-rnn-與梯度爆炸消失>Vanilla RNN 與梯度爆炸/消失</a><ul><li><a href=#rnn-結構>RNN 結構</a></li><li><a href=#梯度爆炸消失>梯度爆炸/消失</a></li></ul></li><li><a href=#lstm--gru-gating-機制>LSTM / GRU gating 機制</a><ul><li><a href=#lstmlong-short-term-memory>LSTM（Long Short-Term Memory）</a></li><li><a href=#grugated-recurrent-unit>GRU（Gated Recurrent Unit）</a></li></ul></li><li><a href=#seq2seqattention-bi-directional-rnn>Seq2Seq＋Attention, Bi-directional RNN</a><ul><li><a href=#seq2seq-架構>Seq2Seq 架構</a></li><li><a href=#attention-機制>Attention 機制</a></li><li><a href=#bi-directional-rnn>Bi-directional RNN</a></li></ul></li><li><a href=#時序預測teacher-forcingscheduled-sampling>時序預測：Teacher Forcing、Scheduled Sampling</a><ul><li><a href=#teacher-forcing>Teacher Forcing</a></li><li><a href=#scheduled-sampling>Scheduled Sampling</a></li></ul></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=vanilla-rnn-與梯度爆炸消失>Vanilla RNN 與梯度爆炸/消失</h2><h3 id=rnn-結構>RNN 結構</h3><ul><li>每步輸出依賴前一狀態與當前輸入：$h_t = f(Wx_t + Uh_{t-1} + b)$</li><li>適合序列建模，但長序列訓練困難</li></ul><h3 id=梯度爆炸消失>梯度爆炸/消失</h3><ul><li>反向傳播時，梯度連乘導致指數級增大（爆炸）或趨近 0（消失）</li><li>影響長期依賴學習</li></ul><h4 id=解法>解法</h4><ul><li>梯度裁剪（clipping）、LSTM/GRU gating、殘差連接</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rnn <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>RNN(input_size<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, hidden_size<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, num_layers<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, batch_first<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>10</span>)  <span style=color:#75715e># batch, seq, feature</span>
</span></span><span style=display:flex><span>out, h <span style=color:#f92672>=</span> rnn(x)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;RNN 輸出 shape:&#34;</span>, out<span style=color:#f92672>.</span>shape)
</span></span></code></pre></div><hr><h2 id=lstm--gru-gating-機制>LSTM / GRU gating 機制</h2><h3 id=lstmlong-short-term-memory>LSTM（Long Short-Term Memory）</h3><ul><li>引入輸入、遺忘、輸出閘門，能記憶長期資訊</li><li>避免梯度消失，提升長序列學習能力</li></ul><h3 id=grugated-recurrent-unit>GRU（Gated Recurrent Unit）</h3><ul><li>結構更簡單，合併部分閘門，訓練更快</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>lstm <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LSTM(input_size<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, hidden_size<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, num_layers<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, batch_first<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>gru <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>GRU(input_size<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, hidden_size<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, num_layers<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, batch_first<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>out_lstm, _ <span style=color:#f92672>=</span> lstm(x)
</span></span><span style=display:flex><span>out_gru, _ <span style=color:#f92672>=</span> gru(x)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;LSTM 輸出 shape:&#34;</span>, out_lstm<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;GRU 輸出 shape:&#34;</span>, out_gru<span style=color:#f92672>.</span>shape)
</span></span></code></pre></div><hr><h2 id=seq2seqattention-bi-directional-rnn>Seq2Seq＋Attention, Bi-directional RNN</h2><h3 id=seq2seq-架構>Seq2Seq 架構</h3><ul><li>Encoder-Decoder 結構，常用於翻譯、摘要、對話生成</li><li>Encoder 將輸入序列壓縮為上下文向量，Decoder 逐步生成輸出</li></ul><h3 id=attention-機制>Attention 機制</h3><ul><li>讓 Decoder 每步都能關注 Encoder 不同部分，解決長序列資訊瓶頸</li></ul><h3 id=bi-directional-rnn>Bi-directional RNN</h3><ul><li>同時考慮前後文，提升序列理解能力</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>bi_rnn <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>RNN(input_size<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, hidden_size<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, num_layers<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, batch_first<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, bidirectional<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>out_bi, _ <span style=color:#f92672>=</span> bi_rnn(x)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Bi-RNN 輸出 shape:&#34;</span>, out_bi<span style=color:#f92672>.</span>shape)
</span></span></code></pre></div><hr><h2 id=時序預測teacher-forcingscheduled-sampling>時序預測：Teacher Forcing、Scheduled Sampling</h2><h3 id=teacher-forcing>Teacher Forcing</h3><ul><li>訓練時用真實標籤作為下一步輸入，加速收斂</li><li>缺點：推論時誤差累積（Exposure Bias）</li></ul><h3 id=scheduled-sampling>Scheduled Sampling</h3><ul><li>逐步減少 Teacher Forcing 機率，提升模型魯棒性</li></ul><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>語音辨識、機器翻譯、對話生成、時間序列預測、NLP 任務</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>忽略梯度爆炸/消失，導致訓練失敗</li><li>LSTM/GRU 結構選擇不當，模型過大或過簡</li><li>Seq2Seq 未加 Attention，長序列表現差</li><li>Teacher Forcing 機率設置不合理，推論效果不佳</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>RNN</td><td>為何會梯度爆炸/消失？如何解決？</td></tr><tr><td>LSTM/GRU</td><td>閘門結構與優缺點？</td></tr><tr><td>Seq2Seq</td><td>Encoder-Decoder 如何運作？</td></tr><tr><td>Attention</td><td>如何幫助長序列建模？</td></tr><tr><td>Teacher Forcing</td><td>有何優缺點？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>長序列建議用 LSTM/GRU + Attention</li><li>訓練時監控梯度，必要時啟用梯度裁剪</li><li>Teacher Forcing/Scheduled Sampling 需根據任務調整</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://www.deeplearningbook.org/contents/rnn.html>Deep Learning Book: Sequence Modeling</a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.nn.RNN.html>PyTorch RNN 官方文件</a></li><li><a href=https://arxiv.org/abs/1706.03762>Attention is All You Need 論文</a></li><li><a href=https://arxiv.org/abs/1506.03099>Scheduled Sampling 論文</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>RNN 為何會梯度爆炸/消失？數學推導？</li><li>LSTM/GRU 閘門結構與公式？</li><li>Seq2Seq 架構與應用場景？</li><li>Attention 機制數學原理？</li><li>Bi-RNN 有何優勢？</li><li>Teacher Forcing 與 Scheduled Sampling 差異？</li><li>如何用 Python 實作 LSTM/GRU？</li><li>RNN 在 NLP/時序預測的應用？</li><li>Exposure Bias 是什麼？如何緩解？</li><li>Seq2Seq 未加 Attention 有何缺點？</li></ol><hr><h2 id=結語>結語</h2><p>循環與序列模型是處理時序與語言資料的關鍵。熟悉 RNN、LSTM、GRU、Seq2Seq、Attention 與時序預測技巧，能讓你在 NLP、語音、金融等領域發揮深度學習實力。下一章將進入 Attention 機制拆解，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>RNN</span>
<span class=tag>LSTM</span>
<span class=tag>GRU</span>
<span class=tag>Seq2Seq</span>
<span class=tag>Attention</span>
<span class=tag>Teacher Forcing</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>