<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>學習率策略全解析：Step, Cosine, Cyclical, Warm-up 與 LR Finder - Yu's Portfolio & Learning Hub</title><meta name=description content="學習率（Learning Rate, LR）是影響模型訓練收斂與泛化的關鍵超參數。從 Constant、Step、Exponential Decay，到 Cosine Annealing、Warm-up、Cyclical、One-Cycle Policy 與 LR Finder，這些策略能顯著提升訓練效率與最終表現。本章將深入原理、實作、面試熱點與常見誤區，幫助你全面掌握學習率調控。
Constant / Step / Exponential Decay Constant LR 固定學習率，適合簡單任務或預訓練初期 Step Decay 每隔固定 epoch 將學習率乘以一個係數（如 0.1） 常用於 ResNet、VGG 等經典架構 Exponential Decay 每個 epoch 以指數方式衰減學習率 import torch import torch.optim as optim optimizer = optim.SGD(model.parameters(), lr=0.1) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) for epoch in range(30): # ...existing code... scheduler.step() Cosine Annealing & Warm-up Cosine Annealing 學習率隨 epoch 呈餘弦曲線下降，訓練後期更平滑 適合 Transformer、Vision Transformer 等現代架構 Warm-up 訓練初期用較小學習率，逐步升高，避免梯度爆炸 常與 Cosine Annealing 結合 scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20) # Warm-up 可用自訂 scheduler 或 transformers get_linear_schedule_with_warmup Cyclical／One-Cycle Policy Cyclical Learning Rate 學習率在訓練過程中週期性上升下降，幫助跳出局部極小 代表：CyclicalLR、Triangular、Exp Range One-Cycle Policy 先升後降，訓練末期快速衰減，提升泛化 適合大規模訓練、超參數搜尋 scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=100, epochs=10) LR Finder 實戰流程 先用指數增長學習率訓練一輪，記錄 loss 與 lr 找 loss 最快下降區間，選擇最佳初始學習率 # 參考 fastai 或 pytorch-lr-finder 套件 # ...existing code... 理論直覺、應用場景與常見誤區 應用場景 Step/Cosine：CV/NLP 主流架構 Warm-up：Transformer、BERT、GPT Cyclical/One-Cycle：超參數搜尋、快速收斂 常見誤區 學習率設太大導致發散，太小收斂慢 忽略 warm-up，導致初期梯度爆炸 Cyclical/One-Cycle 未正確設置 max_lr 面試熱點與經典問題 主題 常見問題 Step vs Cosine 差異與適用場景？ Warm-up 為何能提升穩定性？ Cyclical/One-Cycle 原理與優勢？ LR Finder 如何選最佳學習率？ 學習率策略 對收斂與泛化有何影響？ 使用注意事項 學習率需根據模型、資料與優化器調整 建議用 LR Finder 或 One-Cycle Policy 自動搜尋 Scheduler 設定需與訓練步數、epoch 匹配 延伸閱讀與資源 PyTorch LR Scheduler 官方文件 One-Cycle Policy 論文 fastai LR Finder Cyclical Learning Rates 論文 經典面試題與解法提示 Step Decay、Cosine Annealing、Cyclical LR 原理與差異？ Warm-up 如何提升訓練穩定性？ One-Cycle Policy 的優勢？ LR Finder 如何選最佳學習率？ 學習率策略對收斂與泛化的影響？ 如何用 Python 實作多種 scheduler？ Cyclical/One-Cycle 參數設置原則？ Warm-up 需搭配哪些模型？ 學習率設錯會有什麼後果？ Scheduler 與 optimizer 如何協同設計？ 結語 學習率策略是模型訓練成敗的關鍵。熟悉 Step、Cosine、Cyclical、Warm-up 與 LR Finder，能讓你高效訓練並提升泛化能力。下一章將進入正則化武器庫，敬請期待！
"><meta property="og:title" content="學習率策略全解析：Step, Cosine, Cyclical, Warm-up 與 LR Finder"><meta property="og:description" content="學習率（Learning Rate, LR）是影響模型訓練收斂與泛化的關鍵超參數。從 Constant、Step、Exponential Decay，到 Cosine Annealing、Warm-up、Cyclical、One-Cycle Policy 與 LR Finder，這些策略能顯著提升訓練效率與最終表現。本章將深入原理、實作、面試熱點與常見誤區，幫助你全面掌握學習率調控。
Constant / Step / Exponential Decay Constant LR 固定學習率，適合簡單任務或預訓練初期 Step Decay 每隔固定 epoch 將學習率乘以一個係數（如 0.1） 常用於 ResNet、VGG 等經典架構 Exponential Decay 每個 epoch 以指數方式衰減學習率 import torch import torch.optim as optim optimizer = optim.SGD(model.parameters(), lr=0.1) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) for epoch in range(30): # ...existing code... scheduler.step() Cosine Annealing & Warm-up Cosine Annealing 學習率隨 epoch 呈餘弦曲線下降，訓練後期更平滑 適合 Transformer、Vision Transformer 等現代架構 Warm-up 訓練初期用較小學習率，逐步升高，避免梯度爆炸 常與 Cosine Annealing 結合 scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20) # Warm-up 可用自訂 scheduler 或 transformers get_linear_schedule_with_warmup Cyclical／One-Cycle Policy Cyclical Learning Rate 學習率在訓練過程中週期性上升下降，幫助跳出局部極小 代表：CyclicalLR、Triangular、Exp Range One-Cycle Policy 先升後降，訓練末期快速衰減，提升泛化 適合大規模訓練、超參數搜尋 scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=100, epochs=10) LR Finder 實戰流程 先用指數增長學習率訓練一輪，記錄 loss 與 lr 找 loss 最快下降區間，選擇最佳初始學習率 # 參考 fastai 或 pytorch-lr-finder 套件 # ...existing code... 理論直覺、應用場景與常見誤區 應用場景 Step/Cosine：CV/NLP 主流架構 Warm-up：Transformer、BERT、GPT Cyclical/One-Cycle：超參數搜尋、快速收斂 常見誤區 學習率設太大導致發散，太小收斂慢 忽略 warm-up，導致初期梯度爆炸 Cyclical/One-Cycle 未正確設置 max_lr 面試熱點與經典問題 主題 常見問題 Step vs Cosine 差異與適用場景？ Warm-up 為何能提升穩定性？ Cyclical/One-Cycle 原理與優勢？ LR Finder 如何選最佳學習率？ 學習率策略 對收斂與泛化有何影響？ 使用注意事項 學習率需根據模型、資料與優化器調整 建議用 LR Finder 或 One-Cycle Policy 自動搜尋 Scheduler 設定需與訓練步數、epoch 匹配 延伸閱讀與資源 PyTorch LR Scheduler 官方文件 One-Cycle Policy 論文 fastai LR Finder Cyclical Learning Rates 論文 經典面試題與解法提示 Step Decay、Cosine Annealing、Cyclical LR 原理與差異？ Warm-up 如何提升訓練穩定性？ One-Cycle Policy 的優勢？ LR Finder 如何選最佳學習率？ 學習率策略對收斂與泛化的影響？ 如何用 Python 實作多種 scheduler？ Cyclical/One-Cycle 參數設置原則？ Warm-up 需搭配哪些模型？ 學習率設錯會有什麼後果？ Scheduler 與 optimizer 如何協同設計？ 結語 學習率策略是模型訓練成敗的關鍵。熟悉 Step、Cosine、Cyclical、Warm-up 與 LR Finder，能讓你高效訓練並提升泛化能力。下一章將進入正則化武器庫，敬請期待！
"><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/learning-rate-schedule/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/learning-rate-schedule/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>學習率策略全解析：Step, Cosine, Cyclical, Warm-up 與 LR Finder</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>學習率策略全解析：Step, Cosine, Cyclical, Warm-up 與 LR Finder</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-02-03</span></div></header><div class=article-body><p>學習率（Learning Rate, LR）是影響模型訓練收斂與泛化的關鍵超參數。從 Constant、Step、Exponential Decay，到 Cosine Annealing、Warm-up、Cyclical、One-Cycle Policy 與 LR Finder，這些策略能顯著提升訓練效率與最終表現。本章將深入原理、實作、面試熱點與常見誤區，幫助你全面掌握學習率調控。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#constant--step--exponential-decay>Constant / Step / Exponential Decay</a><ul><li><a href=#constant-lr>Constant LR</a></li><li><a href=#step-decay>Step Decay</a></li><li><a href=#exponential-decay>Exponential Decay</a></li></ul></li><li><a href=#cosine-annealing--warm-up>Cosine Annealing & Warm-up</a><ul><li><a href=#cosine-annealing>Cosine Annealing</a></li><li><a href=#warm-up>Warm-up</a></li></ul></li><li><a href=#cyclicalone-cycle-policy>Cyclical／One-Cycle Policy</a><ul><li><a href=#cyclical-learning-rate>Cyclical Learning Rate</a></li><li><a href=#one-cycle-policy>One-Cycle Policy</a></li></ul></li><li><a href=#lr-finder-實戰流程>LR Finder 實戰流程</a></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=constant--step--exponential-decay>Constant / Step / Exponential Decay</h2><h3 id=constant-lr>Constant LR</h3><ul><li>固定學習率，適合簡單任務或預訓練初期</li></ul><h3 id=step-decay>Step Decay</h3><ul><li>每隔固定 epoch 將學習率乘以一個係數（如 0.1）</li><li>常用於 ResNet、VGG 等經典架構</li></ul><h3 id=exponential-decay>Exponential Decay</h3><ul><li>每個 epoch 以指數方式衰減學習率</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.optim <span style=color:#66d9ef>as</span> optim
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>scheduler <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>lr_scheduler<span style=color:#f92672>.</span>StepLR(optimizer, step_size<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, gamma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>30</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...existing code...</span>
</span></span><span style=display:flex><span>    scheduler<span style=color:#f92672>.</span>step()
</span></span></code></pre></div><hr><h2 id=cosine-annealing--warm-up>Cosine Annealing & Warm-up</h2><h3 id=cosine-annealing>Cosine Annealing</h3><ul><li>學習率隨 epoch 呈餘弦曲線下降，訓練後期更平滑</li><li>適合 Transformer、Vision Transformer 等現代架構</li></ul><h3 id=warm-up>Warm-up</h3><ul><li>訓練初期用較小學習率，逐步升高，避免梯度爆炸</li><li>常與 Cosine Annealing 結合</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>scheduler <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>lr_scheduler<span style=color:#f92672>.</span>CosineAnnealingLR(optimizer, T_max<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Warm-up 可用自訂 scheduler 或 transformers get_linear_schedule_with_warmup</span>
</span></span></code></pre></div><hr><h2 id=cyclicalone-cycle-policy>Cyclical／One-Cycle Policy</h2><h3 id=cyclical-learning-rate>Cyclical Learning Rate</h3><ul><li>學習率在訓練過程中週期性上升下降，幫助跳出局部極小</li><li>代表：CyclicalLR、Triangular、Exp Range</li></ul><h3 id=one-cycle-policy>One-Cycle Policy</h3><ul><li>先升後降，訓練末期快速衰減，提升泛化</li><li>適合大規模訓練、超參數搜尋</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>scheduler <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>lr_scheduler<span style=color:#f92672>.</span>OneCycleLR(optimizer, max_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, steps_per_epoch<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span></code></pre></div><hr><h2 id=lr-finder-實戰流程>LR Finder 實戰流程</h2><ul><li>先用指數增長學習率訓練一輪，記錄 loss 與 lr</li><li>找 loss 最快下降區間，選擇最佳初始學習率</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 參考 fastai 或 pytorch-lr-finder 套件</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ...existing code...</span>
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>Step/Cosine：CV/NLP 主流架構</li><li>Warm-up：Transformer、BERT、GPT</li><li>Cyclical/One-Cycle：超參數搜尋、快速收斂</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>學習率設太大導致發散，太小收斂慢</li><li>忽略 warm-up，導致初期梯度爆炸</li><li>Cyclical/One-Cycle 未正確設置 max_lr</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>Step vs Cosine</td><td>差異與適用場景？</td></tr><tr><td>Warm-up</td><td>為何能提升穩定性？</td></tr><tr><td>Cyclical/One-Cycle</td><td>原理與優勢？</td></tr><tr><td>LR Finder</td><td>如何選最佳學習率？</td></tr><tr><td>學習率策略</td><td>對收斂與泛化有何影響？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>學習率需根據模型、資料與優化器調整</li><li>建議用 LR Finder 或 One-Cycle Policy 自動搜尋</li><li>Scheduler 設定需與訓練步數、epoch 匹配</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate>PyTorch LR Scheduler 官方文件</a></li><li><a href=https://arxiv.org/abs/1708.07120>One-Cycle Policy 論文</a></li><li><a href=https://docs.fast.ai/callback.schedule.html#Learner.lr_find>fastai LR Finder</a></li><li><a href=https://arxiv.org/abs/1506.01186>Cyclical Learning Rates 論文</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>Step Decay、Cosine Annealing、Cyclical LR 原理與差異？</li><li>Warm-up 如何提升訓練穩定性？</li><li>One-Cycle Policy 的優勢？</li><li>LR Finder 如何選最佳學習率？</li><li>學習率策略對收斂與泛化的影響？</li><li>如何用 Python 實作多種 scheduler？</li><li>Cyclical/One-Cycle 參數設置原則？</li><li>Warm-up 需搭配哪些模型？</li><li>學習率設錯會有什麼後果？</li><li>Scheduler 與 optimizer 如何協同設計？</li></ol><hr><h2 id=結語>結語</h2><p>學習率策略是模型訓練成敗的關鍵。熟悉 Step、Cosine、Cyclical、Warm-up 與 LR Finder，能讓你高效訓練並提升泛化能力。下一章將進入正則化武器庫，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>learning-rate</span>
<span class=tag>step-decay</span>
<span class=tag>cosine-annealing</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>