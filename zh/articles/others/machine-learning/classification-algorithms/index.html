<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>分類演算法百寶箱：k-NN、SVM、Naïve Bayes、決策樹全解析 - Yu's Portfolio & Learning Hub</title><meta name=description content='分類演算法是機器學習面試與實務的重點。從無參數的 k-NN，到強大的 SVM、直觀的 Naïve Bayes、靈活的決策樹，每種方法都有其數學基礎、優缺點與適用場景。本章將深入數學推導、直覺圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握分類演算法。
k-NN（K-Nearest Neighbors） 原理與數學基礎 無需訓練，直接根據距離尋找最近的 k 個鄰居，投票決定類別。 距離度量常用歐氏距離、曼哈頓距離等。 優缺點 優點：簡單、無需訓練、可處理多分類。 缺點：資料量大時預測慢、對特徵縮放敏感、維度災難。 Python 實作 from sklearn.neighbors import KNeighborsClassifier from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True) knn = KNeighborsClassifier(n_neighbors=3) knn.fit(X, y) print("預測:", knn.predict(X[:5])) SVM（Support Vector Machine） 原理與數學推導 尋找最大化類別間隔的超平面。 支援硬邊界（無誤差）與軟邊界（允許部分誤差）。 Kernel Trick 可將資料映射到高維空間，處理非線性分類。 常見 Kernel 線性、RBF（高斯）、多項式、Sigmoid 優缺點 優點：泛化能力強、可處理高維資料。 缺點：對參數敏感、資料量大時訓練慢、不易解釋。 Python 實作 from sklearn.svm import SVC svc = SVC(kernel=&#39;rbf&#39;, C=1.0) svc.fit(X, y) print("SVM 預測:", svc.predict(X[:5])) Naïve Bayes 家族 原理 假設特徵間條件獨立，根據貝氏定理計算後驗機率。 常見：高斯、伯努利、多項式 Naïve Bayes。 優缺點 優點：訓練快、對高維資料友好、可處理缺失值。 缺點：特徵獨立假設常不成立、預測機率不精確。 Python 實作 from sklearn.naive_bayes import GaussianNB nb = GaussianNB() nb.fit(X, y) print("Naive Bayes 預測:", nb.predict(X[:5])) 決策樹（C4.5 / CART） 原理 依據資訊增益（C4.5）或基尼指數（CART）分裂特徵，構建樹狀結構。 可處理數值與類別特徵，支援多分類。 優缺點 優點：易解釋、可視化、處理異質特徵。 缺點：易過擬合、對資料微小變動敏感。 Python 實作 from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(criterion=&#39;gini&#39;) dt.fit(X, y) print("決策樹預測:", dt.predict(X[:5])) 常見面試熱點與經典問題 主題 常見問題 k-NN 為何不用訓練？如何選 k？ SVM Kernel Trick 原理？硬/軟邊界差異？ Naive Bayes 何時適用？獨立假設有何影響？ 決策樹 如何避免過擬合？資訊增益與基尼指數差異？ 實務應用與常見誤區 實務應用 k-NN 適合小型資料集、推薦系統、異常偵測。 SVM 適合高維資料、文本分類、影像辨識。 Naive Bayes 適合垃圾郵件分類、醫療診斷。 決策樹適合特徵異質、需解釋性的任務。 常見誤區 k-NN 忽略特徵標準化，導致距離失真。 SVM Kernel 參數未調整，效果不佳。 Naive Bayes 忽略特徵相關性，預測失準。 決策樹未剪枝，嚴重過擬合。 使用注意事項 分類演算法需根據資料特性選擇，並搭配特徵工程與正則化。 k-NN、SVM 對特徵縮放敏感，建議標準化。 決策樹建議搭配剪枝與集成方法提升泛化能力。 延伸閱讀與資源 StatQuest: SVM, k-NN, Naive Bayes Scikit-learn 分類演算法 Kernel Trick 直覺動畫 決策樹與集成方法 結語 分類演算法是機器學習的基礎。熟悉 k-NN、SVM、Naive Bayes、決策樹的原理、優缺點與實作細節，能讓你在面試與專案中靈活應用。下一章將進入集成學習，敬請期待！
'><meta property="og:title" content="分類演算法百寶箱：k-NN、SVM、Naïve Bayes、決策樹全解析"><meta property="og:description" content='分類演算法是機器學習面試與實務的重點。從無參數的 k-NN，到強大的 SVM、直觀的 Naïve Bayes、靈活的決策樹，每種方法都有其數學基礎、優缺點與適用場景。本章將深入數學推導、直覺圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握分類演算法。
k-NN（K-Nearest Neighbors） 原理與數學基礎 無需訓練，直接根據距離尋找最近的 k 個鄰居，投票決定類別。 距離度量常用歐氏距離、曼哈頓距離等。 優缺點 優點：簡單、無需訓練、可處理多分類。 缺點：資料量大時預測慢、對特徵縮放敏感、維度災難。 Python 實作 from sklearn.neighbors import KNeighborsClassifier from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True) knn = KNeighborsClassifier(n_neighbors=3) knn.fit(X, y) print("預測:", knn.predict(X[:5])) SVM（Support Vector Machine） 原理與數學推導 尋找最大化類別間隔的超平面。 支援硬邊界（無誤差）與軟邊界（允許部分誤差）。 Kernel Trick 可將資料映射到高維空間，處理非線性分類。 常見 Kernel 線性、RBF（高斯）、多項式、Sigmoid 優缺點 優點：泛化能力強、可處理高維資料。 缺點：對參數敏感、資料量大時訓練慢、不易解釋。 Python 實作 from sklearn.svm import SVC svc = SVC(kernel=&#39;rbf&#39;, C=1.0) svc.fit(X, y) print("SVM 預測:", svc.predict(X[:5])) Naïve Bayes 家族 原理 假設特徵間條件獨立，根據貝氏定理計算後驗機率。 常見：高斯、伯努利、多項式 Naïve Bayes。 優缺點 優點：訓練快、對高維資料友好、可處理缺失值。 缺點：特徵獨立假設常不成立、預測機率不精確。 Python 實作 from sklearn.naive_bayes import GaussianNB nb = GaussianNB() nb.fit(X, y) print("Naive Bayes 預測:", nb.predict(X[:5])) 決策樹（C4.5 / CART） 原理 依據資訊增益（C4.5）或基尼指數（CART）分裂特徵，構建樹狀結構。 可處理數值與類別特徵，支援多分類。 優缺點 優點：易解釋、可視化、處理異質特徵。 缺點：易過擬合、對資料微小變動敏感。 Python 實作 from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(criterion=&#39;gini&#39;) dt.fit(X, y) print("決策樹預測:", dt.predict(X[:5])) 常見面試熱點與經典問題 主題 常見問題 k-NN 為何不用訓練？如何選 k？ SVM Kernel Trick 原理？硬/軟邊界差異？ Naive Bayes 何時適用？獨立假設有何影響？ 決策樹 如何避免過擬合？資訊增益與基尼指數差異？ 實務應用與常見誤區 實務應用 k-NN 適合小型資料集、推薦系統、異常偵測。 SVM 適合高維資料、文本分類、影像辨識。 Naive Bayes 適合垃圾郵件分類、醫療診斷。 決策樹適合特徵異質、需解釋性的任務。 常見誤區 k-NN 忽略特徵標準化，導致距離失真。 SVM Kernel 參數未調整，效果不佳。 Naive Bayes 忽略特徵相關性，預測失準。 決策樹未剪枝，嚴重過擬合。 使用注意事項 分類演算法需根據資料特性選擇，並搭配特徵工程與正則化。 k-NN、SVM 對特徵縮放敏感，建議標準化。 決策樹建議搭配剪枝與集成方法提升泛化能力。 延伸閱讀與資源 StatQuest: SVM, k-NN, Naive Bayes Scikit-learn 分類演算法 Kernel Trick 直覺動畫 決策樹與集成方法 結語 分類演算法是機器學習的基礎。熟悉 k-NN、SVM、Naive Bayes、決策樹的原理、優缺點與實作細節，能讓你在面試與專案中靈活應用。下一章將進入集成學習，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/classification-algorithms/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/classification-algorithms/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>分類演算法百寶箱：k-NN、SVM、Naïve Bayes、決策樹全解析</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>分類演算法百寶箱：k-NN、SVM、Naïve Bayes、決策樹全解析</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-01-16</span></div></header><div class=article-body><p>分類演算法是機器學習面試與實務的重點。從無參數的 k-NN，到強大的 SVM、直觀的 Naïve Bayes、靈活的決策樹，每種方法都有其數學基礎、優缺點與適用場景。本章將深入數學推導、直覺圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握分類演算法。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#k-nnk-nearest-neighbors>k-NN（K-Nearest Neighbors）</a><ul><li><a href=#原理與數學基礎>原理與數學基礎</a></li><li><a href=#優缺點>優缺點</a></li><li><a href=#python-實作>Python 實作</a></li></ul></li><li><a href=#svmsupport-vector-machine>SVM（Support Vector Machine）</a><ul><li><a href=#原理與數學推導>原理與數學推導</a></li><li><a href=#常見-kernel>常見 Kernel</a></li><li><a href=#優缺點-1>優缺點</a></li><li><a href=#python-實作-1>Python 實作</a></li></ul></li><li><a href=#naïve-bayes-家族>Naïve Bayes 家族</a><ul><li><a href=#原理>原理</a></li><li><a href=#優缺點-2>優缺點</a></li><li><a href=#python-實作-2>Python 實作</a></li></ul></li><li><a href=#決策樹c45--cart>決策樹（C4.5 / CART）</a><ul><li><a href=#原理-1>原理</a></li><li><a href=#優缺點-3>優缺點</a></li><li><a href=#python-實作-3>Python 實作</a></li></ul></li><li><a href=#常見面試熱點與經典問題>常見面試熱點與經典問題</a></li><li><a href=#實務應用與常見誤區>實務應用與常見誤區</a><ul><li><a href=#實務應用>實務應用</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=k-nnk-nearest-neighbors>k-NN（K-Nearest Neighbors）</h2><h3 id=原理與數學基礎>原理與數學基礎</h3><ul><li>無需訓練，直接根據距離尋找最近的 k 個鄰居，投票決定類別。</li><li>距離度量常用歐氏距離、曼哈頓距離等。</li></ul><h3 id=優缺點>優缺點</h3><ul><li>優點：簡單、無需訓練、可處理多分類。</li><li>缺點：資料量大時預測慢、對特徵縮放敏感、維度災難。</li></ul><h3 id=python-實作>Python 實作</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.neighbors <span style=color:#f92672>import</span> KNeighborsClassifier
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> load_iris
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=color:#f92672>=</span> load_iris(return_X_y<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>knn <span style=color:#f92672>=</span> KNeighborsClassifier(n_neighbors<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>knn<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;預測:&#34;</span>, knn<span style=color:#f92672>.</span>predict(X[:<span style=color:#ae81ff>5</span>]))
</span></span></code></pre></div><hr><h2 id=svmsupport-vector-machine>SVM（Support Vector Machine）</h2><h3 id=原理與數學推導>原理與數學推導</h3><ul><li>尋找最大化類別間隔的超平面。</li><li>支援硬邊界（無誤差）與軟邊界（允許部分誤差）。</li><li>Kernel Trick 可將資料映射到高維空間，處理非線性分類。</li></ul><h3 id=常見-kernel>常見 Kernel</h3><ul><li>線性、RBF（高斯）、多項式、Sigmoid</li></ul><h3 id=優缺點-1>優缺點</h3><ul><li>優點：泛化能力強、可處理高維資料。</li><li>缺點：對參數敏感、資料量大時訓練慢、不易解釋。</li></ul><h3 id=python-實作-1>Python 實作</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.svm <span style=color:#f92672>import</span> SVC
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>svc <span style=color:#f92672>=</span> SVC(kernel<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rbf&#39;</span>, C<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>svc<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;SVM 預測:&#34;</span>, svc<span style=color:#f92672>.</span>predict(X[:<span style=color:#ae81ff>5</span>]))
</span></span></code></pre></div><hr><h2 id=naïve-bayes-家族>Naïve Bayes 家族</h2><h3 id=原理>原理</h3><ul><li>假設特徵間條件獨立，根據貝氏定理計算後驗機率。</li><li>常見：高斯、伯努利、多項式 Naïve Bayes。</li></ul><h3 id=優缺點-2>優缺點</h3><ul><li>優點：訓練快、對高維資料友好、可處理缺失值。</li><li>缺點：特徵獨立假設常不成立、預測機率不精確。</li></ul><h3 id=python-實作-2>Python 實作</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.naive_bayes <span style=color:#f92672>import</span> GaussianNB
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nb <span style=color:#f92672>=</span> GaussianNB()
</span></span><span style=display:flex><span>nb<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Naive Bayes 預測:&#34;</span>, nb<span style=color:#f92672>.</span>predict(X[:<span style=color:#ae81ff>5</span>]))
</span></span></code></pre></div><hr><h2 id=決策樹c45--cart>決策樹（C4.5 / CART）</h2><h3 id=原理-1>原理</h3><ul><li>依據資訊增益（C4.5）或基尼指數（CART）分裂特徵，構建樹狀結構。</li><li>可處理數值與類別特徵，支援多分類。</li></ul><h3 id=優缺點-3>優缺點</h3><ul><li>優點：易解釋、可視化、處理異質特徵。</li><li>缺點：易過擬合、對資料微小變動敏感。</li></ul><h3 id=python-實作-3>Python 實作</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.tree <span style=color:#f92672>import</span> DecisionTreeClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dt <span style=color:#f92672>=</span> DecisionTreeClassifier(criterion<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;gini&#39;</span>)
</span></span><span style=display:flex><span>dt<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;決策樹預測:&#34;</span>, dt<span style=color:#f92672>.</span>predict(X[:<span style=color:#ae81ff>5</span>]))
</span></span></code></pre></div><hr><h2 id=常見面試熱點與經典問題>常見面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>k-NN</td><td>為何不用訓練？如何選 k？</td></tr><tr><td>SVM</td><td>Kernel Trick 原理？硬/軟邊界差異？</td></tr><tr><td>Naive Bayes</td><td>何時適用？獨立假設有何影響？</td></tr><tr><td>決策樹</td><td>如何避免過擬合？資訊增益與基尼指數差異？</td></tr></tbody></table><hr><h2 id=實務應用與常見誤區>實務應用與常見誤區</h2><h3 id=實務應用>實務應用</h3><ul><li>k-NN 適合小型資料集、推薦系統、異常偵測。</li><li>SVM 適合高維資料、文本分類、影像辨識。</li><li>Naive Bayes 適合垃圾郵件分類、醫療診斷。</li><li>決策樹適合特徵異質、需解釋性的任務。</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>k-NN 忽略特徵標準化，導致距離失真。</li><li>SVM Kernel 參數未調整，效果不佳。</li><li>Naive Bayes 忽略特徵相關性，預測失準。</li><li>決策樹未剪枝，嚴重過擬合。</li></ul><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>分類演算法需根據資料特性選擇，並搭配特徵工程與正則化。</li><li>k-NN、SVM 對特徵縮放敏感，建議標準化。</li><li>決策樹建議搭配剪枝與集成方法提升泛化能力。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://www.youtube.com/c/joshstarmer>StatQuest: SVM, k-NN, Naive Bayes</a></li><li><a href=https://scikit-learn.org/stable/supervised_learning.html#supervised-learning>Scikit-learn 分類演算法</a></li><li><a href="https://www.youtube.com/watch?v=3liCbRZPrZA">Kernel Trick 直覺動畫</a></li><li><a href=https://scikit-learn.org/stable/modules/tree.html>決策樹與集成方法</a></li></ul><hr><h2 id=結語>結語</h2><p>分類演算法是機器學習的基礎。熟悉 k-NN、SVM、Naive Bayes、決策樹的原理、優缺點與實作細節，能讓你在面試與專案中靈活應用。下一章將進入集成學習，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>k-NN</span>
<span class=tag>SVM</span>
<span class=tag>Naive Bayes</span>
<span class=tag>Kernel Trick</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>