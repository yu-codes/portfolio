<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>線性代數快攻：AI 必備的向量與矩陣基礎 - Yu's Portfolio & Learning Hub</title><meta name=description content='在 AI 與機器學習領域，線性代數是不可或缺的基石。無論是神經網路的權重運算、資料降維，還是推薦系統的矩陣分解，背後都離不開向量與矩陣的操作。本篇將帶你快速掌握 AI 常用的線性代數觀念，並以直覺、圖解與 Python 範例說明。
向量與矩陣運算 什麼是向量與矩陣？ 向量（Vector）：一組有方向與大小的數值序列，常用於描述資料點、特徵等。 矩陣（Matrix）：由多個向量組成的二維陣列，常見於資料集、權重參數等。 名稱 例子 應用場景 向量 $\mathbf{v} = [2, 3, 5]$ 特徵向量、嵌入表示 矩陣 $\mathbf{A} = \begin{bmatrix}1 & 2\3 & 4\end{bmatrix}$ 影像、權重、資料集 基本運算 加法/減法：同型向量或矩陣逐元素相加減。 內積（Dot Product）：$\mathbf{a} \cdot \mathbf{b} = \sum_i a_i b_i$，常用於相似度計算。 矩陣乘法：$\mathbf{A} \mathbf{B}$，資料轉換、神經網路前向傳播核心。 import numpy as np a = np.array([1, 2, 3]) b = np.array([4, 5, 6]) dot = np.dot(a, b) # 內積 A = np.array([[1, 2], [3, 4]]) B = np.array([[5, 6], [7, 8]]) matmul = np.matmul(A, B) # 矩陣乘法 print("向量內積：", dot) print("矩陣乘法：\n", matmul) Rank、Span、Basis：資料的維度與本質 Rank（秩）：矩陣中獨立行（或列）的最大數量，反映資料的「本質維度」。 Span（張成空間）：一組向量可線性組合出所有可能的空間。 Basis（基底）：能張成空間且彼此線性獨立的最小向量組。 概念 直覺說明 應用 Rank 有幾個獨立方向 資料降維、PCA Span 能到達哪些點 特徵空間理解 Basis 最小生成組合 向量壓縮、特徵選擇 特徵值、特徵向量、SVD 與 PCA 特徵值與特徵向量 特徵向量（Eigenvector）：經過矩陣變換後，方向不變的向量。 特徵值（Eigenvalue）：該向量被拉伸或縮放的倍數。 在資料降維、PCA、圖神經網路等場景都會用到。
'><meta property="og:title" content="線性代數快攻：AI 必備的向量與矩陣基礎"><meta property="og:description" content='在 AI 與機器學習領域，線性代數是不可或缺的基石。無論是神經網路的權重運算、資料降維，還是推薦系統的矩陣分解，背後都離不開向量與矩陣的操作。本篇將帶你快速掌握 AI 常用的線性代數觀念，並以直覺、圖解與 Python 範例說明。
向量與矩陣運算 什麼是向量與矩陣？ 向量（Vector）：一組有方向與大小的數值序列，常用於描述資料點、特徵等。 矩陣（Matrix）：由多個向量組成的二維陣列，常見於資料集、權重參數等。 名稱 例子 應用場景 向量 $\mathbf{v} = [2, 3, 5]$ 特徵向量、嵌入表示 矩陣 $\mathbf{A} = \begin{bmatrix}1 & 2\3 & 4\end{bmatrix}$ 影像、權重、資料集 基本運算 加法/減法：同型向量或矩陣逐元素相加減。 內積（Dot Product）：$\mathbf{a} \cdot \mathbf{b} = \sum_i a_i b_i$，常用於相似度計算。 矩陣乘法：$\mathbf{A} \mathbf{B}$，資料轉換、神經網路前向傳播核心。 import numpy as np a = np.array([1, 2, 3]) b = np.array([4, 5, 6]) dot = np.dot(a, b) # 內積 A = np.array([[1, 2], [3, 4]]) B = np.array([[5, 6], [7, 8]]) matmul = np.matmul(A, B) # 矩陣乘法 print("向量內積：", dot) print("矩陣乘法：\n", matmul) Rank、Span、Basis：資料的維度與本質 Rank（秩）：矩陣中獨立行（或列）的最大數量，反映資料的「本質維度」。 Span（張成空間）：一組向量可線性組合出所有可能的空間。 Basis（基底）：能張成空間且彼此線性獨立的最小向量組。 概念 直覺說明 應用 Rank 有幾個獨立方向 資料降維、PCA Span 能到達哪些點 特徵空間理解 Basis 最小生成組合 向量壓縮、特徵選擇 特徵值、特徵向量、SVD 與 PCA 特徵值與特徵向量 特徵向量（Eigenvector）：經過矩陣變換後，方向不變的向量。 特徵值（Eigenvalue）：該向量被拉伸或縮放的倍數。 在資料降維、PCA、圖神經網路等場景都會用到。
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/linear-algebra-for-ai/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/linear-algebra-for-ai/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>線性代數快攻：AI 必備的向量與矩陣基礎</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>線性代數快攻：AI 必備的向量與矩陣基礎</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-04-03</span></div></header><div class=article-body><p>在 AI 與機器學習領域，線性代數是不可或缺的基石。無論是神經網路的權重運算、資料降維，還是推薦系統的矩陣分解，背後都離不開向量與矩陣的操作。本篇將帶你快速掌握 AI 常用的線性代數觀念，並以直覺、圖解與 Python 範例說明。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#向量與矩陣運算>向量與矩陣運算</a><ul><li><a href=#什麼是向量與矩陣>什麼是向量與矩陣？</a></li><li><a href=#基本運算>基本運算</a></li></ul></li><li><a href=#rankspanbasis資料的維度與本質>Rank、Span、Basis：資料的維度與本質</a></li><li><a href=#特徵值特徵向量svd-與-pca>特徵值、特徵向量、SVD 與 PCA</a><ul><li><a href=#特徵值與特徵向量>特徵值與特徵向量</a></li><li><a href=#svd奇異值分解>SVD（奇異值分解）</a></li><li><a href=#pca主成分分析>PCA（主成分分析）</a></li></ul></li><li><a href=#kroneckerhadamard-乘積--廣播機制>Kronecker／Hadamard 乘積 & 廣播機制</a></li><li><a href=#常見面試熱點整理>常見面試熱點整理</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=向量與矩陣運算>向量與矩陣運算</h2><h3 id=什麼是向量與矩陣>什麼是向量與矩陣？</h3><ul><li><strong>向量（Vector）</strong>：一組有方向與大小的數值序列，常用於描述資料點、特徵等。</li><li><strong>矩陣（Matrix）</strong>：由多個向量組成的二維陣列，常見於資料集、權重參數等。</li></ul><table><thead><tr><th>名稱</th><th>例子</th><th>應用場景</th></tr></thead><tbody><tr><td>向量</td><td>$\mathbf{v} = [2, 3, 5]$</td><td>特徵向量、嵌入表示</td></tr><tr><td>矩陣</td><td>$\mathbf{A} = \begin{bmatrix}1 & 2\3 & 4\end{bmatrix}$</td><td>影像、權重、資料集</td></tr></tbody></table><h3 id=基本運算>基本運算</h3><ul><li><strong>加法/減法</strong>：同型向量或矩陣逐元素相加減。</li><li><strong>內積（Dot Product）</strong>：$\mathbf{a} \cdot \mathbf{b} = \sum_i a_i b_i$，常用於相似度計算。</li><li><strong>矩陣乘法</strong>：$\mathbf{A} \mathbf{B}$，資料轉換、神經網路前向傳播核心。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>])
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>])
</span></span><span style=display:flex><span>dot <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(a, b)  <span style=color:#75715e># 內積</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>A <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>]])
</span></span><span style=display:flex><span>B <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>], [<span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>]])
</span></span><span style=display:flex><span>matmul <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>matmul(A, B)  <span style=color:#75715e># 矩陣乘法</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;向量內積：&#34;</span>, dot)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;矩陣乘法：</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, matmul)
</span></span></code></pre></div><hr><h2 id=rankspanbasis資料的維度與本質>Rank、Span、Basis：資料的維度與本質</h2><ul><li><strong>Rank（秩）</strong>：矩陣中獨立行（或列）的最大數量，反映資料的「本質維度」。</li><li><strong>Span（張成空間）</strong>：一組向量可線性組合出所有可能的空間。</li><li><strong>Basis（基底）</strong>：能張成空間且彼此線性獨立的最小向量組。</li></ul><table><thead><tr><th>概念</th><th>直覺說明</th><th>應用</th></tr></thead><tbody><tr><td>Rank</td><td>有幾個獨立方向</td><td>資料降維、PCA</td></tr><tr><td>Span</td><td>能到達哪些點</td><td>特徵空間理解</td></tr><tr><td>Basis</td><td>最小生成組合</td><td>向量壓縮、特徵選擇</td></tr></tbody></table><hr><h2 id=特徵值特徵向量svd-與-pca>特徵值、特徵向量、SVD 與 PCA</h2><h3 id=特徵值與特徵向量>特徵值與特徵向量</h3><ul><li><strong>特徵向量（Eigenvector）</strong>：經過矩陣變換後，方向不變的向量。</li><li><strong>特徵值（Eigenvalue）</strong>：該向量被拉伸或縮放的倍數。</li></ul><blockquote><p>在資料降維、PCA、圖神經網路等場景都會用到。</p></blockquote><h3 id=svd奇異值分解>SVD（奇異值分解）</h3><ul><li>將任意矩陣拆解為三個矩陣的乘積：$\mathbf{A} = \mathbf{U} \Sigma \mathbf{V}^T$</li><li>用於資料壓縮、推薦系統、PCA 等。</li></ul><h3 id=pca主成分分析>PCA（主成分分析）</h3><ul><li>一種常用的降維方法，找出資料中最重要的方向（主成分）。</li><li>本質上就是對協方差矩陣做特徵分解或 SVD。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.decomposition <span style=color:#f92672>import</span> PCA
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>5</span>)  <span style=color:#75715e># 100 筆 5 維資料</span>
</span></span><span style=display:flex><span>pca <span style=color:#f92672>=</span> PCA(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>X_reduced <span style=color:#f92672>=</span> pca<span style=color:#f92672>.</span>fit_transform(X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;降維後資料 shape：&#34;</span>, X_reduced<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;主成分解釋變異量：&#34;</span>, pca<span style=color:#f92672>.</span>explained_variance_ratio_)
</span></span></code></pre></div><hr><h2 id=kroneckerhadamard-乘積--廣播機制>Kronecker／Hadamard 乘積 & 廣播機制</h2><ul><li><strong>Kronecker 乘積</strong>：兩矩陣的「擴展型」乘法，常用於張量運算。</li><li><strong>Hadamard 乘積</strong>：對應元素相乘，常見於神經網路的門控機制。</li><li><strong>廣播（Broadcasting）</strong>：Numpy/PyTorch 等自動擴展維度，簡化運算。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>A <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>]])
</span></span><span style=display:flex><span>B <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>5</span>], [<span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Hadamard 乘積</span>
</span></span><span style=display:flex><span>hadamard <span style=color:#f92672>=</span> A <span style=color:#f92672>*</span> B
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Kronecker 乘積</span>
</span></span><span style=display:flex><span>kronecker <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>kron(A, B)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Hadamard 乘積：</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, hadamard)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Kronecker 乘積：</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, kronecker)
</span></span></code></pre></div><hr><h2 id=常見面試熱點整理>常見面試熱點整理</h2><table><thead><tr><th>熱點主題</th><th>面試常問問題</th></tr></thead><tbody><tr><td>Rank/Basis</td><td>如何判斷資料可否降維？</td></tr><tr><td>特徵值/特徵向量</td><td>為何 PCA 要用特徵分解？</td></tr><tr><td>SVD</td><td>與 Eigen Decomposition 差異？</td></tr><tr><td>Hadamard 乘積</td><td>在神經網路哪裡會用到？</td></tr><tr><td>廣播機制</td><td>Numpy 如何自動對齊維度？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>向量與矩陣運算常見於資料前處理、特徵工程與模型訓練。</li><li>高維資料常需降維（如 PCA），以提升效率與可視化。</li><li>熟悉 Numpy、Pandas、Scikit-learn 等工具可大幅簡化實作。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href="https://www.youtube.com/watch?v=QK_Hv6pG4nE">線性代數（台大李宏毅課程）</a></li><li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">3Blue1Brown：線性代數動畫</a></li><li><a href=https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>Scikit-learn PCA 文件</a></li></ul><hr><h2 id=結語>結語</h2><p>線性代數是 AI 與資料科學的語言。掌握向量、矩陣、特徵值與降維技巧，不僅能幫助你理解模型背後的數學邏輯，也能在面試與實務專案中脫穎而出。未來章節將深入探討微積分、機率統計等 AI 必備數學，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>SVD</span>
<span class=tag>PCA</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>