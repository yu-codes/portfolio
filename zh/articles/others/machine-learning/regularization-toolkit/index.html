<!doctype html><html lang=zh class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>正則化武器庫：L1/L2、Dropout、Early Stopping、Label Smoothing 全解析 - Yu's Portfolio & Learning Hub</title><meta name=description content='正則化（Regularization）是防止模型過擬合、提升泛化能力的關鍵武器。從 L1/L2/Elastic Net，到 Dropout、DropPath、Stochastic Depth、Early Stopping、Label Smoothing、Confidence Penalty，本章將深入原理、實作、面試熱點與常見誤區，幫助你打造更穩健的模型。
L1 / L2 / Elastic Net L1 正則化（Lasso） 懲罰參數絕對值和，促使部分權重為 0，具備特徵選擇效果 L2 正則化（Ridge） 懲罰參數平方和，抑制權重過大，提升模型穩定性 Elastic Net 結合 L1 與 L2，兼顧特徵選擇與穩定性 import torch import torch.nn as nn l1_lambda = 0.01 l2_lambda = 0.01 l1_norm = sum(p.abs().sum() for p in model.parameters()) l2_norm = sum(p.pow(2).sum() for p in model.parameters()) loss = loss_fn(output, target) + l1_lambda * l1_norm + l2_lambda * l2_norm Dropout / DropPath / Stochastic Depth Dropout 訓練時隨機丟棄部分神經元，防止 co-adaptation，提升泛化 DropPath / Stochastic Depth 隨機丟棄整個層或路徑，常用於深層網路（如 ResNet、ViT） drop = nn.Dropout(p=0.5) x = torch.randn(10, 20) print("Dropout 輸出:", drop(x)) Early Stopping 判斷點 監控驗證集 loss，若多輪未提升則提前停止訓練 防止過擬合，節省資源 best_loss = float(&#39;inf&#39;) patience, counter = 5, 0 for epoch in range(epochs): # ...existing code... if val_loss < best_loss: best_loss = val_loss counter = 0 else: counter += 1 if counter >= patience: print("Early stopping triggered") break Label Smoothing & Confidence Penalty Label Smoothing 將 one-hot 標籤平滑，降低模型過度自信，提升泛化 Confidence Penalty 在損失中加入預測分布熵，懲罰過於集中的預測 import torch.nn.functional as F labels = torch.tensor([0, 1, 2]) n_classes = 3 smooth = 0.1 one_hot = F.one_hot(labels, n_classes).float() smoothed = one_hot * (1 - smooth) + smooth / n_classes print("Label Smoothing:", smoothed) # Confidence Penalty logits = torch.randn(4, 10) probs = F.softmax(logits, dim=1) entropy = - (probs * probs.log()).sum(dim=1).mean() loss = loss_fn(logits, targets) - 0.1 * entropy 理論直覺、應用場景與常見誤區 應用場景 L1/L2/Elastic Net：回歸、分類、特徵選擇 Dropout/DropPath：深度網路、過擬合防治 Early Stopping：所有需泛化的任務 Label Smoothing/Confidence Penalty：分類、生成模型 常見誤區 L1/L2 未正確設置權重，導致欠擬合 Dropout 只在訓練時啟用，推論時需關閉 Early Stopping 監控訓練集而非驗證集 Label Smoothing 過度平滑，降低辨識力 面試熱點與經典問題 主題 常見問題 L1 vs L2 差異與適用場景？ Dropout 原理與推論差異？ Early Stopping 如何設計判斷點？ Label Smoothing 何時用？有何效果？ Confidence Penalty 如何提升泛化？ 使用注意事項 正則化強度需根據資料與模型調整 Dropout/DropPath 建議搭配正規化層 Early Stopping 需監控驗證集 loss Label Smoothing/Confidence Penalty 適度使用 延伸閱讀與資源 Dropout 論文 Early Stopping 論文 Label Smoothing 論文 Elastic Net 論文 經典面試題與解法提示 L1/L2/Elastic Net 數學推導與適用場景？ Dropout/DropPath 原理與實作？ Early Stopping 如何設計與調參？ Label Smoothing/Confidence Penalty 差異？ 正則化過強/過弱會有什麼後果？ 如何用 Python 實作 Early Stopping？ Dropout 推論時如何處理？ Elastic Net 何時優於單一正則化？ Label Smoothing 對模型有何影響？ Confidence Penalty 如何提升泛化？ 結語 正則化是模型泛化的基石。熟悉 L1/L2、Dropout、Early Stopping、Label Smoothing 等技巧，能讓你打造更穩健的模型並在面試中脫穎而出。下一章將進入參數初始化與正規化層，敬請期待！
'><meta property="og:title" content="正則化武器庫：L1/L2、Dropout、Early Stopping、Label Smoothing 全解析"><meta property="og:description" content='正則化（Regularization）是防止模型過擬合、提升泛化能力的關鍵武器。從 L1/L2/Elastic Net，到 Dropout、DropPath、Stochastic Depth、Early Stopping、Label Smoothing、Confidence Penalty，本章將深入原理、實作、面試熱點與常見誤區，幫助你打造更穩健的模型。
L1 / L2 / Elastic Net L1 正則化（Lasso） 懲罰參數絕對值和，促使部分權重為 0，具備特徵選擇效果 L2 正則化（Ridge） 懲罰參數平方和，抑制權重過大，提升模型穩定性 Elastic Net 結合 L1 與 L2，兼顧特徵選擇與穩定性 import torch import torch.nn as nn l1_lambda = 0.01 l2_lambda = 0.01 l1_norm = sum(p.abs().sum() for p in model.parameters()) l2_norm = sum(p.pow(2).sum() for p in model.parameters()) loss = loss_fn(output, target) + l1_lambda * l1_norm + l2_lambda * l2_norm Dropout / DropPath / Stochastic Depth Dropout 訓練時隨機丟棄部分神經元，防止 co-adaptation，提升泛化 DropPath / Stochastic Depth 隨機丟棄整個層或路徑，常用於深層網路（如 ResNet、ViT） drop = nn.Dropout(p=0.5) x = torch.randn(10, 20) print("Dropout 輸出:", drop(x)) Early Stopping 判斷點 監控驗證集 loss，若多輪未提升則提前停止訓練 防止過擬合，節省資源 best_loss = float(&#39;inf&#39;) patience, counter = 5, 0 for epoch in range(epochs): # ...existing code... if val_loss < best_loss: best_loss = val_loss counter = 0 else: counter += 1 if counter >= patience: print("Early stopping triggered") break Label Smoothing & Confidence Penalty Label Smoothing 將 one-hot 標籤平滑，降低模型過度自信，提升泛化 Confidence Penalty 在損失中加入預測分布熵，懲罰過於集中的預測 import torch.nn.functional as F labels = torch.tensor([0, 1, 2]) n_classes = 3 smooth = 0.1 one_hot = F.one_hot(labels, n_classes).float() smoothed = one_hot * (1 - smooth) + smooth / n_classes print("Label Smoothing:", smoothed) # Confidence Penalty logits = torch.randn(4, 10) probs = F.softmax(logits, dim=1) entropy = - (probs * probs.log()).sum(dim=1).mean() loss = loss_fn(logits, targets) - 0.1 * entropy 理論直覺、應用場景與常見誤區 應用場景 L1/L2/Elastic Net：回歸、分類、特徵選擇 Dropout/DropPath：深度網路、過擬合防治 Early Stopping：所有需泛化的任務 Label Smoothing/Confidence Penalty：分類、生成模型 常見誤區 L1/L2 未正確設置權重，導致欠擬合 Dropout 只在訓練時啟用，推論時需關閉 Early Stopping 監控訓練集而非驗證集 Label Smoothing 過度平滑，降低辨識力 面試熱點與經典問題 主題 常見問題 L1 vs L2 差異與適用場景？ Dropout 原理與推論差異？ Early Stopping 如何設計判斷點？ Label Smoothing 何時用？有何效果？ Confidence Penalty 如何提升泛化？ 使用注意事項 正則化強度需根據資料與模型調整 Dropout/DropPath 建議搭配正規化層 Early Stopping 需監控驗證集 loss Label Smoothing/Confidence Penalty 適度使用 延伸閱讀與資源 Dropout 論文 Early Stopping 論文 Label Smoothing 論文 Elastic Net 論文 經典面試題與解法提示 L1/L2/Elastic Net 數學推導與適用場景？ Dropout/DropPath 原理與實作？ Early Stopping 如何設計與調參？ Label Smoothing/Confidence Penalty 差異？ 正則化過強/過弱會有什麼後果？ 如何用 Python 實作 Early Stopping？ Dropout 推論時如何處理？ Elastic Net 何時優於單一正則化？ Label Smoothing 對模型有何影響？ Confidence Penalty 如何提升泛化？ 結語 正則化是模型泛化的基石。熟悉 L1/L2、Dropout、Early Stopping、Label Smoothing 等技巧，能讓你打造更穩健的模型並在面試中脫穎而出。下一章將進入參數初始化與正規化層，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/regularization-toolkit/"><link rel=canonical href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/regularization-toolkit/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/zh/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>首頁</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>文章</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>履歷</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>專案</span>
</a><a href=https://yu-codes.github.io/portfolio/zh/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>歸檔</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title=切換主題（淺色/深色） aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title=切換語言 aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/zh/>首頁</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/>文章</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/>其他</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/zh/articles/others/machine-learning/>機器學習</a><span class=separator>&#8250;</span>
<span>正則化武器庫：L1/L2、Dropout、Early Stopping、Label Smoothing 全解析</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>正則化武器庫：L1/L2、Dropout、Early Stopping、Label Smoothing 全解析</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
最後更新: 2025-07-31</span></div></header><div class=article-body><p>正則化（Regularization）是防止模型過擬合、提升泛化能力的關鍵武器。從 L1/L2/Elastic Net，到 Dropout、DropPath、Stochastic Depth、Early Stopping、Label Smoothing、Confidence Penalty，本章將深入原理、實作、面試熱點與常見誤區，幫助你打造更穩健的模型。</p><hr><nav class=article-toc><span class=toc-title>目錄</span><nav id=TableOfContents><ul><li><a href=#l1--l2--elastic-net>L1 / L2 / Elastic Net</a><ul><li><a href=#l1-正則化lasso>L1 正則化（Lasso）</a></li><li><a href=#l2-正則化ridge>L2 正則化（Ridge）</a></li><li><a href=#elastic-net>Elastic Net</a></li></ul></li><li><a href=#dropout--droppath--stochastic-depth>Dropout / DropPath / Stochastic Depth</a><ul><li><a href=#dropout>Dropout</a></li><li><a href=#droppath--stochastic-depth>DropPath / Stochastic Depth</a></li></ul></li><li><a href=#early-stopping-判斷點>Early Stopping 判斷點</a></li><li><a href=#label-smoothing--confidence-penalty>Label Smoothing & Confidence Penalty</a><ul><li><a href=#label-smoothing>Label Smoothing</a></li><li><a href=#confidence-penalty>Confidence Penalty</a></li></ul></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=l1--l2--elastic-net>L1 / L2 / Elastic Net</h2><h3 id=l1-正則化lasso>L1 正則化（Lasso）</h3><ul><li>懲罰參數絕對值和，促使部分權重為 0，具備特徵選擇效果</li></ul><h3 id=l2-正則化ridge>L2 正則化（Ridge）</h3><ul><li>懲罰參數平方和，抑制權重過大，提升模型穩定性</li></ul><h3 id=elastic-net>Elastic Net</h3><ul><li>結合 L1 與 L2，兼顧特徵選擇與穩定性</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>l1_lambda <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>l2_lambda <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>l1_norm <span style=color:#f92672>=</span> sum(p<span style=color:#f92672>.</span>abs()<span style=color:#f92672>.</span>sum() <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>parameters())
</span></span><span style=display:flex><span>l2_norm <span style=color:#f92672>=</span> sum(p<span style=color:#f92672>.</span>pow(<span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>sum() <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>parameters())
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> loss_fn(output, target) <span style=color:#f92672>+</span> l1_lambda <span style=color:#f92672>*</span> l1_norm <span style=color:#f92672>+</span> l2_lambda <span style=color:#f92672>*</span> l2_norm
</span></span></code></pre></div><hr><h2 id=dropout--droppath--stochastic-depth>Dropout / DropPath / Stochastic Depth</h2><h3 id=dropout>Dropout</h3><ul><li>訓練時隨機丟棄部分神經元，防止 co-adaptation，提升泛化</li></ul><h3 id=droppath--stochastic-depth>DropPath / Stochastic Depth</h3><ul><li>隨機丟棄整個層或路徑，常用於深層網路（如 ResNet、ViT）</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>drop <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Dropout 輸出:&#34;</span>, drop(x))
</span></span></code></pre></div><hr><h2 id=early-stopping-判斷點>Early Stopping 判斷點</h2><ul><li>監控驗證集 loss，若多輪未提升則提前停止訓練</li><li>防止過擬合，節省資源</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>best_loss <span style=color:#f92672>=</span> float(<span style=color:#e6db74>&#39;inf&#39;</span>)
</span></span><span style=display:flex><span>patience, counter <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...existing code...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> val_loss <span style=color:#f92672>&lt;</span> best_loss:
</span></span><span style=display:flex><span>        best_loss <span style=color:#f92672>=</span> val_loss
</span></span><span style=display:flex><span>        counter <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        counter <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> counter <span style=color:#f92672>&gt;=</span> patience:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;Early stopping triggered&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span></code></pre></div><hr><h2 id=label-smoothing--confidence-penalty>Label Smoothing & Confidence Penalty</h2><h3 id=label-smoothing>Label Smoothing</h3><ul><li>將 one-hot 標籤平滑，降低模型過度自信，提升泛化</li></ul><h3 id=confidence-penalty>Confidence Penalty</h3><ul><li>在損失中加入預測分布熵，懲罰過於集中的預測</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>labels <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>n_classes <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>smooth <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>one_hot <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(labels, n_classes)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>smoothed <span style=color:#f92672>=</span> one_hot <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> smooth) <span style=color:#f92672>+</span> smooth <span style=color:#f92672>/</span> n_classes
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Label Smoothing:&#34;</span>, smoothed)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Confidence Penalty</span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(logits, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>entropy <span style=color:#f92672>=</span> <span style=color:#f92672>-</span> (probs <span style=color:#f92672>*</span> probs<span style=color:#f92672>.</span>log())<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> loss_fn(logits, targets) <span style=color:#f92672>-</span> <span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> entropy
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>L1/L2/Elastic Net：回歸、分類、特徵選擇</li><li>Dropout/DropPath：深度網路、過擬合防治</li><li>Early Stopping：所有需泛化的任務</li><li>Label Smoothing/Confidence Penalty：分類、生成模型</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>L1/L2 未正確設置權重，導致欠擬合</li><li>Dropout 只在訓練時啟用，推論時需關閉</li><li>Early Stopping 監控訓練集而非驗證集</li><li>Label Smoothing 過度平滑，降低辨識力</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>L1 vs L2</td><td>差異與適用場景？</td></tr><tr><td>Dropout</td><td>原理與推論差異？</td></tr><tr><td>Early Stopping</td><td>如何設計判斷點？</td></tr><tr><td>Label Smoothing</td><td>何時用？有何效果？</td></tr><tr><td>Confidence Penalty</td><td>如何提升泛化？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>正則化強度需根據資料與模型調整</li><li>Dropout/DropPath 建議搭配正規化層</li><li>Early Stopping 需監控驗證集 loss</li><li>Label Smoothing/Confidence Penalty 適度使用</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://arxiv.org/abs/1207.0580>Dropout 論文</a></li><li><a href=https://www.jmlr.org/papers/volume15/prechelt14a/prechelt14a.pdf>Early Stopping 論文</a></li><li><a href=https://arxiv.org/abs/1512.00567>Label Smoothing 論文</a></li><li><a href=https://www.jmlr.org/papers/volume5/zhang04a/zhang04a.pdf>Elastic Net 論文</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>L1/L2/Elastic Net 數學推導與適用場景？</li><li>Dropout/DropPath 原理與實作？</li><li>Early Stopping 如何設計與調參？</li><li>Label Smoothing/Confidence Penalty 差異？</li><li>正則化過強/過弱會有什麼後果？</li><li>如何用 Python 實作 Early Stopping？</li><li>Dropout 推論時如何處理？</li><li>Elastic Net 何時優於單一正則化？</li><li>Label Smoothing 對模型有何影響？</li><li>Confidence Penalty 如何提升泛化？</li></ol><hr><h2 id=結語>結語</h2><p>正則化是模型泛化的基石。熟悉 L1/L2、Dropout、Early Stopping、Label Smoothing 等技巧，能讓你打造更穩健的模型並在面試中脫穎而出。下一章將進入參數初始化與正規化層，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>L1</span>
<span class=tag>L2</span>
<span class=tag>Elastic Net</span>
<span class=tag>Dropout</span>
<span class=tag>Early Stopping</span>
<span class=tag>Label Smoothing</span>
<span class=tag>Confidence Penalty</span></div></div><footer class=article-footer><a href=/portfolio/zh/articles/others/machine-learning/ class=back-link>← 返回 機器學習</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>