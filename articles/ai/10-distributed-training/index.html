<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Distributed Training - Yu's Portfolio & Learning Hub</title><meta name=description content="Large models require distributed training. Understanding parallelism strategies and distributed frameworks enables training models that don&rsquo;t fit on a single GPU.
Why Distributed Training Matters Distributed training enables:
Training larger models Faster training times Efficient GPU utilization State-of-the-art results Subdomains Data Parallelism Distribute data across GPUs, synchronize gradients, and scale batch sizes effectively.
Model Parallelism Split models across devices, implement pipeline parallelism, and handle memory constraints.
Distributed Frameworks Use PyTorch DDP, Horovod, DeepSpeed, or FSDP for distributed training.
"><meta property="og:title" content="Distributed Training"><meta property="og:description" content="Large models require distributed training. Understanding parallelism strategies and distributed frameworks enables training models that don&rsquo;t fit on a single GPU.
Why Distributed Training Matters Distributed training enables:
Training larger models Faster training times Efficient GPU utilization State-of-the-art results Subdomains Data Parallelism Distribute data across GPUs, synchronize gradients, and scale batch sizes effectively.
Model Parallelism Split models across devices, implement pipeline parallelism, and handle memory constraints.
Distributed Frameworks Use PyTorch DDP, Horovod, DeepSpeed, or FSDP for distributed training.
"><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/ai/10-distributed-training/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/ai/10-distributed-training/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/ai/>AI Engineering</a><span class=separator>&#8250;</span>
<span>Distributed Training</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>Distributed Training</h1><div class=article-meta><span class=article-description>Scale model training across multiple GPUs and nodes for large-scale machine learning.</span></div></header><nav class=article-toc><h2 class=toc-title>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#why-distributed-training-matters>Why Distributed Training Matters</a></li><li><a href=#subdomains>Subdomains</a><ul><li><a href=#data-parallelism>Data Parallelism</a></li><li><a href=#model-parallelism>Model Parallelism</a></li><li><a href=#distributed-frameworks>Distributed Frameworks</a></li><li><a href=#large-language-models>Large Language Models</a></li><li><a href=#cloud-ml-training>Cloud ML Training</a></li><li><a href=#multi-node-training>Multi-Node Training</a></li></ul></li><li><a href=#parallelism-strategies>Parallelism Strategies</a></li></ul></nav></nav><div class=article-body><p>Large models require distributed training. Understanding parallelism strategies and distributed frameworks enables training models that don&rsquo;t fit on a single GPU.</p><h2 id=why-distributed-training-matters>Why Distributed Training Matters</h2><p>Distributed training enables:</p><ul><li>Training larger models</li><li>Faster training times</li><li>Efficient GPU utilization</li><li>State-of-the-art results</li></ul><h2 id=subdomains>Subdomains</h2><h3 id=data-parallelism>Data Parallelism</h3><p>Distribute data across GPUs, synchronize gradients, and scale batch sizes effectively.</p><h3 id=model-parallelism>Model Parallelism</h3><p>Split models across devices, implement pipeline parallelism, and handle memory constraints.</p><h3 id=distributed-frameworks>Distributed Frameworks</h3><p>Use PyTorch DDP, Horovod, DeepSpeed, or FSDP for distributed training.</p><h3 id=large-language-models>Large Language Models</h3><p>Understand techniques for training LLMs: ZeRO, tensor parallelism, and activation checkpointing.</p><h3 id=cloud-ml-training>Cloud ML Training</h3><p>Train on AWS SageMaker, GCP Vertex AI, or Azure ML. Optimize for cost and performance.</p><h3 id=multi-node-training>Multi-Node Training</h3><p>Set up multi-node clusters, handle communication overhead, and debug distributed issues.</p><h2 id=parallelism-strategies>Parallelism Strategies</h2><table><thead><tr><th>Strategy</th><th>Memory</th><th>Communication</th><th>Use Case</th></tr></thead><tbody><tr><td>Data</td><td>Per-GPU</td><td>Gradients</td><td>Most models</td></tr><tr><td>Model</td><td>Distributed</td><td>Activations</td><td>Very large models</td></tr><tr><td>Pipeline</td><td>Distributed</td><td>Staged</td><td>Very deep models</td></tr><tr><td>Tensor</td><td>Distributed</td><td>Layer splits</td><td>LLMs</td></tr></tbody></table><hr><p>Train at scale.</p><h2>Subcategories</h2><div class=subsection-list><a href=/portfolio/articles/ai/10-distributed-training/01-data-model-parallel/ class=subsection-item><span class=subsection-title>Data & Model Parallelism</span>
<span class=subsection-desc>Data parallel and model parallel training strategies.</span>
</a><a href=/portfolio/articles/ai/10-distributed-training/02-horovod-deepspeed/ class=subsection-item><span class=subsection-title>Horovod, DeepSpeed & Ray</span>
<span class=subsection-desc>Distributed training with Horovod, DeepSpeed, and Ray.</span>
</a><a href=/portfolio/articles/ai/10-distributed-training/03-mixed-precision/ class=subsection-item><span class=subsection-title>Mixed Precision Training</span>
<span class=subsection-desc>Automatic Mixed Precision (AMP) training techniques.</span>
</a><a href=/portfolio/articles/ai/10-distributed-training/04-cloud-gpu-tpu/ class=subsection-item><span class=subsection-title>Cloud GPU, TPU & Cluster</span>
<span class=subsection-desc>Training on cloud GPU, TPU, and cluster scheduling.</span></a></div></div><footer class=article-footer><a href=/portfolio/articles/ai/ class=back-link>‚Üê Back to AI Engineering</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>