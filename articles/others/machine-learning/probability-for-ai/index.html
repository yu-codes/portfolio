<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>機率論 Essentials：AI 必備的隨機思維與分布直覺 - Yu's Portfolio & Learning Hub</title><meta name=description content='機率論是資料科學與 AI 的基礎語言。從模型預測的不確定性，到生成模型的機率分布，背後都依賴機率論的核心概念。本篇將帶你掌握 AI 常用的機率觀念，並以直覺、圖解與 Python 範例說明。
隨機變數、CDF 與 PDF 隨機變數（Random Variable） 將隨機實驗的結果對應到數值的函數。 分為離散型（Discrete）與連續型（Continuous）。 機率質量函數（PMF）、機率密度函數（PDF）、累積分布函數（CDF） PMF：離散型隨機變數的機率分布（如擲骰子）。 PDF：連續型隨機變數的機率密度（如身高分布）。 CDF：隨機變數小於等於某值的累積機率。 import numpy as np from scipy.stats import norm, bernoulli import matplotlib.pyplot as plt # 連續型：標準常態分布 PDF/CDF x = np.linspace(-3, 3, 100) pdf = norm.pdf(x) cdf = norm.cdf(x) plt.plot(x, pdf, label="PDF") plt.plot(x, cdf, label="CDF") plt.legend(); plt.title("Normal Distribution"); plt.show() # 離散型：伯努利分布 PMF p = 0.7 x = [0, 1] pmf = bernoulli.pmf(x, p) print("Bernoulli PMF:", pmf) 常見分布一覽 分布名稱 參數 應用場景 常態分布 $\mu, \sigma$ 誤差、噪聲建模 伯努利分布 $p$ 二元分類 卡方分布 $k$ 假設檢定 Beta 分布 $\alpha, \beta$ 機率建模、貝氏推論 獨立、條件機率、全機率公式 獨立事件：$P(A \cap B) = P(A)P(B)$ 條件機率：$P(A|B) = \frac{P(A \cap B)}{P(B)}$ 全機率公式：將複雜事件拆解為多個子事件的加總。 # 條件機率範例 P_A = 0.3 P_B = 0.5 P_A_and_B = 0.15 P_A_given_B = P_A_and_B / P_B print("P(A|B) =", P_A_given_B) 貝氏定理 & 先驗 / 後驗直覺 貝氏定理：$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$ 先驗（Prior）：事前對事件的信念。 後驗（Posterior）：觀察到資料後，更新的信念。 在 AI 中，貝氏定理用於機率推論、生成模型、貝氏優化等場景。
'><meta property="og:title" content="機率論 Essentials：AI 必備的隨機思維與分布直覺"><meta property="og:description" content='機率論是資料科學與 AI 的基礎語言。從模型預測的不確定性，到生成模型的機率分布，背後都依賴機率論的核心概念。本篇將帶你掌握 AI 常用的機率觀念，並以直覺、圖解與 Python 範例說明。
隨機變數、CDF 與 PDF 隨機變數（Random Variable） 將隨機實驗的結果對應到數值的函數。 分為離散型（Discrete）與連續型（Continuous）。 機率質量函數（PMF）、機率密度函數（PDF）、累積分布函數（CDF） PMF：離散型隨機變數的機率分布（如擲骰子）。 PDF：連續型隨機變數的機率密度（如身高分布）。 CDF：隨機變數小於等於某值的累積機率。 import numpy as np from scipy.stats import norm, bernoulli import matplotlib.pyplot as plt # 連續型：標準常態分布 PDF/CDF x = np.linspace(-3, 3, 100) pdf = norm.pdf(x) cdf = norm.cdf(x) plt.plot(x, pdf, label="PDF") plt.plot(x, cdf, label="CDF") plt.legend(); plt.title("Normal Distribution"); plt.show() # 離散型：伯努利分布 PMF p = 0.7 x = [0, 1] pmf = bernoulli.pmf(x, p) print("Bernoulli PMF:", pmf) 常見分布一覽 分布名稱 參數 應用場景 常態分布 $\mu, \sigma$ 誤差、噪聲建模 伯努利分布 $p$ 二元分類 卡方分布 $k$ 假設檢定 Beta 分布 $\alpha, \beta$ 機率建模、貝氏推論 獨立、條件機率、全機率公式 獨立事件：$P(A \cap B) = P(A)P(B)$ 條件機率：$P(A|B) = \frac{P(A \cap B)}{P(B)}$ 全機率公式：將複雜事件拆解為多個子事件的加總。 # 條件機率範例 P_A = 0.3 P_B = 0.5 P_A_and_B = 0.15 P_A_given_B = P_A_and_B / P_B print("P(A|B) =", P_A_given_B) 貝氏定理 & 先驗 / 後驗直覺 貝氏定理：$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$ 先驗（Prior）：事前對事件的信念。 後驗（Posterior）：觀察到資料後，更新的信念。 在 AI 中，貝氏定理用於機率推論、生成模型、貝氏優化等場景。
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/probability-for-ai/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/probability-for-ai/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>機率論 Essentials：AI 必備的隨機思維與分布直覺</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>機率論 Essentials：AI 必備的隨機思維與分布直覺</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-10-14</span></div></header><div class=article-body><p>機率論是資料科學與 AI 的基礎語言。從模型預測的不確定性，到生成模型的機率分布，背後都依賴機率論的核心概念。本篇將帶你掌握 AI 常用的機率觀念，並以直覺、圖解與 Python 範例說明。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#隨機變數cdf-與-pdf>隨機變數、CDF 與 PDF</a><ul><li><a href=#隨機變數random-variable>隨機變數（Random Variable）</a></li><li><a href=#機率質量函數pmf機率密度函數pdf累積分布函數cdf>機率質量函數（PMF）、機率密度函數（PDF）、累積分布函數（CDF）</a></li></ul></li><li><a href=#常見分布一覽>常見分布一覽</a></li><li><a href=#獨立條件機率全機率公式>獨立、條件機率、全機率公式</a></li><li><a href=#貝氏定理--先驗--後驗直覺>貝氏定理 & 先驗 / 後驗直覺</a></li><li><a href=#常見面試熱點整理>常見面試熱點整理</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=隨機變數cdf-與-pdf>隨機變數、CDF 與 PDF</h2><h3 id=隨機變數random-variable>隨機變數（Random Variable）</h3><ul><li>將隨機實驗的結果對應到數值的函數。</li><li>分為離散型（Discrete）與連續型（Continuous）。</li></ul><h3 id=機率質量函數pmf機率密度函數pdf累積分布函數cdf>機率質量函數（PMF）、機率密度函數（PDF）、累積分布函數（CDF）</h3><ul><li><strong>PMF</strong>：離散型隨機變數的機率分布（如擲骰子）。</li><li><strong>PDF</strong>：連續型隨機變數的機率密度（如身高分布）。</li><li><strong>CDF</strong>：隨機變數小於等於某值的累積機率。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> scipy.stats <span style=color:#f92672>import</span> norm, bernoulli
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 連續型：標準常態分布 PDF/CDF</span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>pdf <span style=color:#f92672>=</span> norm<span style=color:#f92672>.</span>pdf(x)
</span></span><span style=display:flex><span>cdf <span style=color:#f92672>=</span> norm<span style=color:#f92672>.</span>cdf(x)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x, pdf, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;PDF&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x, cdf, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;CDF&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend(); plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Normal Distribution&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 離散型：伯努利分布 PMF</span>
</span></span><span style=display:flex><span>p <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.7</span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>pmf <span style=color:#f92672>=</span> bernoulli<span style=color:#f92672>.</span>pmf(x, p)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Bernoulli PMF:&#34;</span>, pmf)
</span></span></code></pre></div><hr><h2 id=常見分布一覽>常見分布一覽</h2><table><thead><tr><th>分布名稱</th><th>參數</th><th>應用場景</th></tr></thead><tbody><tr><td>常態分布</td><td>$\mu, \sigma$</td><td>誤差、噪聲建模</td></tr><tr><td>伯努利分布</td><td>$p$</td><td>二元分類</td></tr><tr><td>卡方分布</td><td>$k$</td><td>假設檢定</td></tr><tr><td>Beta 分布</td><td>$\alpha, \beta$</td><td>機率建模、貝氏推論</td></tr></tbody></table><hr><h2 id=獨立條件機率全機率公式>獨立、條件機率、全機率公式</h2><ul><li><strong>獨立事件</strong>：$P(A \cap B) = P(A)P(B)$</li><li><strong>條件機率</strong>：$P(A|B) = \frac{P(A \cap B)}{P(B)}$</li><li><strong>全機率公式</strong>：將複雜事件拆解為多個子事件的加總。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 條件機率範例</span>
</span></span><span style=display:flex><span>P_A <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.3</span>
</span></span><span style=display:flex><span>P_B <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>P_A_and_B <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.15</span>
</span></span><span style=display:flex><span>P_A_given_B <span style=color:#f92672>=</span> P_A_and_B <span style=color:#f92672>/</span> P_B
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;P(A|B) =&#34;</span>, P_A_given_B)
</span></span></code></pre></div><hr><h2 id=貝氏定理--先驗--後驗直覺>貝氏定理 & 先驗 / 後驗直覺</h2><ul><li><strong>貝氏定理</strong>：$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$</li><li><strong>先驗（Prior）</strong>：事前對事件的信念。</li><li><strong>後驗（Posterior）</strong>：觀察到資料後，更新的信念。</li></ul><blockquote><p>在 AI 中，貝氏定理用於機率推論、生成模型、貝氏優化等場景。</p></blockquote><hr><h2 id=常見面試熱點整理>常見面試熱點整理</h2><table><thead><tr><th>熱點主題</th><th>面試常問問題</th></tr></thead><tbody><tr><td>常態分布</td><td>為何常態分布如此重要？</td></tr><tr><td>條件機率</td><td>如何用全機率公式推導？</td></tr><tr><td>貝氏定理</td><td>先驗/後驗的直覺？</td></tr><tr><td>獨立性</td><td>什麼時候事件獨立？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>機率分布選擇會影響模型假設與推論結果。</li><li>條件機率與貝氏定理是理解生成模型與推論的基礎。</li><li>熟悉 SciPy、NumPy 等工具可簡化機率分布運算。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href="https://www.youtube.com/watch?v=Vfo5le26IhY">StatQuest: Probability Distributions</a></li><li><a href=https://zh.khanacademy.org/math/statistics-probability>Khan Academy: 機率與統計</a></li><li><a href=https://docs.scipy.org/doc/scipy/reference/stats.html>Scipy.stats 官方文件</a></li></ul><hr><h2 id=結語>結語</h2><p>機率論讓我們能量化不確定性，為 AI 模型提供理論基礎。掌握隨機變數、分布、條件機率與貝氏定理，能幫助你設計更強大、更靈活的機器學習模型。下一章將進入統計推論，敬請期待！</p></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>