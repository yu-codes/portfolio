<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>最適化基石：AI 訓練的數學與演算法核心 - Yu's Portfolio & Learning Hub</title><meta name=description content='最適化是機器學習與深度學習模型訓練的靈魂。從損失函數的設計，到參數的更新策略，背後都依賴數學上的最適化理論與演算法。本篇將帶你掌握 AI 常用的最適化觀念，並以直覺、圖解與 Python 範例說明。
凸集、凸函數與 Lagrange 乘子 凸集（Convex Set） 若集合內任兩點的連線也都在集合內，則為凸集。 在最適化中，凸集保證只有一個全域最小值，避免陷入局部極小。 凸函數（Convex Function） 任意兩點連線上的函數值不高於端點連線。 損失函數若為凸函數，訓練更穩定、易於收斂。 Lagrange 乘子法 處理有約束條件的最適化問題。 常見於 SVM、正則化等模型。 import numpy as np from scipy.optimize import minimize # 無約束最小化 def f(x): return (x - 2) ** 2 + 1 res = minimize(f, x0=0) print("最小值 x =", res.x, "f(x) =", res.fun) Batch / Mini-Batch / SGD 家族 Batch Gradient Descent：每次用全部資料計算梯度，收斂穩定但慢。 Mini-Batch Gradient Descent：每次用部分資料，兼顧效率與穩定。 Stochastic Gradient Descent (SGD)：每次只用一筆資料，更新頻繁但波動大。 方法 優點 缺點 適用場景 Batch 收斂平滑 記憶體需求高 小型資料集 Mini-Batch 效率與穩定兼顧 需調 batch size 主流深度學習訓練 SGD 即時更新快 收斂不穩定 線上學習 Learning Rate Schedules & Warm-up Learning Rate（學習率）：控制每次參數更新的步伐，過大易震盪，過小則收斂慢。 Learning Rate Schedule：隨訓練進度調整學習率（如 Step Decay、Cosine Annealing）。 Warm-up：訓練初期用較小學習率，避免一開始震盪過大。 import torch import torch.optim as optim model = torch.nn.Linear(10, 1) optimizer = optim.SGD(model.parameters(), lr=0.1) scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) for epoch in range(20): # ...訓練步驟... scheduler.step() print("Epoch", epoch, "Learning Rate:", scheduler.get_last_lr()) Momentum, Adam, RMSProp：何時選誰？ Momentum：累積過去梯度，幫助跳出局部極小。 RMSProp：自動調整每個參數的學習率，適合非平穩目標。 Adam：結合 Momentum 與 RMSProp，現今最常用的優化器之一。 Optimizer 特性 適用情境 SGD 基本款，易理解 小型/凸問題 Momentum 跳脫局部極小 深層網路 RMSProp 適應性學習率 非平穩目標 Adam 綜合型，穩定收斂 主流深度學習訓練 常見面試熱點整理 熱點主題 面試常問問題 凸函數 為何凸函數好優化？ Lagrange 乘子 什麼時候用？怎麼推導？ SGD/Adam 兩者差異與選擇時機？ Learning Rate 如何調整學習率？ 使用注意事項 學習率是最重要的超參數之一，建議多做實驗與調整。 Adam 雖穩定，但有時 SGD + Momentum 反而泛化較佳。 了解最適化理論有助於 debug 訓練異常與提升模型表現。 延伸閱讀與資源 Adam 論文 PyTorch Optimizer 官方文件 結語 最適化理論與演算法是 AI 訓練的核心。熟悉各種優化器、學習率策略與數學基礎，能讓你在模型訓練與調參時更加得心應手。下一章將進入機率論，敬請期待！
'><meta property="og:title" content="最適化基石：AI 訓練的數學與演算法核心"><meta property="og:description" content='最適化是機器學習與深度學習模型訓練的靈魂。從損失函數的設計，到參數的更新策略，背後都依賴數學上的最適化理論與演算法。本篇將帶你掌握 AI 常用的最適化觀念，並以直覺、圖解與 Python 範例說明。
凸集、凸函數與 Lagrange 乘子 凸集（Convex Set） 若集合內任兩點的連線也都在集合內，則為凸集。 在最適化中，凸集保證只有一個全域最小值，避免陷入局部極小。 凸函數（Convex Function） 任意兩點連線上的函數值不高於端點連線。 損失函數若為凸函數，訓練更穩定、易於收斂。 Lagrange 乘子法 處理有約束條件的最適化問題。 常見於 SVM、正則化等模型。 import numpy as np from scipy.optimize import minimize # 無約束最小化 def f(x): return (x - 2) ** 2 + 1 res = minimize(f, x0=0) print("最小值 x =", res.x, "f(x) =", res.fun) Batch / Mini-Batch / SGD 家族 Batch Gradient Descent：每次用全部資料計算梯度，收斂穩定但慢。 Mini-Batch Gradient Descent：每次用部分資料，兼顧效率與穩定。 Stochastic Gradient Descent (SGD)：每次只用一筆資料，更新頻繁但波動大。 方法 優點 缺點 適用場景 Batch 收斂平滑 記憶體需求高 小型資料集 Mini-Batch 效率與穩定兼顧 需調 batch size 主流深度學習訓練 SGD 即時更新快 收斂不穩定 線上學習 Learning Rate Schedules & Warm-up Learning Rate（學習率）：控制每次參數更新的步伐，過大易震盪，過小則收斂慢。 Learning Rate Schedule：隨訓練進度調整學習率（如 Step Decay、Cosine Annealing）。 Warm-up：訓練初期用較小學習率，避免一開始震盪過大。 import torch import torch.optim as optim model = torch.nn.Linear(10, 1) optimizer = optim.SGD(model.parameters(), lr=0.1) scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) for epoch in range(20): # ...訓練步驟... scheduler.step() print("Epoch", epoch, "Learning Rate:", scheduler.get_last_lr()) Momentum, Adam, RMSProp：何時選誰？ Momentum：累積過去梯度，幫助跳出局部極小。 RMSProp：自動調整每個參數的學習率，適合非平穩目標。 Adam：結合 Momentum 與 RMSProp，現今最常用的優化器之一。 Optimizer 特性 適用情境 SGD 基本款，易理解 小型/凸問題 Momentum 跳脫局部極小 深層網路 RMSProp 適應性學習率 非平穩目標 Adam 綜合型，穩定收斂 主流深度學習訓練 常見面試熱點整理 熱點主題 面試常問問題 凸函數 為何凸函數好優化？ Lagrange 乘子 什麼時候用？怎麼推導？ SGD/Adam 兩者差異與選擇時機？ Learning Rate 如何調整學習率？ 使用注意事項 學習率是最重要的超參數之一，建議多做實驗與調整。 Adam 雖穩定，但有時 SGD + Momentum 反而泛化較佳。 了解最適化理論有助於 debug 訓練異常與提升模型表現。 延伸閱讀與資源 Adam 論文 PyTorch Optimizer 官方文件 結語 最適化理論與演算法是 AI 訓練的核心。熟悉各種優化器、學習率策略與數學基礎，能讓你在模型訓練與調參時更加得心應手。下一章將進入機率論，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/optimization-for-ai/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/optimization-for-ai/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>最適化基石：AI 訓練的數學與演算法核心</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>最適化基石：AI 訓練的數學與演算法核心</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-12-23</span></div></header><div class=article-body><p>最適化是機器學習與深度學習模型訓練的靈魂。從損失函數的設計，到參數的更新策略，背後都依賴數學上的最適化理論與演算法。本篇將帶你掌握 AI 常用的最適化觀念，並以直覺、圖解與 Python 範例說明。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#凸集凸函數與-lagrange-乘子>凸集、凸函數與 Lagrange 乘子</a><ul><li><a href=#凸集convex-set>凸集（Convex Set）</a></li><li><a href=#凸函數convex-function>凸函數（Convex Function）</a></li><li><a href=#lagrange-乘子法>Lagrange 乘子法</a></li></ul></li><li><a href=#batch--mini-batch--sgd-家族>Batch / Mini-Batch / SGD 家族</a></li><li><a href=#learning-rate-schedules--warm-up>Learning Rate Schedules & Warm-up</a></li><li><a href=#momentum-adam-rmsprop何時選誰>Momentum, Adam, RMSProp：何時選誰？</a></li><li><a href=#常見面試熱點整理>常見面試熱點整理</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=凸集凸函數與-lagrange-乘子>凸集、凸函數與 Lagrange 乘子</h2><h3 id=凸集convex-set>凸集（Convex Set）</h3><ul><li>若集合內任兩點的連線也都在集合內，則為凸集。</li><li>在最適化中，凸集保證只有一個全域最小值，避免陷入局部極小。</li></ul><h3 id=凸函數convex-function>凸函數（Convex Function）</h3><ul><li>任意兩點連線上的函數值不高於端點連線。</li><li>損失函數若為凸函數，訓練更穩定、易於收斂。</li></ul><h3 id=lagrange-乘子法>Lagrange 乘子法</h3><ul><li>處理有約束條件的最適化問題。</li><li>常見於 SVM、正則化等模型。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> scipy.optimize <span style=color:#f92672>import</span> minimize
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 無約束最小化</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (x <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span>) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> minimize(f, x0<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;最小值 x =&#34;</span>, res<span style=color:#f92672>.</span>x, <span style=color:#e6db74>&#34;f(x) =&#34;</span>, res<span style=color:#f92672>.</span>fun)
</span></span></code></pre></div><hr><h2 id=batch--mini-batch--sgd-家族>Batch / Mini-Batch / SGD 家族</h2><ul><li><strong>Batch Gradient Descent</strong>：每次用全部資料計算梯度，收斂穩定但慢。</li><li><strong>Mini-Batch Gradient Descent</strong>：每次用部分資料，兼顧效率與穩定。</li><li><strong>Stochastic Gradient Descent (SGD)</strong>：每次只用一筆資料，更新頻繁但波動大。</li></ul><table><thead><tr><th>方法</th><th>優點</th><th>缺點</th><th>適用場景</th></tr></thead><tbody><tr><td>Batch</td><td>收斂平滑</td><td>記憶體需求高</td><td>小型資料集</td></tr><tr><td>Mini-Batch</td><td>效率與穩定兼顧</td><td>需調 batch size</td><td>主流深度學習訓練</td></tr><tr><td>SGD</td><td>即時更新快</td><td>收斂不穩定</td><td>線上學習</td></tr></tbody></table><hr><h2 id=learning-rate-schedules--warm-up>Learning Rate Schedules & Warm-up</h2><ul><li><strong>Learning Rate（學習率）</strong>：控制每次參數更新的步伐，過大易震盪，過小則收斂慢。</li><li><strong>Learning Rate Schedule</strong>：隨訓練進度調整學習率（如 Step Decay、Cosine Annealing）。</li><li><strong>Warm-up</strong>：訓練初期用較小學習率，避免一開始震盪過大。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.optim <span style=color:#66d9ef>as</span> optim
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>scheduler <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>lr_scheduler<span style=color:#f92672>.</span>StepLR(optimizer, step_size<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, gamma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>20</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...訓練步驟...</span>
</span></span><span style=display:flex><span>    scheduler<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Epoch&#34;</span>, epoch, <span style=color:#e6db74>&#34;Learning Rate:&#34;</span>, scheduler<span style=color:#f92672>.</span>get_last_lr())
</span></span></code></pre></div><hr><h2 id=momentum-adam-rmsprop何時選誰>Momentum, Adam, RMSProp：何時選誰？</h2><ul><li><strong>Momentum</strong>：累積過去梯度，幫助跳出局部極小。</li><li><strong>RMSProp</strong>：自動調整每個參數的學習率，適合非平穩目標。</li><li><strong>Adam</strong>：結合 Momentum 與 RMSProp，現今最常用的優化器之一。</li></ul><table><thead><tr><th>Optimizer</th><th>特性</th><th>適用情境</th></tr></thead><tbody><tr><td>SGD</td><td>基本款，易理解</td><td>小型/凸問題</td></tr><tr><td>Momentum</td><td>跳脫局部極小</td><td>深層網路</td></tr><tr><td>RMSProp</td><td>適應性學習率</td><td>非平穩目標</td></tr><tr><td>Adam</td><td>綜合型，穩定收斂</td><td>主流深度學習訓練</td></tr></tbody></table><hr><h2 id=常見面試熱點整理>常見面試熱點整理</h2><table><thead><tr><th>熱點主題</th><th>面試常問問題</th></tr></thead><tbody><tr><td>凸函數</td><td>為何凸函數好優化？</td></tr><tr><td>Lagrange 乘子</td><td>什麼時候用？怎麼推導？</td></tr><tr><td>SGD/Adam</td><td>兩者差異與選擇時機？</td></tr><tr><td>Learning Rate</td><td>如何調整學習率？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>學習率是最重要的超參數之一，建議多做實驗與調整。</li><li>Adam 雖穩定，但有時 SGD + Momentum 反而泛化較佳。</li><li>了解最適化理論有助於 debug 訓練異常與提升模型表現。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://arxiv.org/abs/1412.6980>Adam 論文</a></li><li><a href=https://pytorch.org/docs/stable/optim.html>PyTorch Optimizer 官方文件</a></li></ul><hr><h2 id=結語>結語</h2><p>最適化理論與演算法是 AI 訓練的核心。熟悉各種優化器、學習率策略與數學基礎，能讓你在模型訓練與調參時更加得心應手。下一章將進入機率論，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>Lagrange</span>
<span class=tag>SGD</span>
<span class=tag>Adam</span>
<span class=tag>Learning Rate</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>