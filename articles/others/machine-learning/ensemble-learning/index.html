<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>集成學習全攻略：Bagging、Boosting、Stacking 與超學習器 - Yu's Portfolio & Learning Hub</title><meta name=description content='集成學習（Ensemble Learning）是提升模型準確率與穩定性的利器。從 Bagging、Random Forest，到 Boosting（如 AdaBoost、XGBoost、LightGBM），再到 Stacking、Blending 與超學習器，這些方法已成為 Kaggle 競賽與產業應用的標配。本章將深入數學原理、直覺圖解、Python 實作、面試熱點、優缺點與常見誤區，幫助你全面掌握集成學習。
Bagging 與 Random Forest Bagging（Bootstrap Aggregating） 多次隨機有放回抽樣訓練多個弱模型，最後投票或平均。 降低變異（Variance），提升穩定性。 Random Forest Bagging 的進階版，結合多棵決策樹，每棵樹訓練時隨機選特徵分裂。 適合高維、異質特徵資料，抗過擬合。 from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True) rf = RandomForestClassifier(n_estimators=100) rf.fit(X, y) print("RF 預測:", rf.predict(X[:5])) print("特徵重要性:", rf.feature_importances_) 優缺點 優點：抗過擬合、可解釋性佳、訓練快。 缺點：模型大、預測慢、不適合極高維稀疏資料。 Boosting：AdaBoost、Gradient Boosting、XGBoost、LightGBM Boosting 原理 逐步訓練弱模型，每一步聚焦前一步錯誤樣本。 最終加權組合所有弱模型，提升準確率。 AdaBoost 每輪調整樣本權重，讓錯誤樣本被更多關注。 適合簡單弱分類器（如決策樹樁）。 Gradient Boosting 每輪擬合前一輪殘差，逐步逼近真實值。 支援回歸與分類。 XGBoost / LightGBM 進階 Gradient Boosting，支援特徵自動選擇、缺失值處理、分布式訓練。 XGBoost：正則化強、速度快、Kaggle 常勝軍。 LightGBM：更快、支援大資料、leaf-wise 分裂。 from sklearn.ensemble import GradientBoostingClassifier gb = GradientBoostingClassifier(n_estimators=100) gb.fit(X, y) print("GB 預測:", gb.predict(X[:5])) Stacking / Blending 與超學習器 (Super-Learner) Stacking 多種不同模型（Level-0），用另一個模型（Level-1）學習如何組合預測。 可大幅提升泛化能力。 Blending 類似 Stacking，但 Level-1 僅用驗證集訓練，減少資料洩漏風險。 超學習器（Super-Learner） 理論上可逼近最佳泛化誤差的集成方法。 實務上常用於競賽、AutoML。 from sklearn.ensemble import StackingClassifier from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier estimators = [ (&#39;rf&#39;, RandomForestClassifier(n_estimators=10)), (&#39;dt&#39;, DecisionTreeClassifier()) ] stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression()) stack.fit(X, y) print("Stacking 預測:", stack.predict(X[:5])) 理論直覺、圖解與應用場景 Bagging：降低變異，適合高變異弱模型（如決策樹）。 Boosting：降低偏差，適合弱模型表現差但可提升。 Stacking：結合多種模型優勢，提升泛化。 實務應用：金融風控、醫療預測、推薦系統、Kaggle 競賽。 面試熱點與常見誤區 主題 常見問題 Bagging 為何能降低變異？ Random Forest 特徵重要性如何計算？ Boosting 與 Bagging 差異？ XGBoost 為何表現好？有哪些 trick？ Stacking 如何避免資料洩漏？ 常見誤區 Boosting 易過擬合，需調整學習率與樹深。 Stacking 未分層抽樣，導致 Level-1 過擬合。 Random Forest 特徵重要性僅供參考，非因果。 使用注意事項 集成方法需搭配交叉驗證，避免過擬合。 Boosting 類模型需謹慎調參（學習率、樹深、子樣本比例）。 Stacking/Blending 須嚴格分離訓練/驗證集。 延伸閱讀與資源 StatQuest: Bagging, Boosting, Stacking XGBoost 官方文件 LightGBM 官方文件 Scikit-learn Ensemble Methods 經典面試題與解法提示 Bagging 與 Boosting 的數學推導與差異？ 為何 Random Forest 不易過擬合？ XGBoost 如何處理缺失值？ Stacking 如何設計 Level-1 模型？ Boosting 為何對異常值敏感？ Bagging 適合哪些弱模型？ 如何用 Python 實作 Stacking？ LightGBM 與 XGBoost 差異？ 集成方法如何提升泛化能力？ 超學習器理論基礎？ 結語 集成學習是提升模型表現的關鍵武器。熟悉 Bagging、Boosting、Stacking 與超學習器的原理、實作與調參技巧，能讓你在競賽與實務中脫穎而出。下一章將進入非監督學習大補帖，敬請期待！
'><meta property="og:title" content="集成學習全攻略：Bagging、Boosting、Stacking 與超學習器"><meta property="og:description" content='集成學習（Ensemble Learning）是提升模型準確率與穩定性的利器。從 Bagging、Random Forest，到 Boosting（如 AdaBoost、XGBoost、LightGBM），再到 Stacking、Blending 與超學習器，這些方法已成為 Kaggle 競賽與產業應用的標配。本章將深入數學原理、直覺圖解、Python 實作、面試熱點、優缺點與常見誤區，幫助你全面掌握集成學習。
Bagging 與 Random Forest Bagging（Bootstrap Aggregating） 多次隨機有放回抽樣訓練多個弱模型，最後投票或平均。 降低變異（Variance），提升穩定性。 Random Forest Bagging 的進階版，結合多棵決策樹，每棵樹訓練時隨機選特徵分裂。 適合高維、異質特徵資料，抗過擬合。 from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True) rf = RandomForestClassifier(n_estimators=100) rf.fit(X, y) print("RF 預測:", rf.predict(X[:5])) print("特徵重要性:", rf.feature_importances_) 優缺點 優點：抗過擬合、可解釋性佳、訓練快。 缺點：模型大、預測慢、不適合極高維稀疏資料。 Boosting：AdaBoost、Gradient Boosting、XGBoost、LightGBM Boosting 原理 逐步訓練弱模型，每一步聚焦前一步錯誤樣本。 最終加權組合所有弱模型，提升準確率。 AdaBoost 每輪調整樣本權重，讓錯誤樣本被更多關注。 適合簡單弱分類器（如決策樹樁）。 Gradient Boosting 每輪擬合前一輪殘差，逐步逼近真實值。 支援回歸與分類。 XGBoost / LightGBM 進階 Gradient Boosting，支援特徵自動選擇、缺失值處理、分布式訓練。 XGBoost：正則化強、速度快、Kaggle 常勝軍。 LightGBM：更快、支援大資料、leaf-wise 分裂。 from sklearn.ensemble import GradientBoostingClassifier gb = GradientBoostingClassifier(n_estimators=100) gb.fit(X, y) print("GB 預測:", gb.predict(X[:5])) Stacking / Blending 與超學習器 (Super-Learner) Stacking 多種不同模型（Level-0），用另一個模型（Level-1）學習如何組合預測。 可大幅提升泛化能力。 Blending 類似 Stacking，但 Level-1 僅用驗證集訓練，減少資料洩漏風險。 超學習器（Super-Learner） 理論上可逼近最佳泛化誤差的集成方法。 實務上常用於競賽、AutoML。 from sklearn.ensemble import StackingClassifier from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier estimators = [ (&#39;rf&#39;, RandomForestClassifier(n_estimators=10)), (&#39;dt&#39;, DecisionTreeClassifier()) ] stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression()) stack.fit(X, y) print("Stacking 預測:", stack.predict(X[:5])) 理論直覺、圖解與應用場景 Bagging：降低變異，適合高變異弱模型（如決策樹）。 Boosting：降低偏差，適合弱模型表現差但可提升。 Stacking：結合多種模型優勢，提升泛化。 實務應用：金融風控、醫療預測、推薦系統、Kaggle 競賽。 面試熱點與常見誤區 主題 常見問題 Bagging 為何能降低變異？ Random Forest 特徵重要性如何計算？ Boosting 與 Bagging 差異？ XGBoost 為何表現好？有哪些 trick？ Stacking 如何避免資料洩漏？ 常見誤區 Boosting 易過擬合，需調整學習率與樹深。 Stacking 未分層抽樣，導致 Level-1 過擬合。 Random Forest 特徵重要性僅供參考，非因果。 使用注意事項 集成方法需搭配交叉驗證，避免過擬合。 Boosting 類模型需謹慎調參（學習率、樹深、子樣本比例）。 Stacking/Blending 須嚴格分離訓練/驗證集。 延伸閱讀與資源 StatQuest: Bagging, Boosting, Stacking XGBoost 官方文件 LightGBM 官方文件 Scikit-learn Ensemble Methods 經典面試題與解法提示 Bagging 與 Boosting 的數學推導與差異？ 為何 Random Forest 不易過擬合？ XGBoost 如何處理缺失值？ Stacking 如何設計 Level-1 模型？ Boosting 為何對異常值敏感？ Bagging 適合哪些弱模型？ 如何用 Python 實作 Stacking？ LightGBM 與 XGBoost 差異？ 集成方法如何提升泛化能力？ 超學習器理論基礎？ 結語 集成學習是提升模型表現的關鍵武器。熟悉 Bagging、Boosting、Stacking 與超學習器的原理、實作與調參技巧，能讓你在競賽與實務中脫穎而出。下一章將進入非監督學習大補帖，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/ensemble-learning/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/ensemble-learning/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>集成學習全攻略：Bagging、Boosting、Stacking 與超學習器</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>集成學習全攻略：Bagging、Boosting、Stacking 與超學習器</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-10-04</span></div></header><div class=article-body><p>集成學習（Ensemble Learning）是提升模型準確率與穩定性的利器。從 Bagging、Random Forest，到 Boosting（如 AdaBoost、XGBoost、LightGBM），再到 Stacking、Blending 與超學習器，這些方法已成為 Kaggle 競賽與產業應用的標配。本章將深入數學原理、直覺圖解、Python 實作、面試熱點、優缺點與常見誤區，幫助你全面掌握集成學習。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#bagging-與-random-forest>Bagging 與 Random Forest</a><ul><li><a href=#baggingbootstrap-aggregating>Bagging（Bootstrap Aggregating）</a></li><li><a href=#random-forest>Random Forest</a></li><li><a href=#優缺點>優缺點</a></li></ul></li><li><a href=#boostingadaboostgradient-boostingxgboostlightgbm>Boosting：AdaBoost、Gradient Boosting、XGBoost、LightGBM</a><ul><li><a href=#boosting-原理>Boosting 原理</a></li><li><a href=#adaboost>AdaBoost</a></li><li><a href=#gradient-boosting>Gradient Boosting</a></li><li><a href=#xgboost--lightgbm>XGBoost / LightGBM</a></li></ul></li><li><a href=#stacking--blending-與超學習器-super-learner>Stacking / Blending 與超學習器 (Super-Learner)</a><ul><li><a href=#stacking>Stacking</a></li><li><a href=#blending>Blending</a></li><li><a href=#超學習器super-learner>超學習器（Super-Learner）</a></li></ul></li><li><a href=#理論直覺圖解與應用場景>理論直覺、圖解與應用場景</a></li><li><a href=#面試熱點與常見誤區>面試熱點與常見誤區</a><ul><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=bagging-與-random-forest>Bagging 與 Random Forest</h2><h3 id=baggingbootstrap-aggregating>Bagging（Bootstrap Aggregating）</h3><ul><li>多次隨機有放回抽樣訓練多個弱模型，最後投票或平均。</li><li>降低變異（Variance），提升穩定性。</li></ul><h3 id=random-forest>Random Forest</h3><ul><li>Bagging 的進階版，結合多棵決策樹，每棵樹訓練時隨機選特徵分裂。</li><li>適合高維、異質特徵資料，抗過擬合。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestClassifier
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> load_iris
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=color:#f92672>=</span> load_iris(return_X_y<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>rf <span style=color:#f92672>=</span> RandomForestClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>rf<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;RF 預測:&#34;</span>, rf<span style=color:#f92672>.</span>predict(X[:<span style=color:#ae81ff>5</span>]))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;特徵重要性:&#34;</span>, rf<span style=color:#f92672>.</span>feature_importances_)
</span></span></code></pre></div><h3 id=優缺點>優缺點</h3><ul><li>優點：抗過擬合、可解釋性佳、訓練快。</li><li>缺點：模型大、預測慢、不適合極高維稀疏資料。</li></ul><hr><h2 id=boostingadaboostgradient-boostingxgboostlightgbm>Boosting：AdaBoost、Gradient Boosting、XGBoost、LightGBM</h2><h3 id=boosting-原理>Boosting 原理</h3><ul><li>逐步訓練弱模型，每一步聚焦前一步錯誤樣本。</li><li>最終加權組合所有弱模型，提升準確率。</li></ul><h3 id=adaboost>AdaBoost</h3><ul><li>每輪調整樣本權重，讓錯誤樣本被更多關注。</li><li>適合簡單弱分類器（如決策樹樁）。</li></ul><h3 id=gradient-boosting>Gradient Boosting</h3><ul><li>每輪擬合前一輪殘差，逐步逼近真實值。</li><li>支援回歸與分類。</li></ul><h3 id=xgboost--lightgbm>XGBoost / LightGBM</h3><ul><li>進階 Gradient Boosting，支援特徵自動選擇、缺失值處理、分布式訓練。</li><li>XGBoost：正則化強、速度快、Kaggle 常勝軍。</li><li>LightGBM：更快、支援大資料、leaf-wise 分裂。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> GradientBoostingClassifier
</span></span><span style=display:flex><span>gb <span style=color:#f92672>=</span> GradientBoostingClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>gb<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;GB 預測:&#34;</span>, gb<span style=color:#f92672>.</span>predict(X[:<span style=color:#ae81ff>5</span>]))
</span></span></code></pre></div><hr><h2 id=stacking--blending-與超學習器-super-learner>Stacking / Blending 與超學習器 (Super-Learner)</h2><h3 id=stacking>Stacking</h3><ul><li>多種不同模型（Level-0），用另一個模型（Level-1）學習如何組合預測。</li><li>可大幅提升泛化能力。</li></ul><h3 id=blending>Blending</h3><ul><li>類似 Stacking，但 Level-1 僅用驗證集訓練，減少資料洩漏風險。</li></ul><h3 id=超學習器super-learner>超學習器（Super-Learner）</h3><ul><li>理論上可逼近最佳泛化誤差的集成方法。</li><li>實務上常用於競賽、AutoML。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> StackingClassifier
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LogisticRegression
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.tree <span style=color:#f92672>import</span> DecisionTreeClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>estimators <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    (<span style=color:#e6db74>&#39;rf&#39;</span>, RandomForestClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)),
</span></span><span style=display:flex><span>    (<span style=color:#e6db74>&#39;dt&#39;</span>, DecisionTreeClassifier())
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>stack <span style=color:#f92672>=</span> StackingClassifier(estimators<span style=color:#f92672>=</span>estimators, final_estimator<span style=color:#f92672>=</span>LogisticRegression())
</span></span><span style=display:flex><span>stack<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Stacking 預測:&#34;</span>, stack<span style=color:#f92672>.</span>predict(X[:<span style=color:#ae81ff>5</span>]))
</span></span></code></pre></div><hr><h2 id=理論直覺圖解與應用場景>理論直覺、圖解與應用場景</h2><ul><li>Bagging：降低變異，適合高變異弱模型（如決策樹）。</li><li>Boosting：降低偏差，適合弱模型表現差但可提升。</li><li>Stacking：結合多種模型優勢，提升泛化。</li><li>實務應用：金融風控、醫療預測、推薦系統、Kaggle 競賽。</li></ul><hr><h2 id=面試熱點與常見誤區>面試熱點與常見誤區</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>Bagging</td><td>為何能降低變異？</td></tr><tr><td>Random Forest</td><td>特徵重要性如何計算？</td></tr><tr><td>Boosting</td><td>與 Bagging 差異？</td></tr><tr><td>XGBoost</td><td>為何表現好？有哪些 trick？</td></tr><tr><td>Stacking</td><td>如何避免資料洩漏？</td></tr></tbody></table><h3 id=常見誤區>常見誤區</h3><ul><li>Boosting 易過擬合，需調整學習率與樹深。</li><li>Stacking 未分層抽樣，導致 Level-1 過擬合。</li><li>Random Forest 特徵重要性僅供參考，非因果。</li></ul><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>集成方法需搭配交叉驗證，避免過擬合。</li><li>Boosting 類模型需謹慎調參（學習率、樹深、子樣本比例）。</li><li>Stacking/Blending 須嚴格分離訓練/驗證集。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://www.youtube.com/c/joshstarmer>StatQuest: Bagging, Boosting, Stacking</a></li><li><a href=https://xgboost.readthedocs.io/>XGBoost 官方文件</a></li><li><a href=https://lightgbm.readthedocs.io/>LightGBM 官方文件</a></li><li><a href=https://scikit-learn.org/stable/modules/ensemble.html>Scikit-learn Ensemble Methods</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>Bagging 與 Boosting 的數學推導與差異？</li><li>為何 Random Forest 不易過擬合？</li><li>XGBoost 如何處理缺失值？</li><li>Stacking 如何設計 Level-1 模型？</li><li>Boosting 為何對異常值敏感？</li><li>Bagging 適合哪些弱模型？</li><li>如何用 Python 實作 Stacking？</li><li>LightGBM 與 XGBoost 差異？</li><li>集成方法如何提升泛化能力？</li><li>超學習器理論基礎？</li></ol><hr><h2 id=結語>結語</h2><p>集成學習是提升模型表現的關鍵武器。熟悉 Bagging、Boosting、Stacking 與超學習器的原理、實作與調參技巧，能讓你在競賽與實務中脫穎而出。下一章將進入非監督學習大補帖，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>Bagging</span>
<span class=tag>Boosting</span>
<span class=tag>Random Forest</span>
<span class=tag>XGBoost</span>
<span class=tag>Stacking</span>
<span class=tag>Blending</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>