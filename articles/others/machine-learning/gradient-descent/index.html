<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>梯度下降家譜：SGD、Momentum、Adam、Adaptive 優化器全解析 - Yu's Portfolio & Learning Hub</title><meta name=description content="梯度下降法是深度學習訓練的核心。從 Batch/Mini-Batch/SGD，到 Momentum、Nesterov、AdaGrad、RMSProp、Adam、AdamW、NAdam、AdamP，這些優化器直接影響收斂速度與泛化能力。本章將深入數學原理、直覺圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握梯度下降與優化器選擇。
Batch／Mini-Batch／Stochastic GD Batch GD：每次用全部資料計算梯度，收斂平滑但慢，需大記憶體 Mini-Batch GD：每次用部分資料，兼顧效率與穩定，主流選擇 Stochastic GD (SGD)：每次只用一筆資料，更新頻繁但波動大 import torch import torch.optim as optim model = torch.nn.Linear(10, 1) optimizer = optim.SGD(model.parameters(), lr=0.01) for x, y in dataloader: # Mini-Batch optimizer.zero_grad() loss = loss_fn(model(x), y) loss.backward() optimizer.step() Momentum, Nesterov, AdaGrad, RMSProp Momentum 累積過去梯度，幫助跳出局部極小 更新規則：$v_{t+1} = \beta v_t + (1-\beta)\nabla L$，$w_{t+1} = w_t - \eta v_{t+1}$ Nesterov Momentum 先預測一步再計算梯度，收斂更快 AdaGrad 為每個參數自適應調整學習率，適合稀疏資料 RMSProp 解決 AdaGrad 學習率過快衰減，適合非平穩目標 optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True) optimizer_adagrad = optim.Adagrad(model.parameters(), lr=0.01) optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.01) Adam / AdamW／NAdam／AdamP Adam 結合 Momentum 與 RMSProp，適應性強，主流選擇 $m_t$（一階動量）、$v_t$（二階動量），帶偏差校正 AdamW 將權重衰減與梯度分離，泛化更佳 NAdam / AdamP NAdam：結合 Nesterov 動量 AdamP：針對 CNN 設計，提升泛化 optimizer_adam = optim.Adam(model.parameters(), lr=0.001) optimizer_adamw = optim.AdamW(model.parameters(), lr=0.001) SGD vs. Adaptive：收斂速度與泛化 Adaptive（如 Adam）收斂快，適合複雜/稀疏資料 SGD + Momentum 泛化能力常優於 Adam，適合大規模訓練 實務常先用 Adam，收斂後切換 SGD 微調 理論直覺、應用場景與常見誤區 應用場景 SGD：大規模影像、語音、NLP Adam/AdamW：NLP、稀疏特徵、預訓練模型 AdaGrad/RMSProp：稀疏資料、非平穩目標 常見誤區 Adam 泛化不佳，未調整學習率 schedule Nesterov 需正確設置 momentum Adaptive 優化器未設 weight decay，易過擬合 面試熱點與經典問題 主題 常見問題 SGD vs Adam 收斂速度與泛化差異？ Momentum 如何幫助跳出局部極小？ AdamW 為何較 Adam 泛化好？ AdaGrad/RMSProp 適用場景與數學原理？ Nesterov 與 Momentum 差異？ 使用注意事項 優化器選擇需根據任務與資料特性 Adaptive 優化器建議搭配學習率衰減與 weight decay 訓練後期可考慮切換 SGD 微調 延伸閱讀與資源 PyTorch Optimizer 官方文件 Adam 論文 AdamW 論文 RMSProp 論文 經典面試題與解法提示 SGD、Momentum、Nesterov、Adam、AdamW 更新規則？ Adam 為何收斂快但泛化差？ AdamW 與 Adam 的數學差異？ AdaGrad/RMSProp 適用場景？ Adaptive 優化器 weight decay 設置？ SGD 何時優於 Adam？ 如何用 Python 切換優化器？ Nesterov Momentum 的數學推導？ AdamP 有何創新？ 優化器選擇對訓練有何影響？ 結語 梯度下降與優化器是模型訓練的核心。熟悉 SGD、Momentum、Adam、AdamW 等優化器原理與實作，能讓你在訓練與面試中展現專業素養。下一章將進入學習率策略，敬請期待！
"><meta property="og:title" content="梯度下降家譜：SGD、Momentum、Adam、Adaptive 優化器全解析"><meta property="og:description" content="梯度下降法是深度學習訓練的核心。從 Batch/Mini-Batch/SGD，到 Momentum、Nesterov、AdaGrad、RMSProp、Adam、AdamW、NAdam、AdamP，這些優化器直接影響收斂速度與泛化能力。本章將深入數學原理、直覺圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握梯度下降與優化器選擇。
Batch／Mini-Batch／Stochastic GD Batch GD：每次用全部資料計算梯度，收斂平滑但慢，需大記憶體 Mini-Batch GD：每次用部分資料，兼顧效率與穩定，主流選擇 Stochastic GD (SGD)：每次只用一筆資料，更新頻繁但波動大 import torch import torch.optim as optim model = torch.nn.Linear(10, 1) optimizer = optim.SGD(model.parameters(), lr=0.01) for x, y in dataloader: # Mini-Batch optimizer.zero_grad() loss = loss_fn(model(x), y) loss.backward() optimizer.step() Momentum, Nesterov, AdaGrad, RMSProp Momentum 累積過去梯度，幫助跳出局部極小 更新規則：$v_{t+1} = \beta v_t + (1-\beta)\nabla L$，$w_{t+1} = w_t - \eta v_{t+1}$ Nesterov Momentum 先預測一步再計算梯度，收斂更快 AdaGrad 為每個參數自適應調整學習率，適合稀疏資料 RMSProp 解決 AdaGrad 學習率過快衰減，適合非平穩目標 optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True) optimizer_adagrad = optim.Adagrad(model.parameters(), lr=0.01) optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.01) Adam / AdamW／NAdam／AdamP Adam 結合 Momentum 與 RMSProp，適應性強，主流選擇 $m_t$（一階動量）、$v_t$（二階動量），帶偏差校正 AdamW 將權重衰減與梯度分離，泛化更佳 NAdam / AdamP NAdam：結合 Nesterov 動量 AdamP：針對 CNN 設計，提升泛化 optimizer_adam = optim.Adam(model.parameters(), lr=0.001) optimizer_adamw = optim.AdamW(model.parameters(), lr=0.001) SGD vs. Adaptive：收斂速度與泛化 Adaptive（如 Adam）收斂快，適合複雜/稀疏資料 SGD + Momentum 泛化能力常優於 Adam，適合大規模訓練 實務常先用 Adam，收斂後切換 SGD 微調 理論直覺、應用場景與常見誤區 應用場景 SGD：大規模影像、語音、NLP Adam/AdamW：NLP、稀疏特徵、預訓練模型 AdaGrad/RMSProp：稀疏資料、非平穩目標 常見誤區 Adam 泛化不佳，未調整學習率 schedule Nesterov 需正確設置 momentum Adaptive 優化器未設 weight decay，易過擬合 面試熱點與經典問題 主題 常見問題 SGD vs Adam 收斂速度與泛化差異？ Momentum 如何幫助跳出局部極小？ AdamW 為何較 Adam 泛化好？ AdaGrad/RMSProp 適用場景與數學原理？ Nesterov 與 Momentum 差異？ 使用注意事項 優化器選擇需根據任務與資料特性 Adaptive 優化器建議搭配學習率衰減與 weight decay 訓練後期可考慮切換 SGD 微調 延伸閱讀與資源 PyTorch Optimizer 官方文件 Adam 論文 AdamW 論文 RMSProp 論文 經典面試題與解法提示 SGD、Momentum、Nesterov、Adam、AdamW 更新規則？ Adam 為何收斂快但泛化差？ AdamW 與 Adam 的數學差異？ AdaGrad/RMSProp 適用場景？ Adaptive 優化器 weight decay 設置？ SGD 何時優於 Adam？ 如何用 Python 切換優化器？ Nesterov Momentum 的數學推導？ AdamP 有何創新？ 優化器選擇對訓練有何影響？ 結語 梯度下降與優化器是模型訓練的核心。熟悉 SGD、Momentum、Adam、AdamW 等優化器原理與實作，能讓你在訓練與面試中展現專業素養。下一章將進入學習率策略，敬請期待！
"><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/gradient-descent/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/gradient-descent/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>梯度下降家譜：SGD、Momentum、Adam、Adaptive 優化器全解析</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>梯度下降家譜：SGD、Momentum、Adam、Adaptive 優化器全解析</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-01-09</span></div></header><div class=article-body><p>梯度下降法是深度學習訓練的核心。從 Batch/Mini-Batch/SGD，到 Momentum、Nesterov、AdaGrad、RMSProp、Adam、AdamW、NAdam、AdamP，這些優化器直接影響收斂速度與泛化能力。本章將深入數學原理、直覺圖解、Python 實作、面試熱點與常見誤區，幫助你全面掌握梯度下降與優化器選擇。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#batchmini-batchstochastic-gd>Batch／Mini-Batch／Stochastic GD</a></li><li><a href=#momentum-nesterov-adagrad-rmsprop>Momentum, Nesterov, AdaGrad, RMSProp</a><ul><li><a href=#momentum>Momentum</a></li><li><a href=#nesterov-momentum>Nesterov Momentum</a></li><li><a href=#adagrad>AdaGrad</a></li><li><a href=#rmsprop>RMSProp</a></li></ul></li><li><a href=#adam--adamwnadamadamp>Adam / AdamW／NAdam／AdamP</a><ul><li><a href=#adam>Adam</a></li><li><a href=#adamw>AdamW</a></li><li><a href=#nadam--adamp>NAdam / AdamP</a></li></ul></li><li><a href=#sgd-vs-adaptive收斂速度與泛化>SGD vs. Adaptive：收斂速度與泛化</a></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=batchmini-batchstochastic-gd>Batch／Mini-Batch／Stochastic GD</h2><ul><li><strong>Batch GD</strong>：每次用全部資料計算梯度，收斂平滑但慢，需大記憶體</li><li><strong>Mini-Batch GD</strong>：每次用部分資料，兼顧效率與穩定，主流選擇</li><li><strong>Stochastic GD (SGD)</strong>：每次只用一筆資料，更新頻繁但波動大</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.optim <span style=color:#66d9ef>as</span> optim
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> x, y <span style=color:#f92672>in</span> dataloader:  <span style=color:#75715e># Mini-Batch</span>
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> loss_fn(model(x), y)
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>step()
</span></span></code></pre></div><hr><h2 id=momentum-nesterov-adagrad-rmsprop>Momentum, Nesterov, AdaGrad, RMSProp</h2><h3 id=momentum>Momentum</h3><ul><li>累積過去梯度，幫助跳出局部極小</li><li>更新規則：$v_{t+1} = \beta v_t + (1-\beta)\nabla L$，$w_{t+1} = w_t - \eta v_{t+1}$</li></ul><h3 id=nesterov-momentum>Nesterov Momentum</h3><ul><li>先預測一步再計算梯度，收斂更快</li></ul><h3 id=adagrad>AdaGrad</h3><ul><li>為每個參數自適應調整學習率，適合稀疏資料</li></ul><h3 id=rmsprop>RMSProp</h3><ul><li>解決 AdaGrad 學習率過快衰減，適合非平穩目標</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, momentum<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>, nesterov<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>optimizer_adagrad <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adagrad(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>optimizer_rmsprop <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>RMSprop(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span></code></pre></div><hr><h2 id=adam--adamwnadamadamp>Adam / AdamW／NAdam／AdamP</h2><h3 id=adam>Adam</h3><ul><li>結合 Momentum 與 RMSProp，適應性強，主流選擇</li><li>$m_t$（一階動量）、$v_t$（二階動量），帶偏差校正</li></ul><h3 id=adamw>AdamW</h3><ul><li>將權重衰減與梯度分離，泛化更佳</li></ul><h3 id=nadam--adamp>NAdam / AdamP</h3><ul><li>NAdam：結合 Nesterov 動量</li><li>AdamP：針對 CNN 設計，提升泛化</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimizer_adam <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adam(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>)
</span></span><span style=display:flex><span>optimizer_adamw <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>AdamW(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>)
</span></span></code></pre></div><hr><h2 id=sgd-vs-adaptive收斂速度與泛化>SGD vs. Adaptive：收斂速度與泛化</h2><ul><li>Adaptive（如 Adam）收斂快，適合複雜/稀疏資料</li><li>SGD + Momentum 泛化能力常優於 Adam，適合大規模訓練</li><li>實務常先用 Adam，收斂後切換 SGD 微調</li></ul><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>SGD：大規模影像、語音、NLP</li><li>Adam/AdamW：NLP、稀疏特徵、預訓練模型</li><li>AdaGrad/RMSProp：稀疏資料、非平穩目標</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>Adam 泛化不佳，未調整學習率 schedule</li><li>Nesterov 需正確設置 momentum</li><li>Adaptive 優化器未設 weight decay，易過擬合</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>SGD vs Adam</td><td>收斂速度與泛化差異？</td></tr><tr><td>Momentum</td><td>如何幫助跳出局部極小？</td></tr><tr><td>AdamW</td><td>為何較 Adam 泛化好？</td></tr><tr><td>AdaGrad/RMSProp</td><td>適用場景與數學原理？</td></tr><tr><td>Nesterov</td><td>與 Momentum 差異？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>優化器選擇需根據任務與資料特性</li><li>Adaptive 優化器建議搭配學習率衰減與 weight decay</li><li>訓練後期可考慮切換 SGD 微調</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://pytorch.org/docs/stable/optim.html>PyTorch Optimizer 官方文件</a></li><li><a href=https://arxiv.org/abs/1412.6980>Adam 論文</a></li><li><a href=https://arxiv.org/abs/1711.05101>AdamW 論文</a></li><li><a href=https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>RMSProp 論文</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>SGD、Momentum、Nesterov、Adam、AdamW 更新規則？</li><li>Adam 為何收斂快但泛化差？</li><li>AdamW 與 Adam 的數學差異？</li><li>AdaGrad/RMSProp 適用場景？</li><li>Adaptive 優化器 weight decay 設置？</li><li>SGD 何時優於 Adam？</li><li>如何用 Python 切換優化器？</li><li>Nesterov Momentum 的數學推導？</li><li>AdamP 有何創新？</li><li>優化器選擇對訓練有何影響？</li></ol><hr><h2 id=結語>結語</h2><p>梯度下降與優化器是模型訓練的核心。熟悉 SGD、Momentum、Adam、AdamW 等優化器原理與實作，能讓你在訓練與面試中展現專業素養。下一章將進入學習率策略，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>gradient-descent</span>
<span class=tag>sgd</span>
<span class=tag>adam</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>