<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>機器學習核心概念暖身：監督/非監督、Bias-Variance、泛化誤差全解析 - Yu's Portfolio & Learning Hub</title><meta name=description content='機器學習的世界博大精深，但所有進階理論與實作都建立在核心概念之上。本章將帶你從監督/非監督/半監督/強化式學習的本質出發，深入理解 Bias-Variance 葛藤、泛化誤差分解，以及過擬合/欠擬合的診斷與調整流程。內容涵蓋理論、圖解、Python 實作、面試熱點與常見誤區，為後續章節打下堅實基礎。
監督、非監督、半監督、強化式學習差異 監督學習（Supervised Learning） 有標註資料（輸入+目標），學習輸入到目標的映射。 常見任務：分類、迴歸。 例：房價預測、圖片分類。 非監督學習（Unsupervised Learning） 無標註資料，探索資料內部結構。 常見任務：聚類、降維、密度估計。 例：客戶分群、主成分分析（PCA）。 半監督學習（Semi-supervised Learning） 標註資料稀少，結合大量未標註資料提升表現。 例：少量標註圖片+大量未標註圖片訓練分類器。 強化式學習（Reinforcement Learning） 透過與環境互動獲取回饋（獎勵/懲罰），學習最佳策略。 例：AlphaGo、機器人控制。 類型 標註需求 典型任務 代表演算法 監督學習 高 分類/迴歸 SVM, LR, RF, NN 非監督學習 無 聚類/降維 K-means, PCA, GMM 半監督學習 低 分類/迴歸 Pseudo-label, MixMatch 強化學習 無 控制/決策 Q-Learning, DQN, PG Bias-Variance 葛藤與泛化誤差分解 泛化誤差（Generalization Error） 模型在未見過資料上的預測誤差。 分解為三部分：偏差（Bias）、變異（Variance）、噪聲（Noise）。 $$ \text{Generalization Error} = \text{Bias}^2 + \text{Variance} + \text{Noise} $$
偏差（Bias） 模型假設與真實分布的差距。 偏差高 → 欠擬合（Underfitting）。 變異（Variance） 模型對訓練資料的敏感度。 變異高 → 過擬合（Overfitting）。 欠擬合與過擬合診斷 欠擬合：訓練/驗證誤差都高，模型太簡單。 過擬合：訓練誤差低、驗證誤差高，模型太複雜。 圖解 import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression np.random.seed(42) X = np.linspace(0, 1, 100)[:, None] y = np.sin(2 * np.pi * X).ravel() + np.random.randn(100) * 0.1 for degree in [1, 4, 15]: poly = PolynomialFeatures(degree) X_poly = poly.fit_transform(X) model = LinearRegression().fit(X_poly, y) y_pred = model.predict(X_poly) plt.plot(X, y_pred, label=f"degree={degree}") plt.scatter(X, y, s=10, color=&#39;black&#39;) plt.legend(); plt.title("Bias-Variance Trade-off"); plt.show() Underfitting/Overfitting 診斷與調整流程 診斷流程 觀察訓練/驗證誤差：兩者都高→欠擬合；訓練低驗證高→過擬合。 學習曲線：隨資料量變化誤差趨勢。 模型複雜度調整：增加/減少參數、特徵、正則化。 調整技巧 欠擬合：增加模型複雜度、特徵工程、減少正則化。 過擬合：加強正則化、減少模型複雜度、資料增強、早停（Early Stopping）。 Python 實作：學習曲線 from sklearn.model_selection import learning_curve from sklearn.tree import DecisionTreeRegressor train_sizes, train_scores, val_scores = learning_curve( DecisionTreeRegressor(), X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 5) ) plt.plot(train_sizes, np.mean(train_scores, axis=1), label="Train") plt.plot(train_sizes, np.mean(val_scores, axis=1), label="Validation") plt.xlabel("Training Size"); plt.ylabel("Score"); plt.legend() plt.title("Learning Curve"); plt.show() 實務應用與常見誤區 實務應用 模型選擇與調參時，必須同時考慮 Bias-Variance。 監督/非監督學習的選擇取決於資料標註情況與任務目標。 強化學習適合決策與控制問題，需設計合適的獎勵機制。 常見誤區 只看訓練誤差，不檢查泛化能力。 誤以為模型越複雜越好，忽略過擬合風險。 混淆監督與非監督學習的適用場景。 常見面試熱點整理 熱點主題 面試常問問題 監督/非監督差異 何時用哪種？有何代表演算法？ Bias-Variance 如何可視化？如何調整？ 泛化誤差 如何分解？如何降低？ Overfitting 有哪些診斷與解法？ 使用注意事項 訓練/驗證/測試集需嚴格分離，避免資料洩漏。 學習曲線是診斷模型表現的重要工具。 調參時建議結合交叉驗證與多指標評估。 延伸閱讀與資源 StatQuest: Bias and Variance Deep Learning Book: Generalization Scikit-learn: Learning Curves 結語 核心概念是機器學習進階學習的基石。熟悉監督/非監督/半監督/強化學習、Bias-Variance 葛藤與泛化誤差分解，能讓你在模型設計、調參與面試中展現專業素養。下一章將進入經典迴歸模型，敬請期待！
'><meta property="og:title" content="機器學習核心概念暖身：監督/非監督、Bias-Variance、泛化誤差全解析"><meta property="og:description" content='機器學習的世界博大精深，但所有進階理論與實作都建立在核心概念之上。本章將帶你從監督/非監督/半監督/強化式學習的本質出發，深入理解 Bias-Variance 葛藤、泛化誤差分解，以及過擬合/欠擬合的診斷與調整流程。內容涵蓋理論、圖解、Python 實作、面試熱點與常見誤區，為後續章節打下堅實基礎。
監督、非監督、半監督、強化式學習差異 監督學習（Supervised Learning） 有標註資料（輸入+目標），學習輸入到目標的映射。 常見任務：分類、迴歸。 例：房價預測、圖片分類。 非監督學習（Unsupervised Learning） 無標註資料，探索資料內部結構。 常見任務：聚類、降維、密度估計。 例：客戶分群、主成分分析（PCA）。 半監督學習（Semi-supervised Learning） 標註資料稀少，結合大量未標註資料提升表現。 例：少量標註圖片+大量未標註圖片訓練分類器。 強化式學習（Reinforcement Learning） 透過與環境互動獲取回饋（獎勵/懲罰），學習最佳策略。 例：AlphaGo、機器人控制。 類型 標註需求 典型任務 代表演算法 監督學習 高 分類/迴歸 SVM, LR, RF, NN 非監督學習 無 聚類/降維 K-means, PCA, GMM 半監督學習 低 分類/迴歸 Pseudo-label, MixMatch 強化學習 無 控制/決策 Q-Learning, DQN, PG Bias-Variance 葛藤與泛化誤差分解 泛化誤差（Generalization Error） 模型在未見過資料上的預測誤差。 分解為三部分：偏差（Bias）、變異（Variance）、噪聲（Noise）。 $$ \text{Generalization Error} = \text{Bias}^2 + \text{Variance} + \text{Noise} $$
偏差（Bias） 模型假設與真實分布的差距。 偏差高 → 欠擬合（Underfitting）。 變異（Variance） 模型對訓練資料的敏感度。 變異高 → 過擬合（Overfitting）。 欠擬合與過擬合診斷 欠擬合：訓練/驗證誤差都高，模型太簡單。 過擬合：訓練誤差低、驗證誤差高，模型太複雜。 圖解 import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression np.random.seed(42) X = np.linspace(0, 1, 100)[:, None] y = np.sin(2 * np.pi * X).ravel() + np.random.randn(100) * 0.1 for degree in [1, 4, 15]: poly = PolynomialFeatures(degree) X_poly = poly.fit_transform(X) model = LinearRegression().fit(X_poly, y) y_pred = model.predict(X_poly) plt.plot(X, y_pred, label=f"degree={degree}") plt.scatter(X, y, s=10, color=&#39;black&#39;) plt.legend(); plt.title("Bias-Variance Trade-off"); plt.show() Underfitting/Overfitting 診斷與調整流程 診斷流程 觀察訓練/驗證誤差：兩者都高→欠擬合；訓練低驗證高→過擬合。 學習曲線：隨資料量變化誤差趨勢。 模型複雜度調整：增加/減少參數、特徵、正則化。 調整技巧 欠擬合：增加模型複雜度、特徵工程、減少正則化。 過擬合：加強正則化、減少模型複雜度、資料增強、早停（Early Stopping）。 Python 實作：學習曲線 from sklearn.model_selection import learning_curve from sklearn.tree import DecisionTreeRegressor train_sizes, train_scores, val_scores = learning_curve( DecisionTreeRegressor(), X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 5) ) plt.plot(train_sizes, np.mean(train_scores, axis=1), label="Train") plt.plot(train_sizes, np.mean(val_scores, axis=1), label="Validation") plt.xlabel("Training Size"); plt.ylabel("Score"); plt.legend() plt.title("Learning Curve"); plt.show() 實務應用與常見誤區 實務應用 模型選擇與調參時，必須同時考慮 Bias-Variance。 監督/非監督學習的選擇取決於資料標註情況與任務目標。 強化學習適合決策與控制問題，需設計合適的獎勵機制。 常見誤區 只看訓練誤差，不檢查泛化能力。 誤以為模型越複雜越好，忽略過擬合風險。 混淆監督與非監督學習的適用場景。 常見面試熱點整理 熱點主題 面試常問問題 監督/非監督差異 何時用哪種？有何代表演算法？ Bias-Variance 如何可視化？如何調整？ 泛化誤差 如何分解？如何降低？ Overfitting 有哪些診斷與解法？ 使用注意事項 訓練/驗證/測試集需嚴格分離，避免資料洩漏。 學習曲線是診斷模型表現的重要工具。 調參時建議結合交叉驗證與多指標評估。 延伸閱讀與資源 StatQuest: Bias and Variance Deep Learning Book: Generalization Scikit-learn: Learning Curves 結語 核心概念是機器學習進階學習的基石。熟悉監督/非監督/半監督/強化學習、Bias-Variance 葛藤與泛化誤差分解，能讓你在模型設計、調參與面試中展現專業素養。下一章將進入經典迴歸模型，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/core-concepts/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/core-concepts/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>機器學習核心概念暖身：監督/非監督、Bias-Variance、泛化誤差全解析</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>機器學習核心概念暖身：監督/非監督、Bias-Variance、泛化誤差全解析</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-09-01</span></div></header><div class=article-body><p>機器學習的世界博大精深，但所有進階理論與實作都建立在核心概念之上。本章將帶你從監督/非監督/半監督/強化式學習的本質出發，深入理解 Bias-Variance 葛藤、泛化誤差分解，以及過擬合/欠擬合的診斷與調整流程。內容涵蓋理論、圖解、Python 實作、面試熱點與常見誤區，為後續章節打下堅實基礎。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#監督非監督半監督強化式學習差異>監督、非監督、半監督、強化式學習差異</a><ul><li><a href=#監督學習supervised-learning>監督學習（Supervised Learning）</a></li><li><a href=#非監督學習unsupervised-learning>非監督學習（Unsupervised Learning）</a></li><li><a href=#半監督學習semi-supervised-learning>半監督學習（Semi-supervised Learning）</a></li><li><a href=#強化式學習reinforcement-learning>強化式學習（Reinforcement Learning）</a></li></ul></li><li><a href=#bias-variance-葛藤與泛化誤差分解>Bias-Variance 葛藤與泛化誤差分解</a><ul><li><a href=#泛化誤差generalization-error>泛化誤差（Generalization Error）</a></li><li><a href=#偏差bias>偏差（Bias）</a></li><li><a href=#變異variance>變異（Variance）</a></li><li><a href=#欠擬合與過擬合診斷>欠擬合與過擬合診斷</a></li></ul></li><li><a href=#underfittingoverfitting-診斷與調整流程>Underfitting/Overfitting 診斷與調整流程</a><ul><li><a href=#診斷流程>診斷流程</a></li><li><a href=#調整技巧>調整技巧</a></li></ul></li><li><a href=#實務應用與常見誤區>實務應用與常見誤區</a><ul><li><a href=#實務應用>實務應用</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#常見面試熱點整理>常見面試熱點整理</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=監督非監督半監督強化式學習差異>監督、非監督、半監督、強化式學習差異</h2><h3 id=監督學習supervised-learning>監督學習（Supervised Learning）</h3><ul><li>有標註資料（輸入+目標），學習輸入到目標的映射。</li><li>常見任務：分類、迴歸。</li><li>例：房價預測、圖片分類。</li></ul><h3 id=非監督學習unsupervised-learning>非監督學習（Unsupervised Learning）</h3><ul><li>無標註資料，探索資料內部結構。</li><li>常見任務：聚類、降維、密度估計。</li><li>例：客戶分群、主成分分析（PCA）。</li></ul><h3 id=半監督學習semi-supervised-learning>半監督學習（Semi-supervised Learning）</h3><ul><li>標註資料稀少，結合大量未標註資料提升表現。</li><li>例：少量標註圖片+大量未標註圖片訓練分類器。</li></ul><h3 id=強化式學習reinforcement-learning>強化式學習（Reinforcement Learning）</h3><ul><li>透過與環境互動獲取回饋（獎勵/懲罰），學習最佳策略。</li><li>例：AlphaGo、機器人控制。</li></ul><table><thead><tr><th>類型</th><th>標註需求</th><th>典型任務</th><th>代表演算法</th></tr></thead><tbody><tr><td>監督學習</td><td>高</td><td>分類/迴歸</td><td>SVM, LR, RF, NN</td></tr><tr><td>非監督學習</td><td>無</td><td>聚類/降維</td><td>K-means, PCA, GMM</td></tr><tr><td>半監督學習</td><td>低</td><td>分類/迴歸</td><td>Pseudo-label, MixMatch</td></tr><tr><td>強化學習</td><td>無</td><td>控制/決策</td><td>Q-Learning, DQN, PG</td></tr></tbody></table><hr><h2 id=bias-variance-葛藤與泛化誤差分解>Bias-Variance 葛藤與泛化誤差分解</h2><h3 id=泛化誤差generalization-error>泛化誤差（Generalization Error）</h3><ul><li>模型在未見過資料上的預測誤差。</li><li>分解為三部分：偏差（Bias）、變異（Variance）、噪聲（Noise）。</li></ul><p>$$
\text{Generalization Error} = \text{Bias}^2 + \text{Variance} + \text{Noise}
$$</p><h3 id=偏差bias>偏差（Bias）</h3><ul><li>模型假設與真實分布的差距。</li><li>偏差高 → 欠擬合（Underfitting）。</li></ul><h3 id=變異variance>變異（Variance）</h3><ul><li>模型對訓練資料的敏感度。</li><li>變異高 → 過擬合（Overfitting）。</li></ul><h3 id=欠擬合與過擬合診斷>欠擬合與過擬合診斷</h3><ul><li>欠擬合：訓練/驗證誤差都高，模型太簡單。</li><li>過擬合：訓練誤差低、驗證誤差高，模型太複雜。</li></ul><h4 id=圖解>圖解</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> PolynomialFeatures
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LinearRegression
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>100</span>)[:, <span style=color:#66d9ef>None</span>]
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sin(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>pi <span style=color:#f92672>*</span> X)<span style=color:#f92672>.</span>ravel() <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>100</span>) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> degree <span style=color:#f92672>in</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>15</span>]:
</span></span><span style=display:flex><span>    poly <span style=color:#f92672>=</span> PolynomialFeatures(degree)
</span></span><span style=display:flex><span>    X_poly <span style=color:#f92672>=</span> poly<span style=color:#f92672>.</span>fit_transform(X)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> LinearRegression()<span style=color:#f92672>.</span>fit(X_poly, y)
</span></span><span style=display:flex><span>    y_pred <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(X_poly)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(X, y_pred, label<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;degree=</span><span style=color:#e6db74>{</span>degree<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(X, y, s<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;black&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend(); plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Bias-Variance Trade-off&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><hr><h2 id=underfittingoverfitting-診斷與調整流程>Underfitting/Overfitting 診斷與調整流程</h2><h3 id=診斷流程>診斷流程</h3><ol><li><strong>觀察訓練/驗證誤差</strong>：兩者都高→欠擬合；訓練低驗證高→過擬合。</li><li><strong>學習曲線</strong>：隨資料量變化誤差趨勢。</li><li><strong>模型複雜度調整</strong>：增加/減少參數、特徵、正則化。</li></ol><h3 id=調整技巧>調整技巧</h3><ul><li>欠擬合：增加模型複雜度、特徵工程、減少正則化。</li><li>過擬合：加強正則化、減少模型複雜度、資料增強、早停（Early Stopping）。</li></ul><h4 id=python-實作學習曲線>Python 實作：學習曲線</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> learning_curve
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.tree <span style=color:#f92672>import</span> DecisionTreeRegressor
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_sizes, train_scores, val_scores <span style=color:#f92672>=</span> learning_curve(
</span></span><span style=display:flex><span>    DecisionTreeRegressor(), X, y, cv<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, train_sizes<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(train_sizes, np<span style=color:#f92672>.</span>mean(train_scores, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Train&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(train_sizes, np<span style=color:#f92672>.</span>mean(val_scores, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Validation&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;Training Size&#34;</span>); plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;Score&#34;</span>); plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Learning Curve&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><hr><h2 id=實務應用與常見誤區>實務應用與常見誤區</h2><h3 id=實務應用>實務應用</h3><ul><li>模型選擇與調參時，必須同時考慮 Bias-Variance。</li><li>監督/非監督學習的選擇取決於資料標註情況與任務目標。</li><li>強化學習適合決策與控制問題，需設計合適的獎勵機制。</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>只看訓練誤差，不檢查泛化能力。</li><li>誤以為模型越複雜越好，忽略過擬合風險。</li><li>混淆監督與非監督學習的適用場景。</li></ul><hr><h2 id=常見面試熱點整理>常見面試熱點整理</h2><table><thead><tr><th>熱點主題</th><th>面試常問問題</th></tr></thead><tbody><tr><td>監督/非監督差異</td><td>何時用哪種？有何代表演算法？</td></tr><tr><td>Bias-Variance</td><td>如何可視化？如何調整？</td></tr><tr><td>泛化誤差</td><td>如何分解？如何降低？</td></tr><tr><td>Overfitting</td><td>有哪些診斷與解法？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>訓練/驗證/測試集需嚴格分離，避免資料洩漏。</li><li>學習曲線是診斷模型表現的重要工具。</li><li>調參時建議結合交叉驗證與多指標評估。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href="https://www.youtube.com/watch?v=EuBBz3bI-aA">StatQuest: Bias and Variance</a></li><li><a href=https://www.deeplearningbook.org/contents/gener.html>Deep Learning Book: Generalization</a></li><li><a href=https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html>Scikit-learn: Learning Curves</a></li></ul><hr><h2 id=結語>結語</h2><p>核心概念是機器學習進階學習的基石。熟悉監督/非監督/半監督/強化學習、Bias-Variance 葛藤與泛化誤差分解，能讓你在模型設計、調參與面試中展現專業素養。下一章將進入經典迴歸模型，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>Bias-Variance</span>
<span class=tag>Overfitting</span>
<span class=tag>Underfitting</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>