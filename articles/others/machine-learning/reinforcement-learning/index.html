<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>強化式學習速查：MDP、Q-Learning、DQN、Policy Gradient 與探索-利用平衡 - Yu's Portfolio & Learning Hub</title><meta name=description content="強化式學習（Reinforcement Learning, RL）是 AI 自主決策與控制的核心。從馬可夫決策過程（MDP）、Q-Learning、DQN，到 Policy Gradient、探索-利用平衡（ε-Greedy、UCB），這些理論與演算法是自動駕駛、遊戲 AI、機器人等領域的基石。本章將深入數學原理、直覺圖解、Python 實作、應用場景、面試熱點與常見誤區，幫助你快速掌握 RL 核心知識。
MDP（馬可夫決策過程） 定義 $S$：狀態空間，$A$：動作空間，$P$：轉移機率，$R$：獎勵函數，$\gamma$：折扣因子。 目標：學習策略 $\pi(a|s)$，最大化累積期望獎勵。 貝爾曼方程（Bellman Equation） 狀態價值函數 $V^\pi(s)$： $$ V^\pi(s) = \mathbb{E}\pi \left[ \sum{t=0}^\infty \gamma^t R(s_t, a_t) \mid s_0 = s \right] $$ 最優價值函數滿足： $$ V^(s) = \max_a \left[ R(s, a) + \gamma \sum_{s&rsquo;} P(s&rsquo;|s,a) V^(s&rsquo;) \right] $$ Policy / Value Function 策略（Policy）：決定在每個狀態下採取哪個動作。 價值函數（Value Function）：評估狀態或狀態-動作對的好壞。 狀態價值 $V(s)$、動作價值 $Q(s,a)$。 Q-Learning、DQN、Policy Gradient 對比 Q-Learning 無模型、離線學習，更新規則： $$ Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a&rsquo;} Q(s&rsquo;,a&rsquo;) - Q(s,a)] $$ 適合小型離散空間。 DQN（Deep Q-Network） 用深度神經網路近似 Q 函數，適合高維狀態空間。 經典應用：Atari 遊戲 AI。 import numpy as np Q = np.zeros((5, 2)) # 5 狀態, 2 動作 alpha, gamma = 0.1, 0.99 s, a, r, s_next = 0, 1, 1, 2 Q[s, a] += alpha * (r + gamma * np.max(Q[s_next]) - Q[s, a]) Policy Gradient 直接優化策略參數，最大化期望獎勵。 適合連續動作空間，常用於機器人控制、AlphaGo。 import torch import torch.nn as nn class PolicyNet(nn.Module): def __init__(self): super().__init__() self.fc = nn.Linear(4, 2) def forward(self, x): return torch.softmax(self.fc(x), dim=-1) 探索-利用平衡（Exploration-Exploitation） ε-Greedy 以機率 ε 隨機探索，其餘時間選擇最佳動作。 ε 可隨訓練逐步減小。 UCB（Upper Confidence Bound） 根據置信上界選擇動作，兼顧平均回報與不確定性。 常用於多臂賭徒問題（Multi-Armed Bandit）。 理論直覺、應用場景與常見誤區 應用場景 遊戲 AI（AlphaGo、Atari）、機器人控制、自動駕駛、推薦系統 連續決策、動態規劃、資源分配 常見誤區 忽略探索，導致陷入次優策略 Q-Learning 不適用於高維連續空間 DQN 訓練不穩定，需經驗回放與目標網路 Policy Gradient 易陷入高方差，需 baseline 技巧 面試熱點與經典問題 主題 常見問題 MDP 定義與貝爾曼方程？ Q-Learning 更新規則與收斂性？ DQN 如何穩定訓練？ Policy Gradient 優缺點與應用？ 探索-利用 ε-Greedy 與 UCB 差異？ 使用注意事項 強化學習需大量互動資料，訓練成本高 DQN 類方法需經驗回放（Replay Buffer）與目標網路 Policy Gradient 需 variance reduction 技巧（如 baseline、GAE） 延伸閱讀與資源 Deep RL Book OpenAI Spinning Up RL DQN 論文 Policy Gradient 論文 經典面試題與解法提示 MDP 的五大元素與貝爾曼方程推導？ Q-Learning 如何更新？何時收斂？ DQN 如何解決 Q-Learning 的限制？ Policy Gradient 的數學推導與應用場景？ 探索-利用平衡有哪些策略？ ε-Greedy 如何設計 ε 衰減？ UCB 的數學原理與應用？ 強化學習在推薦系統的應用？ DQN 訓練不穩定的原因與解法？ Policy Gradient 如何降低方差？ 結語 強化式學習是 AI 決策與控制的核心。熟悉 MDP、Q-Learning、DQN、Policy Gradient 與探索-利用平衡，能讓你在自動化決策、遊戲 AI、機器人等領域發揮專業實力。下一章將進入倫理、偏差與公平，敬請期待！
"><meta property="og:title" content="強化式學習速查：MDP、Q-Learning、DQN、Policy Gradient 與探索-利用平衡"><meta property="og:description" content="強化式學習（Reinforcement Learning, RL）是 AI 自主決策與控制的核心。從馬可夫決策過程（MDP）、Q-Learning、DQN，到 Policy Gradient、探索-利用平衡（ε-Greedy、UCB），這些理論與演算法是自動駕駛、遊戲 AI、機器人等領域的基石。本章將深入數學原理、直覺圖解、Python 實作、應用場景、面試熱點與常見誤區，幫助你快速掌握 RL 核心知識。
MDP（馬可夫決策過程） 定義 $S$：狀態空間，$A$：動作空間，$P$：轉移機率，$R$：獎勵函數，$\gamma$：折扣因子。 目標：學習策略 $\pi(a|s)$，最大化累積期望獎勵。 貝爾曼方程（Bellman Equation） 狀態價值函數 $V^\pi(s)$： $$ V^\pi(s) = \mathbb{E}\pi \left[ \sum{t=0}^\infty \gamma^t R(s_t, a_t) \mid s_0 = s \right] $$ 最優價值函數滿足： $$ V^(s) = \max_a \left[ R(s, a) + \gamma \sum_{s&rsquo;} P(s&rsquo;|s,a) V^(s&rsquo;) \right] $$ Policy / Value Function 策略（Policy）：決定在每個狀態下採取哪個動作。 價值函數（Value Function）：評估狀態或狀態-動作對的好壞。 狀態價值 $V(s)$、動作價值 $Q(s,a)$。 Q-Learning、DQN、Policy Gradient 對比 Q-Learning 無模型、離線學習，更新規則： $$ Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a&rsquo;} Q(s&rsquo;,a&rsquo;) - Q(s,a)] $$ 適合小型離散空間。 DQN（Deep Q-Network） 用深度神經網路近似 Q 函數，適合高維狀態空間。 經典應用：Atari 遊戲 AI。 import numpy as np Q = np.zeros((5, 2)) # 5 狀態, 2 動作 alpha, gamma = 0.1, 0.99 s, a, r, s_next = 0, 1, 1, 2 Q[s, a] += alpha * (r + gamma * np.max(Q[s_next]) - Q[s, a]) Policy Gradient 直接優化策略參數，最大化期望獎勵。 適合連續動作空間，常用於機器人控制、AlphaGo。 import torch import torch.nn as nn class PolicyNet(nn.Module): def __init__(self): super().__init__() self.fc = nn.Linear(4, 2) def forward(self, x): return torch.softmax(self.fc(x), dim=-1) 探索-利用平衡（Exploration-Exploitation） ε-Greedy 以機率 ε 隨機探索，其餘時間選擇最佳動作。 ε 可隨訓練逐步減小。 UCB（Upper Confidence Bound） 根據置信上界選擇動作，兼顧平均回報與不確定性。 常用於多臂賭徒問題（Multi-Armed Bandit）。 理論直覺、應用場景與常見誤區 應用場景 遊戲 AI（AlphaGo、Atari）、機器人控制、自動駕駛、推薦系統 連續決策、動態規劃、資源分配 常見誤區 忽略探索，導致陷入次優策略 Q-Learning 不適用於高維連續空間 DQN 訓練不穩定，需經驗回放與目標網路 Policy Gradient 易陷入高方差，需 baseline 技巧 面試熱點與經典問題 主題 常見問題 MDP 定義與貝爾曼方程？ Q-Learning 更新規則與收斂性？ DQN 如何穩定訓練？ Policy Gradient 優缺點與應用？ 探索-利用 ε-Greedy 與 UCB 差異？ 使用注意事項 強化學習需大量互動資料，訓練成本高 DQN 類方法需經驗回放（Replay Buffer）與目標網路 Policy Gradient 需 variance reduction 技巧（如 baseline、GAE） 延伸閱讀與資源 Deep RL Book OpenAI Spinning Up RL DQN 論文 Policy Gradient 論文 經典面試題與解法提示 MDP 的五大元素與貝爾曼方程推導？ Q-Learning 如何更新？何時收斂？ DQN 如何解決 Q-Learning 的限制？ Policy Gradient 的數學推導與應用場景？ 探索-利用平衡有哪些策略？ ε-Greedy 如何設計 ε 衰減？ UCB 的數學原理與應用？ 強化學習在推薦系統的應用？ DQN 訓練不穩定的原因與解法？ Policy Gradient 如何降低方差？ 結語 強化式學習是 AI 決策與控制的核心。熟悉 MDP、Q-Learning、DQN、Policy Gradient 與探索-利用平衡，能讓你在自動化決策、遊戲 AI、機器人等領域發揮專業實力。下一章將進入倫理、偏差與公平，敬請期待！
"><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/reinforcement-learning/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/reinforcement-learning/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>強化式學習速查：MDP、Q-Learning、DQN、Policy Gradient 與探索-利用平衡</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>強化式學習速查：MDP、Q-Learning、DQN、Policy Gradient 與探索-利用平衡</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-08-26</span></div></header><div class=article-body><p>強化式學習（Reinforcement Learning, RL）是 AI 自主決策與控制的核心。從馬可夫決策過程（MDP）、Q-Learning、DQN，到 Policy Gradient、探索-利用平衡（ε-Greedy、UCB），這些理論與演算法是自動駕駛、遊戲 AI、機器人等領域的基石。本章將深入數學原理、直覺圖解、Python 實作、應用場景、面試熱點與常見誤區，幫助你快速掌握 RL 核心知識。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#mdp馬可夫決策過程>MDP（馬可夫決策過程）</a><ul><li><a href=#定義>定義</a></li><li><a href=#貝爾曼方程bellman-equation>貝爾曼方程（Bellman Equation）</a></li></ul></li><li><a href=#policy--value-function>Policy / Value Function</a></li><li><a href=#q-learningdqnpolicy-gradient-對比>Q-Learning、DQN、Policy Gradient 對比</a><ul><li><a href=#q-learning>Q-Learning</a></li><li><a href=#dqndeep-q-network>DQN（Deep Q-Network）</a></li><li><a href=#policy-gradient>Policy Gradient</a></li></ul></li><li><a href=#探索-利用平衡exploration-exploitation>探索-利用平衡（Exploration-Exploitation）</a><ul><li><a href=#ε-greedy>ε-Greedy</a></li><li><a href=#ucbupper-confidence-bound>UCB（Upper Confidence Bound）</a></li></ul></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=mdp馬可夫決策過程>MDP（馬可夫決策過程）</h2><h3 id=定義>定義</h3><ul><li>$S$：狀態空間，$A$：動作空間，$P$：轉移機率，$R$：獎勵函數，$\gamma$：折扣因子。</li><li>目標：學習策略 $\pi(a|s)$，最大化累積期望獎勵。</li></ul><h3 id=貝爾曼方程bellman-equation>貝爾曼方程（Bellman Equation）</h3><ul><li>狀態價值函數 $V^\pi(s)$：
$$
V^\pi(s) = \mathbb{E}<em>\pi \left[ \sum</em>{t=0}^\infty \gamma^t R(s_t, a_t) \mid s_0 = s \right]
$$</li><li>最優價值函數滿足：
$$
V^<em>(s) = \max_a \left[ R(s, a) + \gamma \sum_{s&rsquo;} P(s&rsquo;|s,a) V^</em>(s&rsquo;) \right]
$$</li></ul><hr><h2 id=policy--value-function>Policy / Value Function</h2><ul><li><strong>策略（Policy）</strong>：決定在每個狀態下採取哪個動作。</li><li><strong>價值函數（Value Function）</strong>：評估狀態或狀態-動作對的好壞。<ul><li>狀態價值 $V(s)$、動作價值 $Q(s,a)$。</li></ul></li></ul><hr><h2 id=q-learningdqnpolicy-gradient-對比>Q-Learning、DQN、Policy Gradient 對比</h2><h3 id=q-learning>Q-Learning</h3><ul><li>無模型、離線學習，更新規則：
$$
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a&rsquo;} Q(s&rsquo;,a&rsquo;) - Q(s,a)]
$$</li><li>適合小型離散空間。</li></ul><h3 id=dqndeep-q-network>DQN（Deep Q-Network）</h3><ul><li>用深度神經網路近似 Q 函數，適合高維狀態空間。</li><li>經典應用：Atari 遊戲 AI。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Q <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>2</span>))  <span style=color:#75715e># 5 狀態, 2 動作</span>
</span></span><span style=display:flex><span>alpha, gamma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>0.99</span>
</span></span><span style=display:flex><span>s, a, r, s_next <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>Q[s, a] <span style=color:#f92672>+=</span> alpha <span style=color:#f92672>*</span> (r <span style=color:#f92672>+</span> gamma <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>max(Q[s_next]) <span style=color:#f92672>-</span> Q[s, a])
</span></span></code></pre></div><h3 id=policy-gradient>Policy Gradient</h3><ul><li>直接優化策略參數，最大化期望獎勵。</li><li>適合連續動作空間，常用於機器人控制、AlphaGo。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PolicyNet</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>softmax(self<span style=color:#f92672>.</span>fc(x), dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><hr><h2 id=探索-利用平衡exploration-exploitation>探索-利用平衡（Exploration-Exploitation）</h2><h3 id=ε-greedy>ε-Greedy</h3><ul><li>以機率 ε 隨機探索，其餘時間選擇最佳動作。</li><li>ε 可隨訓練逐步減小。</li></ul><h3 id=ucbupper-confidence-bound>UCB（Upper Confidence Bound）</h3><ul><li>根據置信上界選擇動作，兼顧平均回報與不確定性。</li><li>常用於多臂賭徒問題（Multi-Armed Bandit）。</li></ul><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>遊戲 AI（AlphaGo、Atari）、機器人控制、自動駕駛、推薦系統</li><li>連續決策、動態規劃、資源分配</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>忽略探索，導致陷入次優策略</li><li>Q-Learning 不適用於高維連續空間</li><li>DQN 訓練不穩定，需經驗回放與目標網路</li><li>Policy Gradient 易陷入高方差，需 baseline 技巧</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>MDP</td><td>定義與貝爾曼方程？</td></tr><tr><td>Q-Learning</td><td>更新規則與收斂性？</td></tr><tr><td>DQN</td><td>如何穩定訓練？</td></tr><tr><td>Policy Gradient</td><td>優缺點與應用？</td></tr><tr><td>探索-利用</td><td>ε-Greedy 與 UCB 差異？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>強化學習需大量互動資料，訓練成本高</li><li>DQN 類方法需經驗回放（Replay Buffer）與目標網路</li><li>Policy Gradient 需 variance reduction 技巧（如 baseline、GAE）</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://www.deepreinforcementlearningbook.org/>Deep RL Book</a></li><li><a href=https://spinningup.openai.com/>OpenAI Spinning Up RL</a></li><li><a href=https://www.nature.com/articles/nature14236>DQN 論文</a></li><li><a href=https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html>Policy Gradient 論文</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>MDP 的五大元素與貝爾曼方程推導？</li><li>Q-Learning 如何更新？何時收斂？</li><li>DQN 如何解決 Q-Learning 的限制？</li><li>Policy Gradient 的數學推導與應用場景？</li><li>探索-利用平衡有哪些策略？</li><li>ε-Greedy 如何設計 ε 衰減？</li><li>UCB 的數學原理與應用？</li><li>強化學習在推薦系統的應用？</li><li>DQN 訓練不穩定的原因與解法？</li><li>Policy Gradient 如何降低方差？</li></ol><hr><h2 id=結語>結語</h2><p>強化式學習是 AI 決策與控制的核心。熟悉 MDP、Q-Learning、DQN、Policy Gradient 與探索-利用平衡，能讓你在自動化決策、遊戲 AI、機器人等領域發揮專業實力。下一章將進入倫理、偏差與公平，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>MDP</span>
<span class=tag>Q-Learning</span>
<span class=tag>DQN</span>
<span class=tag>Policy Gradient</span>
<span class=tag>ε-Greedy</span>
<span class=tag>UCB</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>