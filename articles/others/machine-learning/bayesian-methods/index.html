<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>貝式方法與機率視角全攻略：生成/判別、貝式迴歸、變分推論與 MC Dropout - Yu's Portfolio & Learning Hub</title><meta name=description content='貝式方法與機率視角是現代機器學習理解不確定性、提升泛化能力的核心。從生成模型與判別模型的本質，到貝式線性迴歸、Gaussian Process Regression、變分推論與 Monte Carlo Dropout，這些理論與實作是面試與研究的熱門話題。本章將深入數學推導、直覺圖解、Python 實作、應用場景、面試熱點與常見誤區，幫助你全面掌握貝式方法。
Generative vs. Discriminative Models 生成模型（Generative Model） 學習 $P(X, Y)$ 或 $P(X)$，可生成新資料、進行密度估計。 例：Naive Bayes、GMM、GAN、VAE。 判別模型（Discriminative Model） 學習 $P(Y|X)$，直接預測標籤。 例：Logistic Regression、SVM、Random Forest。 類型 學習目標 代表模型 優缺點 生成模型 $P(X, Y)$ Naive Bayes, GMM, GAN, VAE 可生成資料、處理缺失，訓練慢 判別模型 $P(Y X)$ LR, SVM, RF 貝式線性迴歸（Bayesian Linear Regression） 理論基礎 將參數視為隨機變數，給定先驗分布，觀察資料後更新為後驗分布。 可直接量化預測不確定性。 數學推導 先驗：$\beta \sim \mathcal{N}(0, \lambda^{-1}I)$ 似然：$y|X, \beta \sim \mathcal{N}(X\beta, \sigma^2I)$ 後驗：$\beta|X, y \sim \mathcal{N}(\mu, \Sigma)$，可解析計算。 Python 實作 import numpy as np X = np.linspace(0, 1, 20)[:, None] y = 2 * X.ravel() + np.random.randn(20) * 0.2 lambda_ = 1.0 sigma2 = 0.04 Sigma_inv = lambda_ * np.eye(1) + (X.T @ X) / sigma2 Sigma = np.linalg.inv(Sigma_inv) mu = Sigma @ (X.T @ y) / sigma2 print("後驗均值:", mu) Gaussian Process Regression（高斯過程回歸） 非參數貝式模型，直接對函數分布建模。 可量化預測均值與不確定性，適合小資料、函數擬合。 理論直覺 假設任意有限點的函數值服從多元高斯分布。 透過核函數（Kernel）控制平滑度與相關性。 Python 實作 from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF gp = GaussianProcessRegressor(kernel=RBF(length_scale=1.0)) gp.fit(X, y) y_pred, y_std = gp.predict(X, return_std=True) print("預測均值:", y_pred[:5]) print("預測標準差:", y_std[:5]) Variational Inference（變分推論） 用簡單分布近似複雜後驗分布，最大化 Evidence Lower Bound (ELBO)。 常用於 VAE、貝式深度學習。 理論推導 將後驗 $P(\theta|D)$ 近似為 $q(\theta)$，最小化 $KL(q||P)$。 透過梯度下降優化 ELBO。 Monte Carlo Dropout 在推論時也啟用 Dropout，多次前向傳播取得預測分布。 可近似貝式不確定性，常用於深度學習模型。 import torch import torch.nn as nn class MCDropoutModel(nn.Module): def __init__(self): super().__init__() self.fc = nn.Linear(10, 1) self.drop = nn.Dropout(p=0.5) def forward(self, x): return self.fc(self.drop(x)) model = MCDropoutModel() model.train() # 保持 Dropout 啟用 preds = [model(torch.randn(1, 10)).item() for _ in range(100)] print("MC Dropout 預測均值:", np.mean(preds), "標準差:", np.std(preds)) 理論直覺、應用場景與常見誤區 應用場景 不確定性量化：醫療、金融、風險控制 小樣本學習：Gaussian Process、貝式迴歸 生成模型：VAE、GAN 深度學習不確定性：MC Dropout 常見誤區 混淆生成/判別模型的學習目標 貝式方法計算量大，實務常需近似推論 MC Dropout 需在推論時保持訓練模式 Gaussian Process 不適合大規模資料 面試熱點與經典問題 主題 常見問題 生成 vs 判別 差異與適用場景？ 貝式迴歸 先驗/後驗推導？ Gaussian Process 優缺點與應用？ 變分推論 為何需近似？ELBO 是什麼？ MC Dropout 如何量化不確定性？ 使用注意事項 貝式方法適合需量化不確定性的任務，但計算成本高。 變分推論與 MC Dropout 需多次前向傳播，注意效能。 Gaussian Process 適合小資料、函數擬合，需選好核函數。 延伸閱讀與資源 Deep Learning Book: Bayesian Methods Pyro: Probabilistic Programming Scikit-learn Gaussian Process MC Dropout 論文 經典面試題與解法提示 生成模型與判別模型的差異？ 貝式線性迴歸的數學推導？ Gaussian Process 如何量化不確定性？ 變分推論的核心思想與應用？ MC Dropout 如何近似貝式不確定性？ 生成模型有哪些應用？ 何時選用貝式方法？ Gaussian Process 的核函數如何選擇？ 變分推論與 MCMC 差異？ 如何用 Python 實作 MC Dropout？ 結語 貝式方法與機率視角是 ML 理論與實務的高階武器。熟悉生成/判別模型、貝式迴歸、Gaussian Process、變分推論與 MC Dropout，能讓你在不確定性建模、研究與面試中展現專業深度。下一章將進入強化式學習速查，敬請期待！
'><meta property="og:title" content="貝式方法與機率視角全攻略：生成/判別、貝式迴歸、變分推論與 MC Dropout"><meta property="og:description" content='貝式方法與機率視角是現代機器學習理解不確定性、提升泛化能力的核心。從生成模型與判別模型的本質，到貝式線性迴歸、Gaussian Process Regression、變分推論與 Monte Carlo Dropout，這些理論與實作是面試與研究的熱門話題。本章將深入數學推導、直覺圖解、Python 實作、應用場景、面試熱點與常見誤區，幫助你全面掌握貝式方法。
Generative vs. Discriminative Models 生成模型（Generative Model） 學習 $P(X, Y)$ 或 $P(X)$，可生成新資料、進行密度估計。 例：Naive Bayes、GMM、GAN、VAE。 判別模型（Discriminative Model） 學習 $P(Y|X)$，直接預測標籤。 例：Logistic Regression、SVM、Random Forest。 類型 學習目標 代表模型 優缺點 生成模型 $P(X, Y)$ Naive Bayes, GMM, GAN, VAE 可生成資料、處理缺失，訓練慢 判別模型 $P(Y X)$ LR, SVM, RF 貝式線性迴歸（Bayesian Linear Regression） 理論基礎 將參數視為隨機變數，給定先驗分布，觀察資料後更新為後驗分布。 可直接量化預測不確定性。 數學推導 先驗：$\beta \sim \mathcal{N}(0, \lambda^{-1}I)$ 似然：$y|X, \beta \sim \mathcal{N}(X\beta, \sigma^2I)$ 後驗：$\beta|X, y \sim \mathcal{N}(\mu, \Sigma)$，可解析計算。 Python 實作 import numpy as np X = np.linspace(0, 1, 20)[:, None] y = 2 * X.ravel() + np.random.randn(20) * 0.2 lambda_ = 1.0 sigma2 = 0.04 Sigma_inv = lambda_ * np.eye(1) + (X.T @ X) / sigma2 Sigma = np.linalg.inv(Sigma_inv) mu = Sigma @ (X.T @ y) / sigma2 print("後驗均值:", mu) Gaussian Process Regression（高斯過程回歸） 非參數貝式模型，直接對函數分布建模。 可量化預測均值與不確定性，適合小資料、函數擬合。 理論直覺 假設任意有限點的函數值服從多元高斯分布。 透過核函數（Kernel）控制平滑度與相關性。 Python 實作 from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF gp = GaussianProcessRegressor(kernel=RBF(length_scale=1.0)) gp.fit(X, y) y_pred, y_std = gp.predict(X, return_std=True) print("預測均值:", y_pred[:5]) print("預測標準差:", y_std[:5]) Variational Inference（變分推論） 用簡單分布近似複雜後驗分布，最大化 Evidence Lower Bound (ELBO)。 常用於 VAE、貝式深度學習。 理論推導 將後驗 $P(\theta|D)$ 近似為 $q(\theta)$，最小化 $KL(q||P)$。 透過梯度下降優化 ELBO。 Monte Carlo Dropout 在推論時也啟用 Dropout，多次前向傳播取得預測分布。 可近似貝式不確定性，常用於深度學習模型。 import torch import torch.nn as nn class MCDropoutModel(nn.Module): def __init__(self): super().__init__() self.fc = nn.Linear(10, 1) self.drop = nn.Dropout(p=0.5) def forward(self, x): return self.fc(self.drop(x)) model = MCDropoutModel() model.train() # 保持 Dropout 啟用 preds = [model(torch.randn(1, 10)).item() for _ in range(100)] print("MC Dropout 預測均值:", np.mean(preds), "標準差:", np.std(preds)) 理論直覺、應用場景與常見誤區 應用場景 不確定性量化：醫療、金融、風險控制 小樣本學習：Gaussian Process、貝式迴歸 生成模型：VAE、GAN 深度學習不確定性：MC Dropout 常見誤區 混淆生成/判別模型的學習目標 貝式方法計算量大，實務常需近似推論 MC Dropout 需在推論時保持訓練模式 Gaussian Process 不適合大規模資料 面試熱點與經典問題 主題 常見問題 生成 vs 判別 差異與適用場景？ 貝式迴歸 先驗/後驗推導？ Gaussian Process 優缺點與應用？ 變分推論 為何需近似？ELBO 是什麼？ MC Dropout 如何量化不確定性？ 使用注意事項 貝式方法適合需量化不確定性的任務，但計算成本高。 變分推論與 MC Dropout 需多次前向傳播，注意效能。 Gaussian Process 適合小資料、函數擬合，需選好核函數。 延伸閱讀與資源 Deep Learning Book: Bayesian Methods Pyro: Probabilistic Programming Scikit-learn Gaussian Process MC Dropout 論文 經典面試題與解法提示 生成模型與判別模型的差異？ 貝式線性迴歸的數學推導？ Gaussian Process 如何量化不確定性？ 變分推論的核心思想與應用？ MC Dropout 如何近似貝式不確定性？ 生成模型有哪些應用？ 何時選用貝式方法？ Gaussian Process 的核函數如何選擇？ 變分推論與 MCMC 差異？ 如何用 Python 實作 MC Dropout？ 結語 貝式方法與機率視角是 ML 理論與實務的高階武器。熟悉生成/判別模型、貝式迴歸、Gaussian Process、變分推論與 MC Dropout，能讓你在不確定性建模、研究與面試中展現專業深度。下一章將進入強化式學習速查，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/bayesian-methods/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/bayesian-methods/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>貝式方法與機率視角全攻略：生成/判別、貝式迴歸、變分推論與 MC Dropout</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>貝式方法與機率視角全攻略：生成/判別、貝式迴歸、變分推論與 MC Dropout</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-12-10</span></div></header><div class=article-body><p>貝式方法與機率視角是現代機器學習理解不確定性、提升泛化能力的核心。從生成模型與判別模型的本質，到貝式線性迴歸、Gaussian Process Regression、變分推論與 Monte Carlo Dropout，這些理論與實作是面試與研究的熱門話題。本章將深入數學推導、直覺圖解、Python 實作、應用場景、面試熱點與常見誤區，幫助你全面掌握貝式方法。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#generative-vs-discriminative-models>Generative vs. Discriminative Models</a><ul><li><a href=#生成模型generative-model>生成模型（Generative Model）</a></li><li><a href=#判別模型discriminative-model>判別模型（Discriminative Model）</a></li></ul></li><li><a href=#貝式線性迴歸bayesian-linear-regression>貝式線性迴歸（Bayesian Linear Regression）</a><ul><li><a href=#理論基礎>理論基礎</a></li><li><a href=#數學推導>數學推導</a></li><li><a href=#python-實作>Python 實作</a></li></ul></li><li><a href=#gaussian-process-regression高斯過程回歸>Gaussian Process Regression（高斯過程回歸）</a><ul><li><a href=#理論直覺>理論直覺</a></li><li><a href=#python-實作-1>Python 實作</a></li></ul></li><li><a href=#variational-inference變分推論>Variational Inference（變分推論）</a><ul><li><a href=#理論推導>理論推導</a></li></ul></li><li><a href=#monte-carlo-dropout>Monte Carlo Dropout</a></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=generative-vs-discriminative-models>Generative vs. Discriminative Models</h2><h3 id=生成模型generative-model>生成模型（Generative Model）</h3><ul><li>學習 $P(X, Y)$ 或 $P(X)$，可生成新資料、進行密度估計。</li><li>例：Naive Bayes、GMM、GAN、VAE。</li></ul><h3 id=判別模型discriminative-model>判別模型（Discriminative Model）</h3><ul><li>學習 $P(Y|X)$，直接預測標籤。</li><li>例：Logistic Regression、SVM、Random Forest。</li></ul><table><thead><tr><th>類型</th><th>學習目標</th><th>代表模型</th><th>優缺點</th></tr></thead><tbody><tr><td>生成模型</td><td>$P(X, Y)$</td><td>Naive Bayes, GMM, GAN, VAE</td><td>可生成資料、處理缺失，訓練慢</td></tr><tr><td>判別模型</td><td>$P(Y</td><td>X)$</td><td>LR, SVM, RF</td></tr></tbody></table><hr><h2 id=貝式線性迴歸bayesian-linear-regression>貝式線性迴歸（Bayesian Linear Regression）</h2><h3 id=理論基礎>理論基礎</h3><ul><li>將參數視為隨機變數，給定先驗分布，觀察資料後更新為後驗分布。</li><li>可直接量化預測不確定性。</li></ul><h3 id=數學推導>數學推導</h3><ul><li>先驗：$\beta \sim \mathcal{N}(0, \lambda^{-1}I)$</li><li>似然：$y|X, \beta \sim \mathcal{N}(X\beta, \sigma^2I)$</li><li>後驗：$\beta|X, y \sim \mathcal{N}(\mu, \Sigma)$，可解析計算。</li></ul><h3 id=python-實作>Python 實作</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>20</span>)[:, <span style=color:#66d9ef>None</span>]
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> X<span style=color:#f92672>.</span>ravel() <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>20</span>) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.2</span>
</span></span><span style=display:flex><span>lambda_ <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>sigma2 <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.04</span>
</span></span><span style=display:flex><span>Sigma_inv <span style=color:#f92672>=</span> lambda_ <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>eye(<span style=color:#ae81ff>1</span>) <span style=color:#f92672>+</span> (X<span style=color:#f92672>.</span>T <span style=color:#f92672>@</span> X) <span style=color:#f92672>/</span> sigma2
</span></span><span style=display:flex><span>Sigma <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(Sigma_inv)
</span></span><span style=display:flex><span>mu <span style=color:#f92672>=</span> Sigma <span style=color:#f92672>@</span> (X<span style=color:#f92672>.</span>T <span style=color:#f92672>@</span> y) <span style=color:#f92672>/</span> sigma2
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;後驗均值:&#34;</span>, mu)
</span></span></code></pre></div><hr><h2 id=gaussian-process-regression高斯過程回歸>Gaussian Process Regression（高斯過程回歸）</h2><ul><li>非參數貝式模型，直接對函數分布建模。</li><li>可量化預測均值與不確定性，適合小資料、函數擬合。</li></ul><h3 id=理論直覺>理論直覺</h3><ul><li>假設任意有限點的函數值服從多元高斯分布。</li><li>透過核函數（Kernel）控制平滑度與相關性。</li></ul><h3 id=python-實作-1>Python 實作</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.gaussian_process <span style=color:#f92672>import</span> GaussianProcessRegressor
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.gaussian_process.kernels <span style=color:#f92672>import</span> RBF
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>gp <span style=color:#f92672>=</span> GaussianProcessRegressor(kernel<span style=color:#f92672>=</span>RBF(length_scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>))
</span></span><span style=display:flex><span>gp<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>y_pred, y_std <span style=color:#f92672>=</span> gp<span style=color:#f92672>.</span>predict(X, return_std<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;預測均值:&#34;</span>, y_pred[:<span style=color:#ae81ff>5</span>])
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;預測標準差:&#34;</span>, y_std[:<span style=color:#ae81ff>5</span>])
</span></span></code></pre></div><hr><h2 id=variational-inference變分推論>Variational Inference（變分推論）</h2><ul><li>用簡單分布近似複雜後驗分布，最大化 Evidence Lower Bound (ELBO)。</li><li>常用於 VAE、貝式深度學習。</li></ul><h3 id=理論推導>理論推導</h3><ul><li>將後驗 $P(\theta|D)$ 近似為 $q(\theta)$，最小化 $KL(q||P)$。</li><li>透過梯度下降優化 ELBO。</li></ul><hr><h2 id=monte-carlo-dropout>Monte Carlo Dropout</h2><ul><li>在推論時也啟用 Dropout，多次前向傳播取得預測分布。</li><li>可近似貝式不確定性，常用於深度學習模型。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MCDropoutModel</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>drop <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>fc(self<span style=color:#f92672>.</span>drop(x))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> MCDropoutModel()
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>train()  <span style=color:#75715e># 保持 Dropout 啟用</span>
</span></span><span style=display:flex><span>preds <span style=color:#f92672>=</span> [model(torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>10</span>))<span style=color:#f92672>.</span>item() <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>100</span>)]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;MC Dropout 預測均值:&#34;</span>, np<span style=color:#f92672>.</span>mean(preds), <span style=color:#e6db74>&#34;標準差:&#34;</span>, np<span style=color:#f92672>.</span>std(preds))
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>不確定性量化：醫療、金融、風險控制</li><li>小樣本學習：Gaussian Process、貝式迴歸</li><li>生成模型：VAE、GAN</li><li>深度學習不確定性：MC Dropout</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>混淆生成/判別模型的學習目標</li><li>貝式方法計算量大，實務常需近似推論</li><li>MC Dropout 需在推論時保持訓練模式</li><li>Gaussian Process 不適合大規模資料</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>生成 vs 判別</td><td>差異與適用場景？</td></tr><tr><td>貝式迴歸</td><td>先驗/後驗推導？</td></tr><tr><td>Gaussian Process</td><td>優缺點與應用？</td></tr><tr><td>變分推論</td><td>為何需近似？ELBO 是什麼？</td></tr><tr><td>MC Dropout</td><td>如何量化不確定性？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>貝式方法適合需量化不確定性的任務，但計算成本高。</li><li>變分推論與 MC Dropout 需多次前向傳播，注意效能。</li><li>Gaussian Process 適合小資料、函數擬合，需選好核函數。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://www.deeplearningbook.org/contents/probabilistic.html>Deep Learning Book: Bayesian Methods</a></li><li><a href=https://pyro.ai/>Pyro: Probabilistic Programming</a></li><li><a href=https://scikit-learn.org/stable/modules/gaussian_process.html>Scikit-learn Gaussian Process</a></li><li><a href=https://arxiv.org/abs/1506.02142>MC Dropout 論文</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>生成模型與判別模型的差異？</li><li>貝式線性迴歸的數學推導？</li><li>Gaussian Process 如何量化不確定性？</li><li>變分推論的核心思想與應用？</li><li>MC Dropout 如何近似貝式不確定性？</li><li>生成模型有哪些應用？</li><li>何時選用貝式方法？</li><li>Gaussian Process 的核函數如何選擇？</li><li>變分推論與 MCMC 差異？</li><li>如何用 Python 實作 MC Dropout？</li></ol><hr><h2 id=結語>結語</h2><p>貝式方法與機率視角是 ML 理論與實務的高階武器。熟悉生成/判別模型、貝式迴歸、Gaussian Process、變分推論與 MC Dropout，能讓你在不確定性建模、研究與面試中展現專業深度。下一章將進入強化式學習速查，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>Gaussian Process</span>
<span class=tag>Variational Inference</span>
<span class=tag>Monte Carlo Dropout</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>