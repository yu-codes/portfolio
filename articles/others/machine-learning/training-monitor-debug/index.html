<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>訓練監控與 Debug：TensorBoard、Loss 爆炸/消失、Learning Curve 判讀與單元測試 - Yu's Portfolio & Learning Hub</title><meta name=description content="模型訓練過程中，監控與 Debug 是確保收斂、提升泛化與快速定位問題的關鍵。本章將深入 TensorBoard/Weights & Biases 指標追蹤、Loss 爆炸/消失原因排查、Learning Curve 與 Validation Gap 判讀、Forward/Backward 單元測試，結合理論、實作、面試熱點與常見誤區，幫助你打造穩健的訓練流程。
TensorBoard / Weights & Biases 指標追蹤 TensorBoard：可視化 loss、accuracy、learning rate、參數分布等 Weights & Biases（wandb）：雲端追蹤、團隊協作、超參數搜尋 from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter() for epoch in range(10): # ...existing code... writer.add_scalar('Loss/train', loss.item(), epoch) writer.add_scalar('Accuracy/val', val_acc, epoch) writer.close() Loss Exploding / Vanishing 原因排查 Loss 爆炸（Exploding） 常見於深層網路、RNN，梯度連乘導致數值爆炸 解法：Gradient Clipping、初始化調整、學習率降低 Loss 消失（Vanishing） 梯度趨近 0，參數無法更新 解法：選擇合適激活函數（ReLU/LeakyReLU）、初始化優化、殘差連接 Learning Curve & Validation Gap 判讀 Learning Curve：訓練/驗證 loss 隨 epoch 變化 Validation Gap：訓練 loss 遠低於驗證 loss，常見於過擬合 Loss 不降：學習率過小、資料/標籤錯誤、模型容量不足 import matplotlib.pyplot as plt plt.plot(train_loss, label='Train Loss') plt.plot(val_loss, label='Val Loss') plt.xlabel('Epoch'); plt.ylabel('Loss') plt.legend(); plt.title('Learning Curve'); plt.show() Unit Test Your Forward / Backward 單元測試 Forward/Backward，確保模型與 loss 可微、梯度正確 可用 torch.autograd.gradcheck、assert 條件 import torch def test_forward_backward(model, x, y): x = x.requires_grad_() output = model(x) loss = loss_fn(output, y) loss.backward() assert all([p.grad is not None for p in model.parameters() if p.requires_grad]), &#34;Gradient missing!&#34; # gradcheck 範例 from torch.autograd import gradcheck # gradcheck(model, (input,)) # 需 double precision 理論直覺、應用場景與常見誤區 應用場景 深度學習訓練監控、模型調參、異常排查、團隊協作 常見誤區 只看 loss，忽略 accuracy、learning rate 等指標 Loss 爆炸未及時啟用 Gradient Clipping Validation Gap 未調整正則化/資料增強 未做單元測試，導致訓練異常難以定位 面試熱點與經典問題 主題 常見問題 TensorBoard 如何追蹤與可視化指標？ Loss 爆炸/消失 原因與解法？ Learning Curve 如何判讀過擬合/欠擬合？ Validation Gap 產生原因與調整方法？ 單元測試 如何驗證模型可微與梯度正確？ 使用注意事項 訓練監控建議多指標聯合追蹤 Loss 爆炸/消失需及時調參與 debug 單元測試可提前發現模型設計問題 延伸閱讀與資源 TensorBoard 官方文件 Weights & Biases 官方文件 PyTorch gradcheck 深度學習 Loss 爆炸/消失解法 經典面試題與解法提示 TensorBoard/Weights & Biases 如何追蹤訓練指標？ Loss 爆炸/消失的數學原因？ Learning Curve 如何判斷過擬合/欠擬合？ Validation Gap 如何調整？ 如何用 Python 單元測試 Forward/Backward？ Loss 不降常見原因？ Gradient Clipping 何時啟用？ 多指標監控的好處？ 單元測試如何設計？ 訓練監控與 Debug 的最佳實踐？ 結語 訓練監控與 Debug 是模型穩健訓練的保障。熟悉 TensorBoard、Loss 爆炸/消失排查、Learning Curve 判讀與單元測試，能讓你高效定位問題並提升模型表現。下一章將進入課程學習與自監督，敬請期待！
"><meta property="og:title" content="訓練監控與 Debug：TensorBoard、Loss 爆炸/消失、Learning Curve 判讀與單元測試"><meta property="og:description" content="模型訓練過程中，監控與 Debug 是確保收斂、提升泛化與快速定位問題的關鍵。本章將深入 TensorBoard/Weights & Biases 指標追蹤、Loss 爆炸/消失原因排查、Learning Curve 與 Validation Gap 判讀、Forward/Backward 單元測試，結合理論、實作、面試熱點與常見誤區，幫助你打造穩健的訓練流程。
TensorBoard / Weights & Biases 指標追蹤 TensorBoard：可視化 loss、accuracy、learning rate、參數分布等 Weights & Biases（wandb）：雲端追蹤、團隊協作、超參數搜尋 from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter() for epoch in range(10): # ...existing code... writer.add_scalar('Loss/train', loss.item(), epoch) writer.add_scalar('Accuracy/val', val_acc, epoch) writer.close() Loss Exploding / Vanishing 原因排查 Loss 爆炸（Exploding） 常見於深層網路、RNN，梯度連乘導致數值爆炸 解法：Gradient Clipping、初始化調整、學習率降低 Loss 消失（Vanishing） 梯度趨近 0，參數無法更新 解法：選擇合適激活函數（ReLU/LeakyReLU）、初始化優化、殘差連接 Learning Curve & Validation Gap 判讀 Learning Curve：訓練/驗證 loss 隨 epoch 變化 Validation Gap：訓練 loss 遠低於驗證 loss，常見於過擬合 Loss 不降：學習率過小、資料/標籤錯誤、模型容量不足 import matplotlib.pyplot as plt plt.plot(train_loss, label='Train Loss') plt.plot(val_loss, label='Val Loss') plt.xlabel('Epoch'); plt.ylabel('Loss') plt.legend(); plt.title('Learning Curve'); plt.show() Unit Test Your Forward / Backward 單元測試 Forward/Backward，確保模型與 loss 可微、梯度正確 可用 torch.autograd.gradcheck、assert 條件 import torch def test_forward_backward(model, x, y): x = x.requires_grad_() output = model(x) loss = loss_fn(output, y) loss.backward() assert all([p.grad is not None for p in model.parameters() if p.requires_grad]), &#34;Gradient missing!&#34; # gradcheck 範例 from torch.autograd import gradcheck # gradcheck(model, (input,)) # 需 double precision 理論直覺、應用場景與常見誤區 應用場景 深度學習訓練監控、模型調參、異常排查、團隊協作 常見誤區 只看 loss，忽略 accuracy、learning rate 等指標 Loss 爆炸未及時啟用 Gradient Clipping Validation Gap 未調整正則化/資料增強 未做單元測試，導致訓練異常難以定位 面試熱點與經典問題 主題 常見問題 TensorBoard 如何追蹤與可視化指標？ Loss 爆炸/消失 原因與解法？ Learning Curve 如何判讀過擬合/欠擬合？ Validation Gap 產生原因與調整方法？ 單元測試 如何驗證模型可微與梯度正確？ 使用注意事項 訓練監控建議多指標聯合追蹤 Loss 爆炸/消失需及時調參與 debug 單元測試可提前發現模型設計問題 延伸閱讀與資源 TensorBoard 官方文件 Weights & Biases 官方文件 PyTorch gradcheck 深度學習 Loss 爆炸/消失解法 經典面試題與解法提示 TensorBoard/Weights & Biases 如何追蹤訓練指標？ Loss 爆炸/消失的數學原因？ Learning Curve 如何判斷過擬合/欠擬合？ Validation Gap 如何調整？ 如何用 Python 單元測試 Forward/Backward？ Loss 不降常見原因？ Gradient Clipping 何時啟用？ 多指標監控的好處？ 單元測試如何設計？ 訓練監控與 Debug 的最佳實踐？ 結語 訓練監控與 Debug 是模型穩健訓練的保障。熟悉 TensorBoard、Loss 爆炸/消失排查、Learning Curve 判讀與單元測試，能讓你高效定位問題並提升模型表現。下一章將進入課程學習與自監督，敬請期待！
"><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/training-monitor-debug/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/training-monitor-debug/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>訓練監控與 Debug：TensorBoard、Loss 爆炸/消失、Learning Curve 判讀與單元測試</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>訓練監控與 Debug：TensorBoard、Loss 爆炸/消失、Learning Curve 判讀與單元測試</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-11-07</span></div></header><div class=article-body><p>模型訓練過程中，監控與 Debug 是確保收斂、提升泛化與快速定位問題的關鍵。本章將深入 TensorBoard/Weights & Biases 指標追蹤、Loss 爆炸/消失原因排查、Learning Curve 與 Validation Gap 判讀、Forward/Backward 單元測試，結合理論、實作、面試熱點與常見誤區，幫助你打造穩健的訓練流程。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#tensorboard--weights--biases-指標追蹤>TensorBoard / Weights & Biases 指標追蹤</a></li><li><a href=#loss-exploding--vanishing-原因排查>Loss Exploding / Vanishing 原因排查</a><ul><li><a href=#loss-爆炸exploding>Loss 爆炸（Exploding）</a></li><li><a href=#loss-消失vanishing>Loss 消失（Vanishing）</a></li></ul></li><li><a href=#learning-curve--validation-gap-判讀>Learning Curve & Validation Gap 判讀</a></li><li><a href=#unit-test-your-forward--backward>Unit Test Your Forward / Backward</a></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=tensorboard--weights--biases-指標追蹤>TensorBoard / Weights & Biases 指標追蹤</h2><ul><li>TensorBoard：可視化 loss、accuracy、learning rate、參數分布等</li><li>Weights & Biases（wandb）：雲端追蹤、團隊協作、超參數搜尋</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.tensorboard <span style=color:#f92672>import</span> SummaryWriter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>writer <span style=color:#f92672>=</span> SummaryWriter()
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...existing code...</span>
</span></span><span style=display:flex><span>    writer<span style=color:#f92672>.</span>add_scalar(<span style=color:#e6db74>&#39;Loss/train&#39;</span>, loss<span style=color:#f92672>.</span>item(), epoch)
</span></span><span style=display:flex><span>    writer<span style=color:#f92672>.</span>add_scalar(<span style=color:#e6db74>&#39;Accuracy/val&#39;</span>, val_acc, epoch)
</span></span><span style=display:flex><span>writer<span style=color:#f92672>.</span>close()
</span></span></code></pre></div><hr><h2 id=loss-exploding--vanishing-原因排查>Loss Exploding / Vanishing 原因排查</h2><h3 id=loss-爆炸exploding>Loss 爆炸（Exploding）</h3><ul><li>常見於深層網路、RNN，梯度連乘導致數值爆炸</li><li>解法：Gradient Clipping、初始化調整、學習率降低</li></ul><h3 id=loss-消失vanishing>Loss 消失（Vanishing）</h3><ul><li>梯度趨近 0，參數無法更新</li><li>解法：選擇合適激活函數（ReLU/LeakyReLU）、初始化優化、殘差連接</li></ul><hr><h2 id=learning-curve--validation-gap-判讀>Learning Curve & Validation Gap 判讀</h2><ul><li>Learning Curve：訓練/驗證 loss 隨 epoch 變化</li><li>Validation Gap：訓練 loss 遠低於驗證 loss，常見於過擬合</li><li>Loss 不降：學習率過小、資料/標籤錯誤、模型容量不足</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(train_loss, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Train Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(val_loss, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Val Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Epoch&#39;</span>); plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend(); plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Learning Curve&#39;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><hr><h2 id=unit-test-your-forward--backward>Unit Test Your Forward / Backward</h2><ul><li>單元測試 Forward/Backward，確保模型與 loss 可微、梯度正確</li><li>可用 torch.autograd.gradcheck、assert 條件</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_forward_backward</span>(model, x, y):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>requires_grad_()
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> model(x)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> loss_fn(output, y)
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> all([p<span style=color:#f92672>.</span>grad <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span> <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>parameters() <span style=color:#66d9ef>if</span> p<span style=color:#f92672>.</span>requires_grad]), <span style=color:#e6db74>&#34;Gradient missing!&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># gradcheck 範例</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.autograd <span style=color:#f92672>import</span> gradcheck
</span></span><span style=display:flex><span><span style=color:#75715e># gradcheck(model, (input,))  # 需 double precision</span>
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>深度學習訓練監控、模型調參、異常排查、團隊協作</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>只看 loss，忽略 accuracy、learning rate 等指標</li><li>Loss 爆炸未及時啟用 Gradient Clipping</li><li>Validation Gap 未調整正則化/資料增強</li><li>未做單元測試，導致訓練異常難以定位</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>TensorBoard</td><td>如何追蹤與可視化指標？</td></tr><tr><td>Loss 爆炸/消失</td><td>原因與解法？</td></tr><tr><td>Learning Curve</td><td>如何判讀過擬合/欠擬合？</td></tr><tr><td>Validation Gap</td><td>產生原因與調整方法？</td></tr><tr><td>單元測試</td><td>如何驗證模型可微與梯度正確？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>訓練監控建議多指標聯合追蹤</li><li>Loss 爆炸/消失需及時調參與 debug</li><li>單元測試可提前發現模型設計問題</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://pytorch.org/docs/stable/tensorboard.html>TensorBoard 官方文件</a></li><li><a href=https://docs.wandb.ai/>Weights & Biases 官方文件</a></li><li><a href=https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradcheck>PyTorch gradcheck</a></li><li><a href=https://www.deeplearningbook.org/contents/numerical.html>深度學習 Loss 爆炸/消失解法</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>TensorBoard/Weights & Biases 如何追蹤訓練指標？</li><li>Loss 爆炸/消失的數學原因？</li><li>Learning Curve 如何判斷過擬合/欠擬合？</li><li>Validation Gap 如何調整？</li><li>如何用 Python 單元測試 Forward/Backward？</li><li>Loss 不降常見原因？</li><li>Gradient Clipping 何時啟用？</li><li>多指標監控的好處？</li><li>單元測試如何設計？</li><li>訓練監控與 Debug 的最佳實踐？</li></ol><hr><h2 id=結語>結語</h2><p>訓練監控與 Debug 是模型穩健訓練的保障。熟悉 TensorBoard、Loss 爆炸/消失排查、Learning Curve 判讀與單元測試，能讓你高效定位問題並提升模型表現。下一章將進入課程學習與自監督，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>TensorBoard</span>
<span class=tag>Weights & Biases</span>
<span class=tag>Loss Exploding</span>
<span class=tag>Loss Vanishing</span>
<span class=tag>Learning Curve</span>
<span class=tag>Validation Gap</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>