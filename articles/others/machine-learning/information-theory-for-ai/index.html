<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>信息理論 & 損失函數：AI 必備的熵、交叉熵與機率距離全解析 - Yu's Portfolio & Learning Hub</title><meta name=description content='信息理論（Information Theory）是現代機器學習、深度學習與資料壓縮的理論基礎。從分類模型的損失函數，到生成模型的機率距離，熵、交叉熵、KL 散度等概念無處不在。本篇將深入剖析這些核心數學工具，並結合直覺、推導、應用與 Python 實作，讓你徹底掌握 AI 必備的信息理論基礎。
熵（Entropy）：不確定性的度量 熵的定義與直覺 熵（Entropy） 衡量隨機變數不確定性的指標，單位為 bit（以 2 為底）或 nat（以 e 為底）。 熵越大，表示資訊越分散、越難預測；熵越小，表示資訊越集中、越容易預測。 $$ H(X) = -\sum_{i} P(x_i) \log P(x_i) $$
熵的應用場景 決策樹分裂（資訊增益） 資料壓縮（霍夫曼編碼） 模型不確定性評估 import numpy as np def entropy(p): p = np.array(p) p = p[p > 0] # 避免 log(0) return -np.sum(p * np.log2(p)) print("均勻分布熵:", entropy([0.5, 0.5])) print("偏態分布熵:", entropy([0.9, 0.1])) 互資訊（Mutual Information）：變數間的資訊共享 互資訊（MI） 衡量兩個隨機變數間共享的資訊量。 MI 越大，表示兩變數關聯越強。 $$ I(X; Y) = \sum_{x, y} P(x, y) \log \frac{P(x, y)}{P(x)P(y)} $$
應用場景 特徵選擇（選出與標籤最相關的特徵） 表徵學習（資訊瓶頸理論） 交叉熵（Cross-Entropy）：機率分布間的距離 交叉熵的定義 衡量「真實分布」與「預測分布」間的距離，是分類任務最常用的損失函數。 $$ H(P, Q) = -\sum_{i} P(x_i) \log Q(x_i) $$
'><meta property="og:title" content="信息理論 & 損失函數：AI 必備的熵、交叉熵與機率距離全解析"><meta property="og:description" content='信息理論（Information Theory）是現代機器學習、深度學習與資料壓縮的理論基礎。從分類模型的損失函數，到生成模型的機率距離，熵、交叉熵、KL 散度等概念無處不在。本篇將深入剖析這些核心數學工具，並結合直覺、推導、應用與 Python 實作，讓你徹底掌握 AI 必備的信息理論基礎。
熵（Entropy）：不確定性的度量 熵的定義與直覺 熵（Entropy） 衡量隨機變數不確定性的指標，單位為 bit（以 2 為底）或 nat（以 e 為底）。 熵越大，表示資訊越分散、越難預測；熵越小，表示資訊越集中、越容易預測。 $$ H(X) = -\sum_{i} P(x_i) \log P(x_i) $$
熵的應用場景 決策樹分裂（資訊增益） 資料壓縮（霍夫曼編碼） 模型不確定性評估 import numpy as np def entropy(p): p = np.array(p) p = p[p > 0] # 避免 log(0) return -np.sum(p * np.log2(p)) print("均勻分布熵:", entropy([0.5, 0.5])) print("偏態分布熵:", entropy([0.9, 0.1])) 互資訊（Mutual Information）：變數間的資訊共享 互資訊（MI） 衡量兩個隨機變數間共享的資訊量。 MI 越大，表示兩變數關聯越強。 $$ I(X; Y) = \sum_{x, y} P(x, y) \log \frac{P(x, y)}{P(x)P(y)} $$
應用場景 特徵選擇（選出與標籤最相關的特徵） 表徵學習（資訊瓶頸理論） 交叉熵（Cross-Entropy）：機率分布間的距離 交叉熵的定義 衡量「真實分布」與「預測分布」間的距離，是分類任務最常用的損失函數。 $$ H(P, Q) = -\sum_{i} P(x_i) \log Q(x_i) $$
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/information-theory-for-ai/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/information-theory-for-ai/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>信息理論 & 損失函數：AI 必備的熵、交叉熵與機率距離全解析</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>信息理論 & 損失函數：AI 必備的熵、交叉熵與機率距離全解析</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-03-04</span></div></header><div class=article-body><p>信息理論（Information Theory）是現代機器學習、深度學習與資料壓縮的理論基礎。從分類模型的損失函數，到生成模型的機率距離，熵、交叉熵、KL 散度等概念無處不在。本篇將深入剖析這些核心數學工具，並結合直覺、推導、應用與 Python 實作，讓你徹底掌握 AI 必備的信息理論基礎。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#熵entropy不確定性的度量>熵（Entropy）：不確定性的度量</a><ul><li><a href=#熵的定義與直覺>熵的定義與直覺</a></li><li><a href=#熵的應用場景>熵的應用場景</a></li></ul></li><li><a href=#互資訊mutual-information變數間的資訊共享>互資訊（Mutual Information）：變數間的資訊共享</a><ul><li><a href=#應用場景>應用場景</a></li></ul></li><li><a href=#交叉熵cross-entropy機率分布間的距離>交叉熵（Cross-Entropy）：機率分布間的距離</a><ul><li><a href=#交叉熵的定義>交叉熵的定義</a></li><li><a href=#交叉熵的直覺>交叉熵的直覺</a></li><li><a href=#python-實作>Python 實作</a></li></ul></li><li><a href=#kl-散度kullback-leibler-divergence分布間的非對稱距離>KL 散度（Kullback-Leibler Divergence）：分布間的非對稱距離</a><ul><li><a href=#kl-散度的定義>KL 散度的定義</a></li><li><a href=#kl-散度的應用>KL 散度的應用</a></li><li><a href=#python-實作-1>Python 實作</a></li></ul></li><li><a href=#js-散度jensen-shannon-divergence對稱的機率距離>JS 散度（Jensen-Shannon Divergence）：對稱的機率距離</a></li><li><a href=#softmax--cross-entropy為何好用>Softmax + Cross-Entropy：為何好用？</a><ul><li><a href=#softmax-函數>Softmax 函數</a></li><li><a href=#結合交叉熵的優點>結合交叉熵的優點</a></li></ul></li><li><a href=#理論推導與直覺圖解>理論推導與直覺圖解</a></li><li><a href=#應用場景與常見誤區>應用場景與常見誤區</a><ul><li><a href=#應用場景-1>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#常見面試熱點整理>常見面試熱點整理</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=熵entropy不確定性的度量>熵（Entropy）：不確定性的度量</h2><h3 id=熵的定義與直覺>熵的定義與直覺</h3><ul><li><strong>熵（Entropy）</strong> 衡量隨機變數不確定性的指標，單位為 bit（以 2 為底）或 nat（以 e 為底）。</li><li>熵越大，表示資訊越分散、越難預測；熵越小，表示資訊越集中、越容易預測。</li></ul><p>$$
H(X) = -\sum_{i} P(x_i) \log P(x_i)
$$</p><h3 id=熵的應用場景>熵的應用場景</h3><ul><li>決策樹分裂（資訊增益）</li><li>資料壓縮（霍夫曼編碼）</li><li>模型不確定性評估</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>entropy</span>(p):
</span></span><span style=display:flex><span>    p <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(p)
</span></span><span style=display:flex><span>    p <span style=color:#f92672>=</span> p[p <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>]  <span style=color:#75715e># 避免 log(0)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>np<span style=color:#f92672>.</span>sum(p <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log2(p))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;均勻分布熵:&#34;</span>, entropy([<span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>]))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;偏態分布熵:&#34;</span>, entropy([<span style=color:#ae81ff>0.9</span>, <span style=color:#ae81ff>0.1</span>]))
</span></span></code></pre></div><hr><h2 id=互資訊mutual-information變數間的資訊共享>互資訊（Mutual Information）：變數間的資訊共享</h2><ul><li><strong>互資訊（MI）</strong> 衡量兩個隨機變數間共享的資訊量。</li><li>MI 越大，表示兩變數關聯越強。</li></ul><p>$$
I(X; Y) = \sum_{x, y} P(x, y) \log \frac{P(x, y)}{P(x)P(y)}
$$</p><h3 id=應用場景>應用場景</h3><ul><li>特徵選擇（選出與標籤最相關的特徵）</li><li>表徵學習（資訊瓶頸理論）</li></ul><hr><h2 id=交叉熵cross-entropy機率分布間的距離>交叉熵（Cross-Entropy）：機率分布間的距離</h2><h3 id=交叉熵的定義>交叉熵的定義</h3><ul><li>衡量「真實分布」與「預測分布」間的距離，是分類任務最常用的損失函數。</li></ul><p>$$
H(P, Q) = -\sum_{i} P(x_i) \log Q(x_i)
$$</p><ul><li>$P$ 為真實分布，$Q$ 為模型預測分布。</li></ul><h3 id=交叉熵的直覺>交叉熵的直覺</h3><ul><li>預測越接近真實分布，交叉熵越小。</li><li>若模型預測與真實分布完全一致，交叉熵等於熵。</li></ul><h3 id=python-實作>Python 實作</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cross_entropy</span>(p, q):
</span></span><span style=display:flex><span>    p, q <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(p), np<span style=color:#f92672>.</span>array(q)
</span></span><span style=display:flex><span>    q <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>clip(q, <span style=color:#ae81ff>1e-12</span>, <span style=color:#ae81ff>1.0</span>)  <span style=color:#75715e># 避免 log(0)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>np<span style=color:#f92672>.</span>sum(p <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(q))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>p <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>q <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0.7</span>, <span style=color:#ae81ff>0.2</span>, <span style=color:#ae81ff>0.1</span>]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;交叉熵:&#34;</span>, cross_entropy(p, q))
</span></span></code></pre></div><hr><h2 id=kl-散度kullback-leibler-divergence分布間的非對稱距離>KL 散度（Kullback-Leibler Divergence）：分布間的非對稱距離</h2><h3 id=kl-散度的定義>KL 散度的定義</h3><ul><li>衡量一個分布 Q 假裝成分布 P 時，平均多花多少「資訊量」。</li><li>非對稱：$KL(P||Q) \neq KL(Q||P)$</li></ul><p>$$
KL(P||Q) = \sum_{i} P(x_i) \log \frac{P(x_i)}{Q(x_i)}
$$</p><h3 id=kl-散度的應用>KL 散度的應用</h3><ul><li>生成模型（如 VAE）</li><li>損失函數（如知識蒸餾）</li><li>機率分布近似</li></ul><h3 id=python-實作-1>Python 實作</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>kl_divergence</span>(p, q):
</span></span><span style=display:flex><span>    p, q <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(p), np<span style=color:#f92672>.</span>array(q)
</span></span><span style=display:flex><span>    p, q <span style=color:#f92672>=</span> p[p <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>], q[p <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    q <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>clip(q, <span style=color:#ae81ff>1e-12</span>, <span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sum(p <span style=color:#f92672>*</span> (np<span style=color:#f92672>.</span>log(p) <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>log(q)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;KL 散度:&#34;</span>, kl_divergence([<span style=color:#ae81ff>0.8</span>, <span style=color:#ae81ff>0.2</span>], [<span style=color:#ae81ff>0.6</span>, <span style=color:#ae81ff>0.4</span>]))
</span></span></code></pre></div><hr><h2 id=js-散度jensen-shannon-divergence對稱的機率距離>JS 散度（Jensen-Shannon Divergence）：對稱的機率距離</h2><ul><li>JS 散度是 KL 散度的對稱化版本，常用於生成模型評估（如 GAN）。</li><li>$$
JS(P||Q) = \frac{1}{2} KL(P||M) + \frac{1}{2} KL(Q||M), \quad M = \frac{P+Q}{2}
$$</li></ul><hr><h2 id=softmax--cross-entropy為何好用>Softmax + Cross-Entropy：為何好用？</h2><h3 id=softmax-函數>Softmax 函數</h3><ul><li>將任意實數向量轉換為機率分布，常用於多分類模型的輸出層。</li></ul><p>$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$</p><h3 id=結合交叉熵的優點>結合交叉熵的優點</h3><ul><li>數學上導數簡潔，便於反向傳播。</li><li>能有效懲罰錯誤預測，提升模型收斂速度。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>softmax</span>(z):
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(z)
</span></span><span style=display:flex><span>    exp_z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>exp(z <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>max(z))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> exp_z <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>sum(exp_z)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> [<span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>0.1</span>]
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Softmax 機率:&#34;</span>, probs)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;交叉熵損失:&#34;</span>, cross_entropy([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>], probs))
</span></span></code></pre></div><hr><h2 id=理論推導與直覺圖解>理論推導與直覺圖解</h2><ul><li>熵、交叉熵、KL 散度皆可視為「平均資訊量」的不同度量。</li><li>交叉熵 = 熵 + KL 散度，反映模型預測與真實分布的差異。</li><li>JS 散度則能衡量兩分布的「對稱距離」，適合生成模型評估。</li></ul><hr><h2 id=應用場景與常見誤區>應用場景與常見誤區</h2><h3 id=應用場景-1>應用場景</h3><ul><li>分類模型訓練（交叉熵損失）</li><li>生成模型（KL/JS 散度）</li><li>決策樹（資訊增益）</li><li>知識蒸餾（KL 散度）</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>KL 散度非對稱，不能當作一般距離使用。</li><li>Softmax 輸出過於極端時，易導致梯度消失或數值不穩。</li><li>交叉熵損失需搭配 one-hot 標籤或機率分布。</li></ul><hr><h2 id=常見面試熱點整理>常見面試熱點整理</h2><table><thead><tr><th>熱點主題</th><th>面試常問問題</th></tr></thead><tbody><tr><td>熵/交叉熵</td><td>兩者差異與應用？</td></tr><tr><td>KL/JS Divergence</td><td>何時用？有何數學性質？</td></tr><tr><td>Softmax + CE</td><td>為何組合效果好？</td></tr><tr><td>機率距離</td><td>如何選擇適合的損失？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>熵與交叉熵常用於分類與資訊理論相關任務。</li><li>KL/JS 散度適合評估分布間差異，但需注意數值穩定性。</li><li>Softmax + Cross-Entropy 是多分類的黃金組合，但需正確處理標籤格式。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://www.deeplearningbook.org/contents/information-theory.html>Deep Learning Book: Information Theory</a></li><li><a href="https://www.youtube.com/watch?v=ErfnhcEV1O8">StatQuest: Cross Entropy, KL Divergence</a></li><li><a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html>Scipy.stats 熵與距離文件</a></li></ul><hr><h2 id=結語>結語</h2><p>信息理論為 AI 提供了衡量不確定性與分布差異的數學工具。掌握熵、交叉熵、KL/JS 散度與 Softmax，不僅能讓你設計更有效的損失函數，也能在生成模型、分類任務與面試中展現深厚的理論素養。下一章將進入數值計算與穩定性，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>KL Divergence</span>
<span class=tag>JS Divergence</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>