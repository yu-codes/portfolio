<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Attention 機制拆解：Scaled Dot-Product、Multi-Head、QKV 幾何與 Masking - Yu's Portfolio & Learning Hub</title><meta name=description content='Attention 機制是現代深度學習（尤其是 NLP 和 Vision Transformer）的核心。從 Scaled Dot-Product Attention、Multi-Head & Self-Attention，到 Q/K/V 幾何意義與 Masking 技巧，本章將結合理論推導、圖解、Python 實作、面試熱點與常見誤區，幫助你徹底掌握 Attention。
Scaled Dot-Product Attention 數學公式 $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
$Q$（Query）、$K$（Key）、$V$（Value）為輸入矩陣 $d_k$ 為 Key 維度，縮放避免梯度消失/爆炸 Python 實作 import torch import torch.nn.functional as F Q = torch.randn(2, 4, 8) # batch, seq, d_k K = torch.randn(2, 4, 8) V = torch.randn(2, 4, 8) scores = torch.matmul(Q, K.transpose(-2, -1)) / (8 ** 0.5) attn = F.softmax(scores, dim=-1) output = torch.matmul(attn, V) print("Attention 輸出 shape:", output.shape) Multi-Head & Self-Attention 可視化 Multi-Head Attention 多組 Q/K/V 並行計算，捕捉不同子空間資訊 輸出拼接後再線性變換，提升模型表達力 Self-Attention Q/K/V 皆來自同一序列，捕捉序列內部依賴 可視化：每個 token 對其他 token 的關注分數 Q/K/V 的幾何意義 Q：查詢向量，代表當前 token 想「問」什麼 K：鍵向量，代表每個 token 能「回答」什麼 V：值向量，攜帶實際資訊 Attention = 查詢與所有鍵的相似度加權所有值 Masking：Padding vs. Causal Padding Mask 遮蔽填充（padding）位置，避免影響注意力分數 常用於批次處理長度不一的序列 Causal Mask 遮蔽未來資訊，確保自回歸生成時僅依賴過去 常用於語言模型（如 GPT） seq_len = 5 mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() scores = torch.randn(seq_len, seq_len) scores.masked_fill_(mask, float(&#39;-inf&#39;)) attn = F.softmax(scores, dim=-1) print("Causal Masked Attention:", attn) 理論直覺、應用場景與常見誤區 應用場景 NLP（翻譯、摘要、問答）、Vision Transformer、語音辨識、推薦系統 常見誤區 忽略縮放因子，導致梯度不穩 Masking 實作錯誤，導致洩漏未來資訊 Multi-Head 輸出未正確拼接 面試熱點與經典問題 主題 常見問題 Scaled Dot-Product 為何要縮放？ Multi-Head 有何優勢？ Q/K/V 幾何意義？ Masking Padding vs Causal 差異？ Self-Attention 如何捕捉長距依賴？ 使用注意事項 注意 Q/K/V 維度一致性 Masking 必須正確設計，避免資訊洩漏 多頭注意力需拼接後再線性變換 延伸閱讀與資源 Attention is All You Need 論文 PyTorch MultiheadAttention The Illustrated Transformer 經典面試題與解法提示 Scaled Dot-Product Attention 數學推導？ Multi-Head Attention 如何提升表達力？ Q/K/V 的幾何直覺？ Self-Attention 與傳統 RNN 差異？ Masking 有哪些類型？如何實作？ 如何用 Python 實作簡單 Attention？ 為何要做縮放？有何數值意義？ Multi-Head 拼接與線性變換細節？ Attention 如何捕捉長距依賴？ Masking 實作錯誤會有什麼後果？ 結語 Attention 機制是深度學習的革命。熟悉 Scaled Dot-Product、Multi-Head、QKV 幾何與 Masking，能讓你在 NLP、Vision、生成模型等領域發揮 Transformer 強大威力。下一章將進入 Transformer 家族，敬請期待！
'><meta property="og:title" content="Attention 機制拆解：Scaled Dot-Product、Multi-Head、QKV 幾何與 Masking"><meta property="og:description" content='Attention 機制是現代深度學習（尤其是 NLP 和 Vision Transformer）的核心。從 Scaled Dot-Product Attention、Multi-Head & Self-Attention，到 Q/K/V 幾何意義與 Masking 技巧，本章將結合理論推導、圖解、Python 實作、面試熱點與常見誤區，幫助你徹底掌握 Attention。
Scaled Dot-Product Attention 數學公式 $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
$Q$（Query）、$K$（Key）、$V$（Value）為輸入矩陣 $d_k$ 為 Key 維度，縮放避免梯度消失/爆炸 Python 實作 import torch import torch.nn.functional as F Q = torch.randn(2, 4, 8) # batch, seq, d_k K = torch.randn(2, 4, 8) V = torch.randn(2, 4, 8) scores = torch.matmul(Q, K.transpose(-2, -1)) / (8 ** 0.5) attn = F.softmax(scores, dim=-1) output = torch.matmul(attn, V) print("Attention 輸出 shape:", output.shape) Multi-Head & Self-Attention 可視化 Multi-Head Attention 多組 Q/K/V 並行計算，捕捉不同子空間資訊 輸出拼接後再線性變換，提升模型表達力 Self-Attention Q/K/V 皆來自同一序列，捕捉序列內部依賴 可視化：每個 token 對其他 token 的關注分數 Q/K/V 的幾何意義 Q：查詢向量，代表當前 token 想「問」什麼 K：鍵向量，代表每個 token 能「回答」什麼 V：值向量，攜帶實際資訊 Attention = 查詢與所有鍵的相似度加權所有值 Masking：Padding vs. Causal Padding Mask 遮蔽填充（padding）位置，避免影響注意力分數 常用於批次處理長度不一的序列 Causal Mask 遮蔽未來資訊，確保自回歸生成時僅依賴過去 常用於語言模型（如 GPT） seq_len = 5 mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() scores = torch.randn(seq_len, seq_len) scores.masked_fill_(mask, float(&#39;-inf&#39;)) attn = F.softmax(scores, dim=-1) print("Causal Masked Attention:", attn) 理論直覺、應用場景與常見誤區 應用場景 NLP（翻譯、摘要、問答）、Vision Transformer、語音辨識、推薦系統 常見誤區 忽略縮放因子，導致梯度不穩 Masking 實作錯誤，導致洩漏未來資訊 Multi-Head 輸出未正確拼接 面試熱點與經典問題 主題 常見問題 Scaled Dot-Product 為何要縮放？ Multi-Head 有何優勢？ Q/K/V 幾何意義？ Masking Padding vs Causal 差異？ Self-Attention 如何捕捉長距依賴？ 使用注意事項 注意 Q/K/V 維度一致性 Masking 必須正確設計，避免資訊洩漏 多頭注意力需拼接後再線性變換 延伸閱讀與資源 Attention is All You Need 論文 PyTorch MultiheadAttention The Illustrated Transformer 經典面試題與解法提示 Scaled Dot-Product Attention 數學推導？ Multi-Head Attention 如何提升表達力？ Q/K/V 的幾何直覺？ Self-Attention 與傳統 RNN 差異？ Masking 有哪些類型？如何實作？ 如何用 Python 實作簡單 Attention？ 為何要做縮放？有何數值意義？ Multi-Head 拼接與線性變換細節？ Attention 如何捕捉長距依賴？ Masking 實作錯誤會有什麼後果？ 結語 Attention 機制是深度學習的革命。熟悉 Scaled Dot-Product、Multi-Head、QKV 幾何與 Masking，能讓你在 NLP、Vision、生成模型等領域發揮 Transformer 強大威力。下一章將進入 Transformer 家族，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/attention-mechanism/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/attention-mechanism/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>Attention 機制拆解：Scaled Dot-Product、Multi-Head、QKV 幾何與 Masking</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>Attention 機制拆解：Scaled Dot-Product、Multi-Head、QKV 幾何與 Masking</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-05-14</span></div></header><div class=article-body><p>Attention 機制是現代深度學習（尤其是 NLP 和 Vision Transformer）的核心。從 Scaled Dot-Product Attention、Multi-Head & Self-Attention，到 Q/K/V 幾何意義與 Masking 技巧，本章將結合理論推導、圖解、Python 實作、面試熱點與常見誤區，幫助你徹底掌握 Attention。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#scaled-dot-product-attention>Scaled Dot-Product Attention</a><ul><li><a href=#數學公式>數學公式</a></li><li><a href=#python-實作>Python 實作</a></li></ul></li><li><a href=#multi-head--self-attention-可視化>Multi-Head & Self-Attention 可視化</a><ul><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#self-attention>Self-Attention</a></li></ul></li><li><a href=#qkv-的幾何意義>Q/K/V 的幾何意義</a></li><li><a href=#maskingpadding-vs-causal>Masking：Padding vs. Causal</a><ul><li><a href=#padding-mask>Padding Mask</a></li><li><a href=#causal-mask>Causal Mask</a></li></ul></li><li><a href=#理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</a><ul><li><a href=#應用場景>應用場景</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#面試熱點與經典問題>面試熱點與經典問題</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#經典面試題與解法提示>經典面試題與解法提示</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=scaled-dot-product-attention>Scaled Dot-Product Attention</h2><h3 id=數學公式>數學公式</h3><p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p><ul><li>$Q$（Query）、$K$（Key）、$V$（Value）為輸入矩陣</li><li>$d_k$ 為 Key 維度，縮放避免梯度消失/爆炸</li></ul><h3 id=python-實作>Python 實作</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Q <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>8</span>)  <span style=color:#75715e># batch, seq, d_k</span>
</span></span><span style=display:flex><span>K <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>V <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>scores <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>matmul(Q, K<span style=color:#f92672>.</span>transpose(<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)) <span style=color:#f92672>/</span> (<span style=color:#ae81ff>8</span> <span style=color:#f92672>**</span> <span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>attn <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(scores, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>matmul(attn, V)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Attention 輸出 shape:&#34;</span>, output<span style=color:#f92672>.</span>shape)
</span></span></code></pre></div><hr><h2 id=multi-head--self-attention-可視化>Multi-Head & Self-Attention 可視化</h2><h3 id=multi-head-attention>Multi-Head Attention</h3><ul><li>多組 Q/K/V 並行計算，捕捉不同子空間資訊</li><li>輸出拼接後再線性變換，提升模型表達力</li></ul><h3 id=self-attention>Self-Attention</h3><ul><li>Q/K/V 皆來自同一序列，捕捉序列內部依賴</li><li>可視化：每個 token 對其他 token 的關注分數</li></ul><hr><h2 id=qkv-的幾何意義>Q/K/V 的幾何意義</h2><ul><li>Q：查詢向量，代表當前 token 想「問」什麼</li><li>K：鍵向量，代表每個 token 能「回答」什麼</li><li>V：值向量，攜帶實際資訊</li><li>Attention = 查詢與所有鍵的相似度加權所有值</li></ul><hr><h2 id=maskingpadding-vs-causal>Masking：Padding vs. Causal</h2><h3 id=padding-mask>Padding Mask</h3><ul><li>遮蔽填充（padding）位置，避免影響注意力分數</li><li>常用於批次處理長度不一的序列</li></ul><h3 id=causal-mask>Causal Mask</h3><ul><li>遮蔽未來資訊，確保自回歸生成時僅依賴過去</li><li>常用於語言模型（如 GPT）</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>seq_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>triu(torch<span style=color:#f92672>.</span>ones(seq_len, seq_len), diagonal<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>bool()
</span></span><span style=display:flex><span>scores <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(seq_len, seq_len)
</span></span><span style=display:flex><span>scores<span style=color:#f92672>.</span>masked_fill_(mask, float(<span style=color:#e6db74>&#39;-inf&#39;</span>))
</span></span><span style=display:flex><span>attn <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(scores, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Causal Masked Attention:&#34;</span>, attn)
</span></span></code></pre></div><hr><h2 id=理論直覺應用場景與常見誤區>理論直覺、應用場景與常見誤區</h2><h3 id=應用場景>應用場景</h3><ul><li>NLP（翻譯、摘要、問答）、Vision Transformer、語音辨識、推薦系統</li></ul><h3 id=常見誤區>常見誤區</h3><ul><li>忽略縮放因子，導致梯度不穩</li><li>Masking 實作錯誤，導致洩漏未來資訊</li><li>Multi-Head 輸出未正確拼接</li></ul><hr><h2 id=面試熱點與經典問題>面試熱點與經典問題</h2><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>Scaled Dot-Product</td><td>為何要縮放？</td></tr><tr><td>Multi-Head</td><td>有何優勢？</td></tr><tr><td>Q/K/V</td><td>幾何意義？</td></tr><tr><td>Masking</td><td>Padding vs Causal 差異？</td></tr><tr><td>Self-Attention</td><td>如何捕捉長距依賴？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>注意 Q/K/V 維度一致性</li><li>Masking 必須正確設計，避免資訊洩漏</li><li>多頭注意力需拼接後再線性變換</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://arxiv.org/abs/1706.03762>Attention is All You Need 論文</a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html>PyTorch MultiheadAttention</a></li><li><a href=https://jalammar.github.io/illustrated-transformer/>The Illustrated Transformer</a></li></ul><hr><h2 id=經典面試題與解法提示>經典面試題與解法提示</h2><ol><li>Scaled Dot-Product Attention 數學推導？</li><li>Multi-Head Attention 如何提升表達力？</li><li>Q/K/V 的幾何直覺？</li><li>Self-Attention 與傳統 RNN 差異？</li><li>Masking 有哪些類型？如何實作？</li><li>如何用 Python 實作簡單 Attention？</li><li>為何要做縮放？有何數值意義？</li><li>Multi-Head 拼接與線性變換細節？</li><li>Attention 如何捕捉長距依賴？</li><li>Masking 實作錯誤會有什麼後果？</li></ol><hr><h2 id=結語>結語</h2><p>Attention 機制是深度學習的革命。熟悉 Scaled Dot-Product、Multi-Head、QKV 幾何與 Masking，能讓你在 NLP、Vision、生成模型等領域發揮 Transformer 強大威力。下一章將進入 Transformer 家族，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>Attention</span>
<span class=tag>Scaled Dot-Product</span>
<span class=tag>Multi-Head</span>
<span class=tag>Self-Attention</span>
<span class=tag>QKV</span>
<span class=tag>Masking</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>