<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>數值計算與穩定性：AI 實作必備的數值技巧與陷阱 - Yu's Portfolio & Learning Hub</title><meta name=description content='數值計算是連接理論與實作的橋樑。再完美的數學模型，若忽略數值誤差與計算穩定性，實際運作時都可能出現爆炸或崩潰。本篇將深入探討 AI 常見的數值問題、穩定化技巧、稀疏運算加速，並結合理論、實作與面試重點，讓你在實戰中少踩坑。
浮點誤差、Underflow/Overflow 浮點數的本質與限制 電腦用有限位元表示實數，導致精度有限。 常見問題：加減極小數、極大數時精度損失。 Underflow/Overflow Underflow：數值太小被當成 0。 Overflow：數值太大超出表示範圍，變成無窮大或 NaN。 實例：浮點誤差 a = 1e16 b = 1.0 print("a + b - a =", (a + b) - a) # 理論上應為 1.0，實際上可能為 0.0 常見陷阱 連加小數、極端指數運算、極小機率相乘（如序列模型） Log-Sum-Exp Trick：避免數值爆炸的黃金法則 問題來源 Softmax、交叉熵等常用到 $\exp(x)$，大 x 易爆炸，小 x 易 underflow。 解法：Log-Sum-Exp $$ \log \sum_i e^{x_i} = a + \log \sum_i e^{x_i - a} $$
其中 $a = \max(x_i)$，可顯著提升數值穩定性。
import numpy as np def log_sum_exp(x): a = np.max(x) return a + np.log(np.sum(np.exp(x - a))) x = np.array([1000, 1001, 1002]) print("不穩定計算:", np.log(np.sum(np.exp(x)))) # 會溢位 print("穩定計算:", log_sum_exp(x)) Gradient Clipping：防止梯度爆炸 深度網路訓練時，梯度可能因連乘而爆炸，導致參數更新異常。 Gradient Clipping：將梯度限制在某範圍內，常見於 RNN、深層網路。 import torch for param in model.parameters(): torch.nn.utils.clip_grad_norm_(param, max_norm=1.0) 稀疏矩陣存儲與運算加速 稀疏矩陣的意義 多數元素為 0 的矩陣，常見於 NLP、推薦系統、圖神經網路。 若用一般矩陣存儲，浪費大量記憶體與運算資源。 稀疏矩陣格式 CSR（Compressed Sparse Row）、CSC、COO 等格式。 支援高效的矩陣乘法、切片、轉置等操作。 from scipy.sparse import csr_matrix dense = np.zeros((1000, 1000)) dense[0, 1] = 3 sparse = csr_matrix(dense) print("稀疏矩陣非零元素數量:", sparse.nnz) 稀疏運算加速 PyTorch、TensorFlow 皆支援稀疏張量，適合大規模圖資料、推薦系統。 數值穩定性在 AI 實務的關鍵應用 Softmax 輸出層、交叉熵損失、注意力機制（Transformer）、RNN 訓練 生成模型（如 VAE）中的對數機率計算 大型稀疏圖的鄰接矩陣運算 常見面試熱點整理 熱點主題 面試常問問題 浮點誤差 為何 a+b-b 不等於 a？ Log-Sum-Exp 何時用？數學推導？ Gradient Clipping 什麼時候需要？ 稀疏矩陣 如何存儲與運算？ 使用注意事項 計算機精度有限，需主動檢查數值穩定性。 訓練深層網路時，建議預設啟用 Gradient Clipping。 稀疏矩陣適合高維低密度資料，能大幅節省資源。 延伸閱讀與資源 Scipy Sparse Matrix 官方文件 PyTorch 稀疏張量 Deep Learning Book: Numerical Computation 結語 數值計算與穩定性是 AI 實作不可忽視的細節。掌握浮點誤差、Log-Sum-Exp、Gradient Clipping 與稀疏矩陣運算，能讓你的模型更穩定、更高效，也能在面試與專案中展現專業素養。下一章將進入統計學在 ML 實務的應用，敬請期待！
'><meta property="og:title" content="數值計算與穩定性：AI 實作必備的數值技巧與陷阱"><meta property="og:description" content='數值計算是連接理論與實作的橋樑。再完美的數學模型，若忽略數值誤差與計算穩定性，實際運作時都可能出現爆炸或崩潰。本篇將深入探討 AI 常見的數值問題、穩定化技巧、稀疏運算加速，並結合理論、實作與面試重點，讓你在實戰中少踩坑。
浮點誤差、Underflow/Overflow 浮點數的本質與限制 電腦用有限位元表示實數，導致精度有限。 常見問題：加減極小數、極大數時精度損失。 Underflow/Overflow Underflow：數值太小被當成 0。 Overflow：數值太大超出表示範圍，變成無窮大或 NaN。 實例：浮點誤差 a = 1e16 b = 1.0 print("a + b - a =", (a + b) - a) # 理論上應為 1.0，實際上可能為 0.0 常見陷阱 連加小數、極端指數運算、極小機率相乘（如序列模型） Log-Sum-Exp Trick：避免數值爆炸的黃金法則 問題來源 Softmax、交叉熵等常用到 $\exp(x)$，大 x 易爆炸，小 x 易 underflow。 解法：Log-Sum-Exp $$ \log \sum_i e^{x_i} = a + \log \sum_i e^{x_i - a} $$
其中 $a = \max(x_i)$，可顯著提升數值穩定性。
import numpy as np def log_sum_exp(x): a = np.max(x) return a + np.log(np.sum(np.exp(x - a))) x = np.array([1000, 1001, 1002]) print("不穩定計算:", np.log(np.sum(np.exp(x)))) # 會溢位 print("穩定計算:", log_sum_exp(x)) Gradient Clipping：防止梯度爆炸 深度網路訓練時，梯度可能因連乘而爆炸，導致參數更新異常。 Gradient Clipping：將梯度限制在某範圍內，常見於 RNN、深層網路。 import torch for param in model.parameters(): torch.nn.utils.clip_grad_norm_(param, max_norm=1.0) 稀疏矩陣存儲與運算加速 稀疏矩陣的意義 多數元素為 0 的矩陣，常見於 NLP、推薦系統、圖神經網路。 若用一般矩陣存儲，浪費大量記憶體與運算資源。 稀疏矩陣格式 CSR（Compressed Sparse Row）、CSC、COO 等格式。 支援高效的矩陣乘法、切片、轉置等操作。 from scipy.sparse import csr_matrix dense = np.zeros((1000, 1000)) dense[0, 1] = 3 sparse = csr_matrix(dense) print("稀疏矩陣非零元素數量:", sparse.nnz) 稀疏運算加速 PyTorch、TensorFlow 皆支援稀疏張量，適合大規模圖資料、推薦系統。 數值穩定性在 AI 實務的關鍵應用 Softmax 輸出層、交叉熵損失、注意力機制（Transformer）、RNN 訓練 生成模型（如 VAE）中的對數機率計算 大型稀疏圖的鄰接矩陣運算 常見面試熱點整理 熱點主題 面試常問問題 浮點誤差 為何 a+b-b 不等於 a？ Log-Sum-Exp 何時用？數學推導？ Gradient Clipping 什麼時候需要？ 稀疏矩陣 如何存儲與運算？ 使用注意事項 計算機精度有限，需主動檢查數值穩定性。 訓練深層網路時，建議預設啟用 Gradient Clipping。 稀疏矩陣適合高維低密度資料，能大幅節省資源。 延伸閱讀與資源 Scipy Sparse Matrix 官方文件 PyTorch 稀疏張量 Deep Learning Book: Numerical Computation 結語 數值計算與穩定性是 AI 實作不可忽視的細節。掌握浮點誤差、Log-Sum-Exp、Gradient Clipping 與稀疏矩陣運算，能讓你的模型更穩定、更高效，也能在面試與專案中展現專業素養。下一章將進入統計學在 ML 實務的應用，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/numerical-computation-for-ai/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/numerical-computation-for-ai/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>數值計算與穩定性：AI 實作必備的數值技巧與陷阱</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>數值計算與穩定性：AI 實作必備的數值技巧與陷阱</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-06-21</span></div></header><div class=article-body><p>數值計算是連接理論與實作的橋樑。再完美的數學模型，若忽略數值誤差與計算穩定性，實際運作時都可能出現爆炸或崩潰。本篇將深入探討 AI 常見的數值問題、穩定化技巧、稀疏運算加速，並結合理論、實作與面試重點，讓你在實戰中少踩坑。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#浮點誤差underflowoverflow>浮點誤差、Underflow/Overflow</a><ul><li><a href=#浮點數的本質與限制>浮點數的本質與限制</a></li><li><a href=#underflowoverflow>Underflow/Overflow</a></li><li><a href=#常見陷阱>常見陷阱</a></li></ul></li><li><a href=#log-sum-exp-trick避免數值爆炸的黃金法則>Log-Sum-Exp Trick：避免數值爆炸的黃金法則</a><ul><li><a href=#問題來源>問題來源</a></li><li><a href=#解法log-sum-exp>解法：Log-Sum-Exp</a></li></ul></li><li><a href=#gradient-clipping防止梯度爆炸>Gradient Clipping：防止梯度爆炸</a></li><li><a href=#稀疏矩陣存儲與運算加速>稀疏矩陣存儲與運算加速</a><ul><li><a href=#稀疏矩陣的意義>稀疏矩陣的意義</a></li><li><a href=#稀疏矩陣格式>稀疏矩陣格式</a></li><li><a href=#稀疏運算加速>稀疏運算加速</a></li></ul></li><li><a href=#數值穩定性在-ai-實務的關鍵應用>數值穩定性在 AI 實務的關鍵應用</a></li><li><a href=#常見面試熱點整理>常見面試熱點整理</a></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=浮點誤差underflowoverflow>浮點誤差、Underflow/Overflow</h2><h3 id=浮點數的本質與限制>浮點數的本質與限制</h3><ul><li>電腦用有限位元表示實數，導致精度有限。</li><li>常見問題：加減極小數、極大數時精度損失。</li></ul><h3 id=underflowoverflow>Underflow/Overflow</h3><ul><li><strong>Underflow</strong>：數值太小被當成 0。</li><li><strong>Overflow</strong>：數值太大超出表示範圍，變成無窮大或 NaN。</li></ul><h4 id=實例浮點誤差>實例：浮點誤差</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>a <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e16</span>
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;a + b - a =&#34;</span>, (a <span style=color:#f92672>+</span> b) <span style=color:#f92672>-</span> a)  <span style=color:#75715e># 理論上應為 1.0，實際上可能為 0.0</span>
</span></span></code></pre></div><h3 id=常見陷阱>常見陷阱</h3><ul><li>連加小數、極端指數運算、極小機率相乘（如序列模型）</li></ul><hr><h2 id=log-sum-exp-trick避免數值爆炸的黃金法則>Log-Sum-Exp Trick：避免數值爆炸的黃金法則</h2><h3 id=問題來源>問題來源</h3><ul><li>Softmax、交叉熵等常用到 $\exp(x)$，大 x 易爆炸，小 x 易 underflow。</li></ul><h3 id=解法log-sum-exp>解法：Log-Sum-Exp</h3><p>$$
\log \sum_i e^{x_i} = a + \log \sum_i e^{x_i - a}
$$</p><p>其中 $a = \max(x_i)$，可顯著提升數值穩定性。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>log_sum_exp</span>(x):
</span></span><span style=display:flex><span>    a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>max(x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> a <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>log(np<span style=color:#f92672>.</span>sum(np<span style=color:#f92672>.</span>exp(x <span style=color:#f92672>-</span> a)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1000</span>, <span style=color:#ae81ff>1001</span>, <span style=color:#ae81ff>1002</span>])
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;不穩定計算:&#34;</span>, np<span style=color:#f92672>.</span>log(np<span style=color:#f92672>.</span>sum(np<span style=color:#f92672>.</span>exp(x))))  <span style=color:#75715e># 會溢位</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;穩定計算:&#34;</span>, log_sum_exp(x))
</span></span></code></pre></div><hr><h2 id=gradient-clipping防止梯度爆炸>Gradient Clipping：防止梯度爆炸</h2><ul><li>深度網路訓練時，梯度可能因連乘而爆炸，導致參數更新異常。</li><li><strong>Gradient Clipping</strong>：將梯度限制在某範圍內，常見於 RNN、深層網路。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>parameters():
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>clip_grad_norm_(param, max_norm<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>)
</span></span></code></pre></div><hr><h2 id=稀疏矩陣存儲與運算加速>稀疏矩陣存儲與運算加速</h2><h3 id=稀疏矩陣的意義>稀疏矩陣的意義</h3><ul><li>多數元素為 0 的矩陣，常見於 NLP、推薦系統、圖神經網路。</li><li>若用一般矩陣存儲，浪費大量記憶體與運算資源。</li></ul><h3 id=稀疏矩陣格式>稀疏矩陣格式</h3><ul><li>CSR（Compressed Sparse Row）、CSC、COO 等格式。</li><li>支援高效的矩陣乘法、切片、轉置等操作。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> scipy.sparse <span style=color:#f92672>import</span> csr_matrix
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dense <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1000</span>, <span style=color:#ae81ff>1000</span>))
</span></span><span style=display:flex><span>dense[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>sparse <span style=color:#f92672>=</span> csr_matrix(dense)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;稀疏矩陣非零元素數量:&#34;</span>, sparse<span style=color:#f92672>.</span>nnz)
</span></span></code></pre></div><h3 id=稀疏運算加速>稀疏運算加速</h3><ul><li>PyTorch、TensorFlow 皆支援稀疏張量，適合大規模圖資料、推薦系統。</li></ul><hr><h2 id=數值穩定性在-ai-實務的關鍵應用>數值穩定性在 AI 實務的關鍵應用</h2><ul><li>Softmax 輸出層、交叉熵損失、注意力機制（Transformer）、RNN 訓練</li><li>生成模型（如 VAE）中的對數機率計算</li><li>大型稀疏圖的鄰接矩陣運算</li></ul><hr><h2 id=常見面試熱點整理>常見面試熱點整理</h2><table><thead><tr><th>熱點主題</th><th>面試常問問題</th></tr></thead><tbody><tr><td>浮點誤差</td><td>為何 a+b-b 不等於 a？</td></tr><tr><td>Log-Sum-Exp</td><td>何時用？數學推導？</td></tr><tr><td>Gradient Clipping</td><td>什麼時候需要？</td></tr><tr><td>稀疏矩陣</td><td>如何存儲與運算？</td></tr></tbody></table><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>計算機精度有限，需主動檢查數值穩定性。</li><li>訓練深層網路時，建議預設啟用 Gradient Clipping。</li><li>稀疏矩陣適合高維低密度資料，能大幅節省資源。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href=https://docs.scipy.org/doc/scipy/reference/sparse.html>Scipy Sparse Matrix 官方文件</a></li><li><a href=https://pytorch.org/docs/stable/sparse.html>PyTorch 稀疏張量</a></li><li><a href=https://www.deeplearningbook.org/contents/numerical.html>Deep Learning Book: Numerical Computation</a></li></ul><hr><h2 id=結語>結語</h2><p>數值計算與穩定性是 AI 實作不可忽視的細節。掌握浮點誤差、Log-Sum-Exp、Gradient Clipping 與稀疏矩陣運算，能讓你的模型更穩定、更高效，也能在面試與專案中展現專業素養。下一章將進入統計學在 ML 實務的應用，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>Log-Sum-Exp</span>
<span class=tag>Gradient Clipping</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>