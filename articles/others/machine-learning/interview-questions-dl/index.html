<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>深度學習挑戰題庫：經典口試與白板題 - Yu's Portfolio & Learning Hub</title><meta name=description content="本章彙整深度學習主題的經典面試題，每章精選 10-15 題，涵蓋理論推導、實作、直覺解釋與白板題。每題附上解法提示與常見誤區，幫助你在面試與實戰中脫穎而出。
DL1 前菜：感知機 → MLP 感知機學習規則與收斂條件？ 為何感知機無法解 XOR 問題？ Sigmoid、Tanh、ReLU、GELU 優缺點比較？ MLP 為何能逼近任意連續函數？ 如何計算 MLP 參數量？ 激活函數選擇對訓練有何影響？ 如何用 Python 實作簡單感知機/MLP？ MLP 過擬合時有哪些解法？ ReLU 死神經元問題如何緩解？ GELU 為何在 Transformer 中表現佳？ DL2 卷積網路 (CNN) 精要 卷積層參數量如何計算？ 池化層有何作用？何時用 Max/Avg？ ResNet 殘差連接數學推導？ EfficientNet 複合縮放如何設計？ Inception Block 為何能捕捉多尺度特徵？ TCN 與 RNN 差異？ 轉置卷積常見問題與解法？ CNN 在 NLP 的應用有哪些？ 如何用 Python 實作簡單 CNN？ DenseNet 為何特徵重用？ DL3 循環 & 序列模型 (RNN) RNN 為何會梯度爆炸/消失？數學推導？ LSTM/GRU 閘門結構與公式？ Seq2Seq 架構與應用場景？ Attention 機制數學原理？ Bi-RNN 有何優勢？ Teacher Forcing 與 Scheduled Sampling 差異？ 如何用 Python 實作 LSTM/GRU？ RNN 在 NLP/時序預測的應用？ Exposure Bias 是什麼？如何緩解？ Seq2Seq 未加 Attention 有何缺點？ DL4 Attention 機制拆解 Scaled Dot-Product Attention 數學推導？ Multi-Head Attention 如何提升表達力？ Q/K/V 的幾何直覺？ Self-Attention 與傳統 RNN 差異？ Masking 有哪些類型？如何實作？ 如何用 Python 實作簡單 Attention？ 為何要做縮放？有何數值意義？ Multi-Head 拼接與線性變換細節？ Attention 如何捕捉長距依賴？ Masking 實作錯誤會有什麼後果？ DL5 Transformer 家族 Encoder/Decoder Block 結構與差異？ Sinusoid/ALiBi/RoPE 位置編碼原理？ Self-Attention 複雜度如何優化？ BERT 與 GPT 架構與訓練差異？ DeiT/Swin 在 Vision Transformer 的創新？ 長序列 Transformer 如何設計？ 位置編碼缺失會有什麼問題？ 如何用 Python 實作位置編碼？ Encoder/Decoder Block 混用會有什麼後果？ FlashAttention 有何優勢？ DL6 預訓練策略 & 微調 預訓練與從零訓練的收斂差異？ Feature-based、Fine-tune、Prompt-tune 差異？ Llama-2/3 微調的資源瓶頸？ AMP、梯度累積的原理與作用？ 如何選擇微調策略？ 微調過程如何避免過擬合？ Prompt-tune 適合哪些場景？ 如何用 Python 微調 Llama？ 微調資料格式化注意事項？ PEFT 技巧有哪些？ DL7 參數高效微調 (PEFT) LoRA/QLoRA 的數學原理與優缺點？ Adapter 結構與多任務切換？ Prefix/P-Tuning 適用場景與限制？ Rank/α 如何選擇與調參？ QLoRA 量化的數值風險？ 如何用 Python 實作 LoRA/QLoRA？ PEFT 與全參數微調的比較？ 多任務微調如何設計 Adapter？ 量化模型推論時需注意什麼？ PEFT 技巧在產業落地的挑戰？ DL8 生成模型百花齊放 GAN 損失函數與訓練技巧？ VAE 的 ELBO 推導與 KL 項作用？ Diffusion Model 的正向/反向過程？ Flow-based Model 如何保證可逆？ ControlNet 如何提升可控性？ 生成模型如何評估品質？ GAN mode collapse 如何解決？ Diffusion 推理加速方法？ VAE 潛在空間設計原則？ 如何用 Python 實作簡單 GAN/VAE？ DL9 正規化 & 訓練技巧 BatchNorm、LayerNorm、GroupNorm、RMSNorm 差異？ 殘差連接如何幫助深層網路訓練？ Dropout 的數學原理與推論差異？ Stochastic Depth/DropPath 適用場景？ Label Smoothing 有何優缺點？ MixUp/CutMix 如何提升泛化？ 如何用 Python 實作正規化與資料增強？ BatchNorm 在推論時如何運作？ Dropout/MixUp 過度使用會有什麼問題？ 正規化與訓練技巧如何組合應用？ DL10 加速 & 壓縮實戰 AMP 的原理與數值風險？ 知識蒸餾如何設計 Teacher/Student？ QAT 與 Post-training Quantization 差異？ TensorRT/ONNX 如何加速推論？ Flash-Attention 計算複雜度與優勢？ Edge AI 部署常見挑戰？ Streaming 生成的應用與限制？ 如何用 Python 實作 AMP/QAT？ 量化後精度下降如何調整？ 推論優化與壓縮技術如何組合應用？ DL11 多模態 & 視覺語言 CLIP 的對比學習損失如何設計？ BLIP-2 架構與 Q-Former 作用？ LLaVA 如何實現圖文對話？ Cross-Attention 權重共享的優缺點？ 多模態模型如何對齊影像與文本特徵？ CLIP/BLIP-2/LLaVA 輸入格式設計？ 多模態應用的資料挑戰？ 如何用 Python 實作 Cross-Attention？ 權重共享會有什麼風險？ 多模態模型在醫療/推薦的應用？ 解題技巧與常見誤區 計算題：先寫公式再帶數字，避免粗心。 推導題：分步驟寫清楚，標明假設。 直覺題：用圖解、生活例子輔助說明。 實作題：熟悉 numpy、torch、transformers 等常用 API。 常見誤區：混淆定義、忽略假設、過度依賴單一指標。 結語 本題庫涵蓋深度學習經典面試題與解法直覺。建議每題都動手推導、實作與解釋，並多練習口頭表達。祝你面試順利、學習愉快！
"><meta property="og:title" content="深度學習挑戰題庫：經典口試與白板題"><meta property="og:description" content="本章彙整深度學習主題的經典面試題，每章精選 10-15 題，涵蓋理論推導、實作、直覺解釋與白板題。每題附上解法提示與常見誤區，幫助你在面試與實戰中脫穎而出。
DL1 前菜：感知機 → MLP 感知機學習規則與收斂條件？ 為何感知機無法解 XOR 問題？ Sigmoid、Tanh、ReLU、GELU 優缺點比較？ MLP 為何能逼近任意連續函數？ 如何計算 MLP 參數量？ 激活函數選擇對訓練有何影響？ 如何用 Python 實作簡單感知機/MLP？ MLP 過擬合時有哪些解法？ ReLU 死神經元問題如何緩解？ GELU 為何在 Transformer 中表現佳？ DL2 卷積網路 (CNN) 精要 卷積層參數量如何計算？ 池化層有何作用？何時用 Max/Avg？ ResNet 殘差連接數學推導？ EfficientNet 複合縮放如何設計？ Inception Block 為何能捕捉多尺度特徵？ TCN 與 RNN 差異？ 轉置卷積常見問題與解法？ CNN 在 NLP 的應用有哪些？ 如何用 Python 實作簡單 CNN？ DenseNet 為何特徵重用？ DL3 循環 & 序列模型 (RNN) RNN 為何會梯度爆炸/消失？數學推導？ LSTM/GRU 閘門結構與公式？ Seq2Seq 架構與應用場景？ Attention 機制數學原理？ Bi-RNN 有何優勢？ Teacher Forcing 與 Scheduled Sampling 差異？ 如何用 Python 實作 LSTM/GRU？ RNN 在 NLP/時序預測的應用？ Exposure Bias 是什麼？如何緩解？ Seq2Seq 未加 Attention 有何缺點？ DL4 Attention 機制拆解 Scaled Dot-Product Attention 數學推導？ Multi-Head Attention 如何提升表達力？ Q/K/V 的幾何直覺？ Self-Attention 與傳統 RNN 差異？ Masking 有哪些類型？如何實作？ 如何用 Python 實作簡單 Attention？ 為何要做縮放？有何數值意義？ Multi-Head 拼接與線性變換細節？ Attention 如何捕捉長距依賴？ Masking 實作錯誤會有什麼後果？ DL5 Transformer 家族 Encoder/Decoder Block 結構與差異？ Sinusoid/ALiBi/RoPE 位置編碼原理？ Self-Attention 複雜度如何優化？ BERT 與 GPT 架構與訓練差異？ DeiT/Swin 在 Vision Transformer 的創新？ 長序列 Transformer 如何設計？ 位置編碼缺失會有什麼問題？ 如何用 Python 實作位置編碼？ Encoder/Decoder Block 混用會有什麼後果？ FlashAttention 有何優勢？ DL6 預訓練策略 & 微調 預訓練與從零訓練的收斂差異？ Feature-based、Fine-tune、Prompt-tune 差異？ Llama-2/3 微調的資源瓶頸？ AMP、梯度累積的原理與作用？ 如何選擇微調策略？ 微調過程如何避免過擬合？ Prompt-tune 適合哪些場景？ 如何用 Python 微調 Llama？ 微調資料格式化注意事項？ PEFT 技巧有哪些？ DL7 參數高效微調 (PEFT) LoRA/QLoRA 的數學原理與優缺點？ Adapter 結構與多任務切換？ Prefix/P-Tuning 適用場景與限制？ Rank/α 如何選擇與調參？ QLoRA 量化的數值風險？ 如何用 Python 實作 LoRA/QLoRA？ PEFT 與全參數微調的比較？ 多任務微調如何設計 Adapter？ 量化模型推論時需注意什麼？ PEFT 技巧在產業落地的挑戰？ DL8 生成模型百花齊放 GAN 損失函數與訓練技巧？ VAE 的 ELBO 推導與 KL 項作用？ Diffusion Model 的正向/反向過程？ Flow-based Model 如何保證可逆？ ControlNet 如何提升可控性？ 生成模型如何評估品質？ GAN mode collapse 如何解決？ Diffusion 推理加速方法？ VAE 潛在空間設計原則？ 如何用 Python 實作簡單 GAN/VAE？ DL9 正規化 & 訓練技巧 BatchNorm、LayerNorm、GroupNorm、RMSNorm 差異？ 殘差連接如何幫助深層網路訓練？ Dropout 的數學原理與推論差異？ Stochastic Depth/DropPath 適用場景？ Label Smoothing 有何優缺點？ MixUp/CutMix 如何提升泛化？ 如何用 Python 實作正規化與資料增強？ BatchNorm 在推論時如何運作？ Dropout/MixUp 過度使用會有什麼問題？ 正規化與訓練技巧如何組合應用？ DL10 加速 & 壓縮實戰 AMP 的原理與數值風險？ 知識蒸餾如何設計 Teacher/Student？ QAT 與 Post-training Quantization 差異？ TensorRT/ONNX 如何加速推論？ Flash-Attention 計算複雜度與優勢？ Edge AI 部署常見挑戰？ Streaming 生成的應用與限制？ 如何用 Python 實作 AMP/QAT？ 量化後精度下降如何調整？ 推論優化與壓縮技術如何組合應用？ DL11 多模態 & 視覺語言 CLIP 的對比學習損失如何設計？ BLIP-2 架構與 Q-Former 作用？ LLaVA 如何實現圖文對話？ Cross-Attention 權重共享的優缺點？ 多模態模型如何對齊影像與文本特徵？ CLIP/BLIP-2/LLaVA 輸入格式設計？ 多模態應用的資料挑戰？ 如何用 Python 實作 Cross-Attention？ 權重共享會有什麼風險？ 多模態模型在醫療/推薦的應用？ 解題技巧與常見誤區 計算題：先寫公式再帶數字，避免粗心。 推導題：分步驟寫清楚，標明假設。 直覺題：用圖解、生活例子輔助說明。 實作題：熟悉 numpy、torch、transformers 等常用 API。 常見誤區：混淆定義、忽略假設、過度依賴單一指標。 結語 本題庫涵蓋深度學習經典面試題與解法直覺。建議每題都動手推導、實作與解釋，並多練習口頭表達。祝你面試順利、學習愉快！
"><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/interview-questions-dl/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/interview-questions-dl/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>深度學習挑戰題庫：經典口試與白板題</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>深度學習挑戰題庫：經典口試與白板題</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-04-17</span></div></header><div class=article-body><p>本章彙整深度學習主題的經典面試題，每章精選 10-15 題，涵蓋理論推導、實作、直覺解釋與白板題。每題附上解法提示與常見誤區，幫助你在面試與實戰中脫穎而出。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#dl1-前菜感知機--mlp>DL1 前菜：感知機 → MLP</a></li><li><a href=#dl2-卷積網路-cnn-精要>DL2 卷積網路 (CNN) 精要</a></li><li><a href=#dl3-循環--序列模型-rnn>DL3 循環 & 序列模型 (RNN)</a></li><li><a href=#dl4-attention-機制拆解>DL4 Attention 機制拆解</a></li><li><a href=#dl5-transformer-家族>DL5 Transformer 家族</a></li><li><a href=#dl6-預訓練策略--微調>DL6 預訓練策略 & 微調</a></li><li><a href=#dl7-參數高效微調-peft>DL7 參數高效微調 (PEFT)</a></li><li><a href=#dl8-生成模型百花齊放>DL8 生成模型百花齊放</a></li><li><a href=#dl9-正規化--訓練技巧>DL9 正規化 & 訓練技巧</a></li><li><a href=#dl10-加速--壓縮實戰>DL10 加速 & 壓縮實戰</a></li><li><a href=#dl11-多模態--視覺語言>DL11 多模態 & 視覺語言</a></li><li><a href=#解題技巧與常見誤區>解題技巧與常見誤區</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=dl1-前菜感知機--mlp>DL1 前菜：感知機 → MLP</h2><ol><li>感知機學習規則與收斂條件？</li><li>為何感知機無法解 XOR 問題？</li><li>Sigmoid、Tanh、ReLU、GELU 優缺點比較？</li><li>MLP 為何能逼近任意連續函數？</li><li>如何計算 MLP 參數量？</li><li>激活函數選擇對訓練有何影響？</li><li>如何用 Python 實作簡單感知機/MLP？</li><li>MLP 過擬合時有哪些解法？</li><li>ReLU 死神經元問題如何緩解？</li><li>GELU 為何在 Transformer 中表現佳？</li></ol><hr><h2 id=dl2-卷積網路-cnn-精要>DL2 卷積網路 (CNN) 精要</h2><ol><li>卷積層參數量如何計算？</li><li>池化層有何作用？何時用 Max/Avg？</li><li>ResNet 殘差連接數學推導？</li><li>EfficientNet 複合縮放如何設計？</li><li>Inception Block 為何能捕捉多尺度特徵？</li><li>TCN 與 RNN 差異？</li><li>轉置卷積常見問題與解法？</li><li>CNN 在 NLP 的應用有哪些？</li><li>如何用 Python 實作簡單 CNN？</li><li>DenseNet 為何特徵重用？</li></ol><hr><h2 id=dl3-循環--序列模型-rnn>DL3 循環 & 序列模型 (RNN)</h2><ol><li>RNN 為何會梯度爆炸/消失？數學推導？</li><li>LSTM/GRU 閘門結構與公式？</li><li>Seq2Seq 架構與應用場景？</li><li>Attention 機制數學原理？</li><li>Bi-RNN 有何優勢？</li><li>Teacher Forcing 與 Scheduled Sampling 差異？</li><li>如何用 Python 實作 LSTM/GRU？</li><li>RNN 在 NLP/時序預測的應用？</li><li>Exposure Bias 是什麼？如何緩解？</li><li>Seq2Seq 未加 Attention 有何缺點？</li></ol><hr><h2 id=dl4-attention-機制拆解>DL4 Attention 機制拆解</h2><ol><li>Scaled Dot-Product Attention 數學推導？</li><li>Multi-Head Attention 如何提升表達力？</li><li>Q/K/V 的幾何直覺？</li><li>Self-Attention 與傳統 RNN 差異？</li><li>Masking 有哪些類型？如何實作？</li><li>如何用 Python 實作簡單 Attention？</li><li>為何要做縮放？有何數值意義？</li><li>Multi-Head 拼接與線性變換細節？</li><li>Attention 如何捕捉長距依賴？</li><li>Masking 實作錯誤會有什麼後果？</li></ol><hr><h2 id=dl5-transformer-家族>DL5 Transformer 家族</h2><ol><li>Encoder/Decoder Block 結構與差異？</li><li>Sinusoid/ALiBi/RoPE 位置編碼原理？</li><li>Self-Attention 複雜度如何優化？</li><li>BERT 與 GPT 架構與訓練差異？</li><li>DeiT/Swin 在 Vision Transformer 的創新？</li><li>長序列 Transformer 如何設計？</li><li>位置編碼缺失會有什麼問題？</li><li>如何用 Python 實作位置編碼？</li><li>Encoder/Decoder Block 混用會有什麼後果？</li><li>FlashAttention 有何優勢？</li></ol><hr><h2 id=dl6-預訓練策略--微調>DL6 預訓練策略 & 微調</h2><ol><li>預訓練與從零訓練的收斂差異？</li><li>Feature-based、Fine-tune、Prompt-tune 差異？</li><li>Llama-2/3 微調的資源瓶頸？</li><li>AMP、梯度累積的原理與作用？</li><li>如何選擇微調策略？</li><li>微調過程如何避免過擬合？</li><li>Prompt-tune 適合哪些場景？</li><li>如何用 Python 微調 Llama？</li><li>微調資料格式化注意事項？</li><li>PEFT 技巧有哪些？</li></ol><hr><h2 id=dl7-參數高效微調-peft>DL7 參數高效微調 (PEFT)</h2><ol><li>LoRA/QLoRA 的數學原理與優缺點？</li><li>Adapter 結構與多任務切換？</li><li>Prefix/P-Tuning 適用場景與限制？</li><li>Rank/α 如何選擇與調參？</li><li>QLoRA 量化的數值風險？</li><li>如何用 Python 實作 LoRA/QLoRA？</li><li>PEFT 與全參數微調的比較？</li><li>多任務微調如何設計 Adapter？</li><li>量化模型推論時需注意什麼？</li><li>PEFT 技巧在產業落地的挑戰？</li></ol><hr><h2 id=dl8-生成模型百花齊放>DL8 生成模型百花齊放</h2><ol><li>GAN 損失函數與訓練技巧？</li><li>VAE 的 ELBO 推導與 KL 項作用？</li><li>Diffusion Model 的正向/反向過程？</li><li>Flow-based Model 如何保證可逆？</li><li>ControlNet 如何提升可控性？</li><li>生成模型如何評估品質？</li><li>GAN mode collapse 如何解決？</li><li>Diffusion 推理加速方法？</li><li>VAE 潛在空間設計原則？</li><li>如何用 Python 實作簡單 GAN/VAE？</li></ol><hr><h2 id=dl9-正規化--訓練技巧>DL9 正規化 & 訓練技巧</h2><ol><li>BatchNorm、LayerNorm、GroupNorm、RMSNorm 差異？</li><li>殘差連接如何幫助深層網路訓練？</li><li>Dropout 的數學原理與推論差異？</li><li>Stochastic Depth/DropPath 適用場景？</li><li>Label Smoothing 有何優缺點？</li><li>MixUp/CutMix 如何提升泛化？</li><li>如何用 Python 實作正規化與資料增強？</li><li>BatchNorm 在推論時如何運作？</li><li>Dropout/MixUp 過度使用會有什麼問題？</li><li>正規化與訓練技巧如何組合應用？</li></ol><hr><h2 id=dl10-加速--壓縮實戰>DL10 加速 & 壓縮實戰</h2><ol><li>AMP 的原理與數值風險？</li><li>知識蒸餾如何設計 Teacher/Student？</li><li>QAT 與 Post-training Quantization 差異？</li><li>TensorRT/ONNX 如何加速推論？</li><li>Flash-Attention 計算複雜度與優勢？</li><li>Edge AI 部署常見挑戰？</li><li>Streaming 生成的應用與限制？</li><li>如何用 Python 實作 AMP/QAT？</li><li>量化後精度下降如何調整？</li><li>推論優化與壓縮技術如何組合應用？</li></ol><hr><h2 id=dl11-多模態--視覺語言>DL11 多模態 & 視覺語言</h2><ol><li>CLIP 的對比學習損失如何設計？</li><li>BLIP-2 架構與 Q-Former 作用？</li><li>LLaVA 如何實現圖文對話？</li><li>Cross-Attention 權重共享的優缺點？</li><li>多模態模型如何對齊影像與文本特徵？</li><li>CLIP/BLIP-2/LLaVA 輸入格式設計？</li><li>多模態應用的資料挑戰？</li><li>如何用 Python 實作 Cross-Attention？</li><li>權重共享會有什麼風險？</li><li>多模態模型在醫療/推薦的應用？</li></ol><hr><h2 id=解題技巧與常見誤區>解題技巧與常見誤區</h2><ul><li><strong>計算題</strong>：先寫公式再帶數字，避免粗心。</li><li><strong>推導題</strong>：分步驟寫清楚，標明假設。</li><li><strong>直覺題</strong>：用圖解、生活例子輔助說明。</li><li><strong>實作題</strong>：熟悉 numpy、torch、transformers 等常用 API。</li><li><strong>常見誤區</strong>：混淆定義、忽略假設、過度依賴單一指標。</li></ul><hr><h2 id=結語>結語</h2><p>本題庫涵蓋深度學習經典面試題與解法直覺。建議每題都動手推導、實作與解釋，並多練習口頭表達。祝你面試順利、學習愉快！</p></div><div class=article-tags><div class=tags><span class=tag>interview</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>