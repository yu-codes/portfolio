<!doctype html><html lang=en class=html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>經典迴歸模型全攻略：線性、Ridge、Lasso、Logistic 與資料處理 - Yu's Portfolio & Learning Hub</title><meta name=description content='迴歸模型是機器學習的入門與核心。從最基礎的線性迴歸，到正則化的 Ridge/Lasso、再到分類用的 Logistic Regression，這些模型不僅是面試常客，也是實務專案的基石。本章將深入數學推導、直覺圖解、Python 實作、資料處理技巧與面試熱點，幫助你全面掌握迴歸模型。
線性迴歸與多項式迴歸 線性迴歸（Linear Regression） 假設 $y = X\beta + \epsilon$，最小化殘差平方和。 封閉解：$\hat{\beta} = (X^TX)^{-1}X^Ty$ 適用於連續型目標預測。 多項式迴歸（Polynomial Regression） 將特徵升維，擬合非線性關係。 容易過擬合，需正則化或交叉驗證。 import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures X = np.linspace(0, 1, 100)[:, None] y = 2 * X.ravel() + 0.5 + np.random.randn(100) * 0.1 # 線性迴歸 lr = LinearRegression().fit(X, y) plt.plot(X, lr.predict(X), label="Linear") # 多項式迴歸 poly = PolynomialFeatures(4) X_poly = poly.fit_transform(X) lr_poly = LinearRegression().fit(X_poly, y) plt.plot(X, lr_poly.predict(X_poly), label="Poly deg=4") plt.scatter(X, y, s=10, color=&#39;black&#39;) plt.legend(); plt.title("Linear vs Polynomial Regression"); plt.show() Ridge、Lasso、Elastic Net 正則化 Ridge Regression（L2 正則化） 加入懲罰項 $\lambda |\beta|^2$，抑制參數過大。 適合多重共線性、特徵多的情境。 Lasso Regression（L1 正則化） 懲罰項 $\lambda |\beta|_1$，可做特徵選擇（產生稀疏解）。 適合特徵選擇需求。 Elastic Net 結合 L1 與 L2，兼顧特徵選擇與穩定性。 from sklearn.linear_model import Ridge, Lasso, ElasticNet ridge = Ridge(alpha=1.0).fit(X, y) lasso = Lasso(alpha=0.1).fit(X, y) enet = ElasticNet(alpha=0.1, l1_ratio=0.5).fit(X, y) print("Ridge 係數:", ridge.coef_) print("Lasso 係數:", lasso.coef_) print("ElasticNet 係數:", enet.coef_) Logistic & Softmax Regression Logistic Regression（二元分類） 預測機率 $P(y=1|x) = \sigma(x^T\beta)$，$\sigma$ 為 sigmoid。 損失函數為交叉熵，常用於二元分類。 Softmax Regression（多元分類） 將 sigmoid 擴展為 softmax，適用於多類別。 輸出每類別的機率，損失同為交叉熵。 from sklearn.linear_model import LogisticRegression X_cls = np.random.randn(200, 2) y_cls = (X_cls[:, 0] + X_cls[:, 1] > 0).astype(int) clf = LogisticRegression().fit(X_cls, y_cls) print("預測機率:", clf.predict_proba(X_cls[:5])) print("預測標籤:", clf.predict(X_cls[:5])) 偏態資料的對數轉換與重抽樣 偏態資料（Skewed Data） 常見於收入、房價等資料，右偏或左偏。 直接建模易受極端值影響。 對數轉換（Log Transform） 將偏態資料轉為近似常態，提升模型穩定性。 注意：資料需大於 0。 重抽樣（Resampling） 欠抽樣（Undersampling）：減少多數類樣本。 過抽樣（Oversampling）：增加少數類樣本（如 SMOTE）。 import pandas as pd data = pd.DataFrame({&#39;income&#39;: np.random.exponential(50000, 1000)}) data[&#39;log_income&#39;] = np.log1p(data[&#39;income&#39;]) data[&#39;income&#39;].hist(alpha=0.5, label=&#39;Original&#39;) data[&#39;log_income&#39;].hist(alpha=0.5, label=&#39;Log-Transformed&#39;) plt.legend(); plt.title("Skewed Data Log Transform"); plt.show() 面試熱點與常見誤區 面試熱點 主題 常見問題 Ridge/Lasso 何時選用？數學差異？ Logistic Regression 為何用交叉熵？如何推導？ 多項式迴歸 如何避免過擬合？ 偏態資料處理 何時用對數轉換？有何風險？ 重抽樣 何時用 SMOTE？有何缺點？ 常見誤區 忽略特徵標準化對正則化模型的影響。 Lasso 係數全為 0 可能是 alpha 太大。 Logistic Regression 不適合極端不平衡資料。 對數轉換後忘記逆轉換預測結果。 使用注意事項 正則化模型需標準化特徵，避免懲罰不公平。 多項式迴歸易過擬合，建議搭配交叉驗證。 處理偏態資料時，注意資料分布與業務意義。 延伸閱讀與資源 StatQuest: Ridge, Lasso, Elastic Net Scikit-learn Regression Models Logistic Regression 推導 Imbalanced-learn 官方文件 結語 經典迴歸模型是機器學習的基礎。熟悉線性、Ridge、Lasso、Logistic Regression 與資料處理技巧，能讓你在面試與實務中游刃有餘。下一章將進入分類演算法百寶箱，敬請期待！
'><meta property="og:title" content="經典迴歸模型全攻略：線性、Ridge、Lasso、Logistic 與資料處理"><meta property="og:description" content='迴歸模型是機器學習的入門與核心。從最基礎的線性迴歸，到正則化的 Ridge/Lasso、再到分類用的 Logistic Regression，這些模型不僅是面試常客，也是實務專案的基石。本章將深入數學推導、直覺圖解、Python 實作、資料處理技巧與面試熱點，幫助你全面掌握迴歸模型。
線性迴歸與多項式迴歸 線性迴歸（Linear Regression） 假設 $y = X\beta + \epsilon$，最小化殘差平方和。 封閉解：$\hat{\beta} = (X^TX)^{-1}X^Ty$ 適用於連續型目標預測。 多項式迴歸（Polynomial Regression） 將特徵升維，擬合非線性關係。 容易過擬合，需正則化或交叉驗證。 import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures X = np.linspace(0, 1, 100)[:, None] y = 2 * X.ravel() + 0.5 + np.random.randn(100) * 0.1 # 線性迴歸 lr = LinearRegression().fit(X, y) plt.plot(X, lr.predict(X), label="Linear") # 多項式迴歸 poly = PolynomialFeatures(4) X_poly = poly.fit_transform(X) lr_poly = LinearRegression().fit(X_poly, y) plt.plot(X, lr_poly.predict(X_poly), label="Poly deg=4") plt.scatter(X, y, s=10, color=&#39;black&#39;) plt.legend(); plt.title("Linear vs Polynomial Regression"); plt.show() Ridge、Lasso、Elastic Net 正則化 Ridge Regression（L2 正則化） 加入懲罰項 $\lambda |\beta|^2$，抑制參數過大。 適合多重共線性、特徵多的情境。 Lasso Regression（L1 正則化） 懲罰項 $\lambda |\beta|_1$，可做特徵選擇（產生稀疏解）。 適合特徵選擇需求。 Elastic Net 結合 L1 與 L2，兼顧特徵選擇與穩定性。 from sklearn.linear_model import Ridge, Lasso, ElasticNet ridge = Ridge(alpha=1.0).fit(X, y) lasso = Lasso(alpha=0.1).fit(X, y) enet = ElasticNet(alpha=0.1, l1_ratio=0.5).fit(X, y) print("Ridge 係數:", ridge.coef_) print("Lasso 係數:", lasso.coef_) print("ElasticNet 係數:", enet.coef_) Logistic & Softmax Regression Logistic Regression（二元分類） 預測機率 $P(y=1|x) = \sigma(x^T\beta)$，$\sigma$ 為 sigmoid。 損失函數為交叉熵，常用於二元分類。 Softmax Regression（多元分類） 將 sigmoid 擴展為 softmax，適用於多類別。 輸出每類別的機率，損失同為交叉熵。 from sklearn.linear_model import LogisticRegression X_cls = np.random.randn(200, 2) y_cls = (X_cls[:, 0] + X_cls[:, 1] > 0).astype(int) clf = LogisticRegression().fit(X_cls, y_cls) print("預測機率:", clf.predict_proba(X_cls[:5])) print("預測標籤:", clf.predict(X_cls[:5])) 偏態資料的對數轉換與重抽樣 偏態資料（Skewed Data） 常見於收入、房價等資料，右偏或左偏。 直接建模易受極端值影響。 對數轉換（Log Transform） 將偏態資料轉為近似常態，提升模型穩定性。 注意：資料需大於 0。 重抽樣（Resampling） 欠抽樣（Undersampling）：減少多數類樣本。 過抽樣（Oversampling）：增加少數類樣本（如 SMOTE）。 import pandas as pd data = pd.DataFrame({&#39;income&#39;: np.random.exponential(50000, 1000)}) data[&#39;log_income&#39;] = np.log1p(data[&#39;income&#39;]) data[&#39;income&#39;].hist(alpha=0.5, label=&#39;Original&#39;) data[&#39;log_income&#39;].hist(alpha=0.5, label=&#39;Log-Transformed&#39;) plt.legend(); plt.title("Skewed Data Log Transform"); plt.show() 面試熱點與常見誤區 面試熱點 主題 常見問題 Ridge/Lasso 何時選用？數學差異？ Logistic Regression 為何用交叉熵？如何推導？ 多項式迴歸 如何避免過擬合？ 偏態資料處理 何時用對數轉換？有何風險？ 重抽樣 何時用 SMOTE？有何缺點？ 常見誤區 忽略特徵標準化對正則化模型的影響。 Lasso 係數全為 0 可能是 alpha 太大。 Logistic Regression 不適合極端不平衡資料。 對數轉換後忘記逆轉換預測結果。 使用注意事項 正則化模型需標準化特徵，避免懲罰不公平。 多項式迴歸易過擬合，建議搭配交叉驗證。 處理偏態資料時，注意資料分布與業務意義。 延伸閱讀與資源 StatQuest: Ridge, Lasso, Elastic Net Scikit-learn Regression Models Logistic Regression 推導 Imbalanced-learn 官方文件 結語 經典迴歸模型是機器學習的基礎。熟悉線性、Ridge、Lasso、Logistic Regression 與資料處理技巧，能讓你在面試與實務中游刃有餘。下一章將進入分類演算法百寶箱，敬請期待！
'><meta property="og:url" content="https://yu-codes.github.io/portfolio/articles/others/machine-learning/regression-models/"><link rel=canonical href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/regression-models/><link rel=stylesheet href=https://yu-codes.github.io/portfolio/css/main.css></head><body class=body><div class=container><aside class=sidebar><div class=sidebar-header><h1 class=name>Yu</h1><p class=tagline>Developer, dreamer, debugger.</p></div><nav class=sidebar-nav><a href=https://yu-codes.github.io/portfolio/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
<span>HOME</span>
</a><a href=https://yu-codes.github.io/portfolio/articles/ class="nav-item active"><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5.0 016.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5.0 014 19.5v-15A2.5 2.5.0 016.5 2z"/></svg>
<span>ARTICLES</span>
</a><a href=https://yu-codes.github.io/portfolio/resume/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="12" y1="11" x2="12" y2="17"/><line x1="9" y1="14" x2="15" y2="14"/></svg>
<span>RESUME</span>
</a><a href=https://yu-codes.github.io/portfolio/projects/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="8" height="8"/><rect x="13" y="3" width="8" height="8"/><rect x="3" y="13" width="8" height="8"/><rect x="13" y="13" width="8" height="8"/></svg>
<span>PROJECTS</span>
</a><a href=https://yu-codes.github.io/portfolio/archives/ class=nav-item><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
<span>ARCHIVES</span></a></nav><div class=sidebar-footer><div class=controls><button class="control-btn theme-toggle" title="Toggle theme (light/dark)" aria-label="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".25"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="1.5" fill="none"/><path d="M12 3a9 9 0 000 18" fill="currentColor" opacity=".9"/><path d="M12 3a9 9 0 010 18" fill="currentColor" opacity=".25"/></svg>
</button>
<button class="control-btn lang-toggle" title="Toggle language" aria-label="Toggle language">
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M2 12h20"/><path d="M12 2a15.3 15.3.0 014 10 15.3 15.3.0 01-4 10A15.3 15.3.0 018 12a15.3 15.3.0 014-10z"/></svg></button></div><div class=social-links><a href=https://github.com/yu-codes target=_blank rel="noopener noreferrer" class=social-link title=GitHub><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
</a><a href=mailto:dylan.jhou1120@gmail.com class=social-link title=Email><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></aside><div class=main-content><div class=topbar><div class=breadcrumb><a href=https://yu-codes.github.io/portfolio/>HOME</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/>ARTICLES</a>
<span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/>Others</a><span class=separator>&#8250;</span>
<a href=https://yu-codes.github.io/portfolio/articles/others/machine-learning/>Machine Learning</a><span class=separator>&#8250;</span>
<span>經典迴歸模型全攻略：線性、Ridge、Lasso、Logistic 與資料處理</span></div></div><main class=content-area><article class=article-content><header class=article-header><h1>經典迴歸模型全攻略：線性、Ridge、Lasso、Logistic 與資料處理</h1><div class=article-meta><span class=article-author><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg>
Yu-Han Jhou
</span><span class=article-updated><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
Last updated: 2025-08-05</span></div></header><div class=article-body><p>迴歸模型是機器學習的入門與核心。從最基礎的線性迴歸，到正則化的 Ridge/Lasso、再到分類用的 Logistic Regression，這些模型不僅是面試常客，也是實務專案的基石。本章將深入數學推導、直覺圖解、Python 實作、資料處理技巧與面試熱點，幫助你全面掌握迴歸模型。</p><hr><nav class=article-toc><span class=toc-title>Table of Contents</span><nav id=TableOfContents><ul><li><a href=#線性迴歸與多項式迴歸>線性迴歸與多項式迴歸</a><ul><li><a href=#線性迴歸linear-regression>線性迴歸（Linear Regression）</a></li><li><a href=#多項式迴歸polynomial-regression>多項式迴歸（Polynomial Regression）</a></li></ul></li><li><a href=#ridgelassoelastic-net-正則化>Ridge、Lasso、Elastic Net 正則化</a><ul><li><a href=#ridge-regressionl2-正則化>Ridge Regression（L2 正則化）</a></li><li><a href=#lasso-regressionl1-正則化>Lasso Regression（L1 正則化）</a></li><li><a href=#elastic-net>Elastic Net</a></li></ul></li><li><a href=#logistic--softmax-regression>Logistic & Softmax Regression</a><ul><li><a href=#logistic-regression二元分類>Logistic Regression（二元分類）</a></li><li><a href=#softmax-regression多元分類>Softmax Regression（多元分類）</a></li></ul></li><li><a href=#偏態資料的對數轉換與重抽樣>偏態資料的對數轉換與重抽樣</a><ul><li><a href=#偏態資料skewed-data>偏態資料（Skewed Data）</a></li><li><a href=#對數轉換log-transform>對數轉換（Log Transform）</a></li><li><a href=#重抽樣resampling>重抽樣（Resampling）</a></li></ul></li><li><a href=#面試熱點與常見誤區>面試熱點與常見誤區</a><ul><li><a href=#面試熱點>面試熱點</a></li><li><a href=#常見誤區>常見誤區</a></li></ul></li><li><a href=#使用注意事項>使用注意事項</a></li><li><a href=#延伸閱讀與資源>延伸閱讀與資源</a></li><li><a href=#結語>結語</a></li></ul></nav></nav><h2 id=線性迴歸與多項式迴歸>線性迴歸與多項式迴歸</h2><h3 id=線性迴歸linear-regression>線性迴歸（Linear Regression）</h3><ul><li>假設 $y = X\beta + \epsilon$，最小化殘差平方和。</li><li>封閉解：$\hat{\beta} = (X^TX)^{-1}X^Ty$</li><li>適用於連續型目標預測。</li></ul><h3 id=多項式迴歸polynomial-regression>多項式迴歸（Polynomial Regression）</h3><ul><li>將特徵升維，擬合非線性關係。</li><li>容易過擬合，需正則化或交叉驗證。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LinearRegression
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> PolynomialFeatures
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>100</span>)[:, <span style=color:#66d9ef>None</span>]
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> X<span style=color:#f92672>.</span>ravel() <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>100</span>) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 線性迴歸</span>
</span></span><span style=display:flex><span>lr <span style=color:#f92672>=</span> LinearRegression()<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(X, lr<span style=color:#f92672>.</span>predict(X), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Linear&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 多項式迴歸</span>
</span></span><span style=display:flex><span>poly <span style=color:#f92672>=</span> PolynomialFeatures(<span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>X_poly <span style=color:#f92672>=</span> poly<span style=color:#f92672>.</span>fit_transform(X)
</span></span><span style=display:flex><span>lr_poly <span style=color:#f92672>=</span> LinearRegression()<span style=color:#f92672>.</span>fit(X_poly, y)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(X, lr_poly<span style=color:#f92672>.</span>predict(X_poly), label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Poly deg=4&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(X, y, s<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;black&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend(); plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Linear vs Polynomial Regression&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><hr><h2 id=ridgelassoelastic-net-正則化>Ridge、Lasso、Elastic Net 正則化</h2><h3 id=ridge-regressionl2-正則化>Ridge Regression（L2 正則化）</h3><ul><li>加入懲罰項 $\lambda |\beta|^2$，抑制參數過大。</li><li>適合多重共線性、特徵多的情境。</li></ul><h3 id=lasso-regressionl1-正則化>Lasso Regression（L1 正則化）</h3><ul><li>懲罰項 $\lambda |\beta|_1$，可做特徵選擇（產生稀疏解）。</li><li>適合特徵選擇需求。</li></ul><h3 id=elastic-net>Elastic Net</h3><ul><li>結合 L1 與 L2，兼顧特徵選擇與穩定性。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> Ridge, Lasso, ElasticNet
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ridge <span style=color:#f92672>=</span> Ridge(alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>)<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>lasso <span style=color:#f92672>=</span> Lasso(alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>enet <span style=color:#f92672>=</span> ElasticNet(alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, l1_ratio<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Ridge 係數:&#34;</span>, ridge<span style=color:#f92672>.</span>coef_)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Lasso 係數:&#34;</span>, lasso<span style=color:#f92672>.</span>coef_)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;ElasticNet 係數:&#34;</span>, enet<span style=color:#f92672>.</span>coef_)
</span></span></code></pre></div><hr><h2 id=logistic--softmax-regression>Logistic & Softmax Regression</h2><h3 id=logistic-regression二元分類>Logistic Regression（二元分類）</h3><ul><li>預測機率 $P(y=1|x) = \sigma(x^T\beta)$，$\sigma$ 為 sigmoid。</li><li>損失函數為交叉熵，常用於二元分類。</li></ul><h3 id=softmax-regression多元分類>Softmax Regression（多元分類）</h3><ul><li>將 sigmoid 擴展為 softmax，適用於多類別。</li><li>輸出每類別的機率，損失同為交叉熵。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LogisticRegression
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X_cls <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>200</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>y_cls <span style=color:#f92672>=</span> (X_cls[:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> X_cls[:, <span style=color:#ae81ff>1</span>] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>astype(int)
</span></span><span style=display:flex><span>clf <span style=color:#f92672>=</span> LogisticRegression()<span style=color:#f92672>.</span>fit(X_cls, y_cls)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;預測機率:&#34;</span>, clf<span style=color:#f92672>.</span>predict_proba(X_cls[:<span style=color:#ae81ff>5</span>]))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;預測標籤:&#34;</span>, clf<span style=color:#f92672>.</span>predict(X_cls[:<span style=color:#ae81ff>5</span>]))
</span></span></code></pre></div><hr><h2 id=偏態資料的對數轉換與重抽樣>偏態資料的對數轉換與重抽樣</h2><h3 id=偏態資料skewed-data>偏態資料（Skewed Data）</h3><ul><li>常見於收入、房價等資料，右偏或左偏。</li><li>直接建模易受極端值影響。</li></ul><h3 id=對數轉換log-transform>對數轉換（Log Transform）</h3><ul><li>將偏態資料轉為近似常態，提升模型穩定性。</li><li>注意：資料需大於 0。</li></ul><h3 id=重抽樣resampling>重抽樣（Resampling）</h3><ul><li>欠抽樣（Undersampling）：減少多數類樣本。</li><li>過抽樣（Oversampling）：增加少數類樣本（如 SMOTE）。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;income&#39;</span>: np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>exponential(<span style=color:#ae81ff>50000</span>, <span style=color:#ae81ff>1000</span>)})
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#39;log_income&#39;</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>log1p(data[<span style=color:#e6db74>&#39;income&#39;</span>])
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#39;income&#39;</span>]<span style=color:#f92672>.</span>hist(alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Original&#39;</span>)
</span></span><span style=display:flex><span>data[<span style=color:#e6db74>&#39;log_income&#39;</span>]<span style=color:#f92672>.</span>hist(alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Log-Transformed&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend(); plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Skewed Data Log Transform&#34;</span>); plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><hr><h2 id=面試熱點與常見誤區>面試熱點與常見誤區</h2><h3 id=面試熱點>面試熱點</h3><table><thead><tr><th>主題</th><th>常見問題</th></tr></thead><tbody><tr><td>Ridge/Lasso</td><td>何時選用？數學差異？</td></tr><tr><td>Logistic Regression</td><td>為何用交叉熵？如何推導？</td></tr><tr><td>多項式迴歸</td><td>如何避免過擬合？</td></tr><tr><td>偏態資料處理</td><td>何時用對數轉換？有何風險？</td></tr><tr><td>重抽樣</td><td>何時用 SMOTE？有何缺點？</td></tr></tbody></table><h3 id=常見誤區>常見誤區</h3><ul><li>忽略特徵標準化對正則化模型的影響。</li><li>Lasso 係數全為 0 可能是 alpha 太大。</li><li>Logistic Regression 不適合極端不平衡資料。</li><li>對數轉換後忘記逆轉換預測結果。</li></ul><hr><h2 id=使用注意事項>使用注意事項</h2><ul><li>正則化模型需標準化特徵，避免懲罰不公平。</li><li>多項式迴歸易過擬合，建議搭配交叉驗證。</li><li>處理偏態資料時，注意資料分布與業務意義。</li></ul><hr><h2 id=延伸閱讀與資源>延伸閱讀與資源</h2><ul><li><a href="https://www.youtube.com/watch?v=NGf0voTMlcs">StatQuest: Ridge, Lasso, Elastic Net</a></li><li><a href=https://scikit-learn.org/stable/supervised_learning.html#supervised-learning>Scikit-learn Regression Models</a></li><li><a href=https://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf>Logistic Regression 推導</a></li><li><a href=https://imbalanced-learn.org/stable/>Imbalanced-learn 官方文件</a></li></ul><hr><h2 id=結語>結語</h2><p>經典迴歸模型是機器學習的基礎。熟悉線性、Ridge、Lasso、Logistic Regression 與資料處理技巧，能讓你在面試與實務中游刃有餘。下一章將進入分類演算法百寶箱，敬請期待！</p></div><div class=article-tags><div class=tags><span class=tag>Ridge</span>
<span class=tag>Lasso</span>
<span class=tag>Logistic Regression</span>
<span class=tag>Softmax</span></div></div><footer class=article-footer><a href=/portfolio/articles/others/machine-learning/ class=back-link>← Back to Machine Learning</a></footer></article></main><footer class=site-footer><p>&copy; 2026 YuHan Jhou. All rights reserved.</p></footer></div></div><script src=https://yu-codes.github.io/portfolio/js/theme.js defer></script><script src=https://yu-codes.github.io/portfolio/js/roadmap.js defer></script><script src=https://yu-codes.github.io/portfolio/js/parallax.js defer></script><script src=https://yu-codes.github.io/portfolio/js/page-transition.js defer></script></body></html>